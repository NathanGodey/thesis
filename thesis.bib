% Encoding: UTF-8
@inproceedings{bis-etal-2021-much,
	title        = {Too Much in Common: Shifting of Embeddings in Transformer Language Models and its Implications},
	author       = {Bi{\'s}, Daniel  and Podkorytov, Maksim  and Liu, Xiuwen},
	year         = 2021,
	month        = jun,
	booktitle    = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {5117--5130},
	doi          = {10.18653/v1/2021.naacl-main.403},
	url          = {https://aclanthology.org/2021.naacl-main.403},
	editor       = {Toutanova, Kristina  and Rumshisky, Anna  and Zettlemoyer, Luke  and Hakkani-Tur, Dilek  and Beltagy, Iz  and Bethard, Steven  and Cotterell, Ryan  and Chakraborty, Tanmoy  and Zhou, Yichao},
	abstract     = {The success of language models based on the Transformer architecture appears to be inconsistent with observed anisotropic properties of representations learned by such models. We resolve this by showing, contrary to previous studies, that the representations do not occupy a narrow cone, but rather drift in common directions. At any training step, all of the embeddings except for the ground-truth target embedding are updated with gradient in the same direction. Compounded over the training set, the embeddings drift and share common components, manifested in their shape in all the models we have empirically tested. Our experiments show that isotropy can be restored using a simple transformation.}
}
@inproceedings{mikolov-etal-2013-linguistic,
	title        = {Linguistic Regularities in Continuous Space Word Representations},
	author       = {Mikolov, Tomas  and Yih, Wen-tau  and Zweig, Geoffrey},
	year         = 2013,
	month        = jun,
	booktitle    = {Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies},
	publisher    = {Association for Computational Linguistics},
	address      = {Atlanta, Georgia},
	pages        = {746--751},
	url          = {https://aclanthology.org/N13-1090},
	editor       = {Vanderwende, Lucy  and Daum{\'e} III, Hal  and Kirchhoff, Katrin}
}
@inproceedings{pmlr-v97-allen19a,
	title        = {Analogies Explained: Towards Understanding Word Embeddings},
	author       = {Allen, Carl and Hospedales, Timothy},
	year         = 2019,
	month        = {09--15 Jun},
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = 97,
	pages        = {223--231},
	url          = {https://proceedings.mlr.press/v97/allen19a.html},
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	pdf          = {http://proceedings.mlr.press/v97/allen19a/allen19a.pdf},
	abstract     = {Word embeddings generated by neural network methods such as word2vec (W2V) are well known to exhibit seemingly linear behaviour, e.g. the embeddings of analogy “woman is to queen as man is to king” approximately describe a parallelogram. This property is particularly intriguing since the embeddings are not trained to achieve it. Several explanations have been proposed, but each introduces assumptions that do not hold in practice. We derive a probabilistically grounded definition of paraphrasing that we re-interpret as word transformation, a mathematical description of “$w_x$ is to $w_y$”. From these concepts we prove existence of linear relationship between W2V-type embeddings that underlie the analogical phenomenon, identifying explicit error terms.}
}
@inproceedings{ethayarajh-etal-2019-towards,
	title        = {Towards Understanding Linear Word Analogies},
	author       = {Ethayarajh, Kawin  and Duvenaud, David  and Hirst, Graeme},
	year         = 2019,
	month        = jul,
	booktitle    = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	publisher    = {Association for Computational Linguistics},
	address      = {Florence, Italy},
	pages        = {3253--3262},
	doi          = {10.18653/v1/P19-1315},
	url          = {https://aclanthology.org/P19-1315},
	editor       = {Korhonen, Anna  and Traum, David  and M{\`a}rquez, Llu{\'\i}s},
	abstract     = {A surprising property of word vectors is that word analogies can often be solved with vector arithmetic. However, it is unclear why arithmetic operators correspond to non-linear embedding models such as skip-gram with negative sampling (SGNS). We provide a formal explanation of this phenomenon without making the strong assumptions that past theories have made about the vector space and word distribution. Our theory has several implications. Past work has conjectured that linear substructures exist in vector spaces because relations can be represented as ratios; we prove that this holds for SGNS. We provide novel justification for the addition of SGNS word vectors by showing that it automatically down-weights the more frequent word, as weighting schemes do ad hoc. Lastly, we offer an information theoretic interpretation of Euclidean distance in vector spaces, justifying its use in capturing word dissimilarity.}
}
@techreport{francis79browncorpus,
	title        = {Brown Corpus Manual},
	author       = {Francis, W. N. and Kucera, H.},
	year         = 1979,
	url          = {http://icame.uib.no/brown/bcm.html},
	added-at     = {2008-02-29T17:14:20.000+0100},
	biburl       = {https://www.bibsonomy.org/bibtex/260bb0c74c2ecced0632393e47eb64f48/sb3000},
	institution  = {Department of Linguistics, Brown University, Providence, Rhode Island, US},
	interhash    = {119c367841941ad1a8f0db35d9f1c0b9},
	intrahash    = {60bb0c74c2ecced0632393e47eb64f48},
	keywords     = {corpus linguistics text-mining},
	timestamp    = {2008-02-29T17:14:20.000+0100}
}
@article{sgd,
	title        = {{A Stochastic Approximation Method}},
	author       = {Herbert Robbins and Sutton Monro},
	year         = 1951,
	journal      = {The Annals of Mathematical Statistics},
	publisher    = {Institute of Mathematical Statistics},
	volume       = 22,
	number       = 3,
	pages        = {400 -- 407},
	doi          = {10.1214/aoms/1177729586},
	url          = {https://doi.org/10.1214/aoms/1177729586}
}
@book{aronoff1993morphology,
	title        = {Morphology by Itself: Stems and Inflectional Classes},
	author       = {Aronoff, M.},
	year         = 1993,
	publisher    = {MIT Press},
	series       = {Linguistic Inquiry Monographs},
	isbn         = 9780262510721,
	url          = {https://books.google.fr/books?id=PFMH1qg-Z5kC},
	lccn         = 93020543
}
@inproceedings{adi2017finegrained,
	title        = {Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks},
	author       = {Yossi Adi and Einat Kermany and Yonatan Belinkov and Ofer Lavi and Yoav Goldberg},
	year         = 2017,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=BJh6Ztuxl}
}
@inproceedings{jawahar-etal-2019-bert,
	title        = {What Does {BERT} Learn about the Structure of Language?},
	author       = {Jawahar, Ganesh  and Sagot, Beno{\^\i}t  and Seddah, Djam{\'e}},
	year         = 2019,
	month        = jul,
	booktitle    = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	publisher    = {Association for Computational Linguistics},
	address      = {Florence, Italy},
	pages        = {3651--3657},
	doi          = {10.18653/v1/P19-1356},
	url          = {https://aclanthology.org/P19-1356},
	editor       = {Korhonen, Anna  and Traum, David  and M{\`a}rquez, Llu{\'\i}s},
	abstract     = {BERT is a recent language representation model that has surprisingly performed well in diverse language understanding benchmarks. This result indicates the possibility that BERT networks capture structural information about language. In this work, we provide novel support for this claim by performing a series of experiments to unpack the elements of English language structure learned by BERT. Our findings are fourfold. BERT{'}s phrasal representation captures the phrase-level information in the lower layers. The intermediate layers of BERT compose a rich hierarchy of linguistic information, starting with surface features at the bottom, syntactic features in the middle followed by semantic features at the top. BERT requires deeper layers while tracking subject-verb agreement to handle long-term dependency problem. Finally, the compositional scheme underlying BERT mimics classical, tree-like structures.}
}
@inproceedings{tenney-etal-2019-bert,
	title        = {{BERT} Rediscovers the Classical {NLP} Pipeline},
	author       = {Tenney, Ian  and Das, Dipanjan  and Pavlick, Ellie},
	year         = 2019,
	month        = jul,
	booktitle    = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	publisher    = {Association for Computational Linguistics},
	address      = {Florence, Italy},
	pages        = {4593--4601},
	doi          = {10.18653/v1/P19-1452},
	url          = {https://aclanthology.org/P19-1452},
	editor       = {Korhonen, Anna  and Traum, David  and M{\`a}rquez, Llu{\'\i}s},
	abstract     = {Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.}
}
@inproceedings{peters-etal-2018-dissecting,
	title        = {Dissecting Contextual Word Embeddings: Architecture and Representation},
	author       = {Peters, Matthew E.  and Neumann, Mark  and Zettlemoyer, Luke  and Yih, Wen-tau},
	year         = 2018,
	month        = oct # {-} # nov,
	booktitle    = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	publisher    = {Association for Computational Linguistics},
	address      = {Brussels, Belgium},
	pages        = {1499--1509},
	doi          = {10.18653/v1/D18-1179},
	url          = {https://aclanthology.org/D18-1179},
	editor       = {Riloff, Ellen  and Chiang, David  and Hockenmaier, Julia  and Tsujii, Jun{'}ichi},
	abstract     = {Contextual word representations derived from pre-trained bidirectional language models (biLMs) have recently been shown to provide significant improvements to the state of the art for a wide range of NLP tasks. However, many questions remain as to how and why these models are so effective. In this paper, we present a detailed empirical study of how the choice of neural architecture (e.g. LSTM, CNN, or self attention) influences both end task accuracy and qualitative properties of the representations that are learned. We show there is a tradeoff between speed and accuracy, but all architectures learn high quality contextual representations that outperform word embeddings for four challenging NLP tasks. Additionally, all architectures learn representations that vary with network depth, from exclusively morphological based at the word embedding layer through local syntax based in the lower contextual layers to longer range semantics such coreference at the upper layers. Together, these results suggest that unsupervised biLMs, independent of architecture, are learning much more about the structure of language than previously appreciated.}
}
@inproceedings{liu-etal-2019-linguistic,
	title        = {Linguistic Knowledge and Transferability of Contextual Representations},
	author       = {Liu, Nelson F.  and Gardner, Matt  and Belinkov, Yonatan  and Peters, Matthew E.  and Smith, Noah A.},
	year         = 2019,
	month        = jun,
	booktitle    = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Minneapolis, Minnesota},
	pages        = {1073--1094},
	doi          = {10.18653/v1/N19-1112},
	url          = {https://aclanthology.org/N19-1112},
	editor       = {Burstein, Jill  and Doran, Christy  and Solorio, Thamar},
	abstract     = {Contextual word representations derived from large-scale neural language models are successful across a diverse set of NLP tasks, suggesting that they encode useful and transferable features of language. To shed light on the linguistic knowledge they capture, we study the representations produced by several recent pretrained contextualizers (variants of ELMo, the OpenAI transformer language model, and BERT) with a suite of sixteen diverse probing tasks. We find that linear models trained on top of frozen contextual representations are competitive with state-of-the-art task-specific models in many cases, but fail on tasks requiring fine-grained linguistic knowledge (e.g., conjunct identification). To investigate the transferability of contextual word representations, we quantify differences in the transferability of individual layers within contextualizers, especially between recurrent neural networks (RNNs) and transformers. For instance, higher layers of RNNs are more task-specific, while transformer layers do not exhibit the same monotonic trend. In addition, to better understand what makes contextual word representations transferable, we compare language model pretraining with eleven supervised pretraining tasks. For any given task, pretraining on a closely related task yields better performance than language model pretraining (which is better on average) when the pretraining dataset is fixed. However, language model pretraining on more data gives the best results.}
}
@inproceedings{jiang2024on,
	title        = {On the Origins of Linear Representations in Large Language Models},
	author       = {Yibo Jiang and Goutham Rajendran and Pradeep Kumar Ravikumar and Bryon Aragam and Victor Veitch},
	year         = 2024,
	booktitle    = {Forty-first International Conference on Machine Learning},
	url          = {https://openreview.net/forum?id=otuTw4Mghk}
}
@inproceedings{ethayarajh-2019-contextual,
	title        = {How Contextual are Contextualized Word Representations? {C}omparing the Geometry of {BERT}, {ELM}o, and {GPT}-2 Embeddings},
	author       = {Ethayarajh, Kawin},
	year         = 2019,
	month        = nov,
	booktitle    = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
	publisher    = {Association for Computational Linguistics},
	address      = {Hong Kong, China},
	pages        = {55--65},
	doi          = {10.18653/v1/D19-1006},
	url          = {https://aclanthology.org/D19-1006},
	editor       = {Inui, Kentaro  and Jiang, Jing  and Ng, Vincent  and Wan, Xiaojun},
	abstract     = {Replacing static word embeddings with contextualized word representations has yielded significant improvements on many NLP tasks. However, just how contextual are the contextualized representations produced by models such as ELMo and BERT? Are there infinitely many context-specific representations for each word, or are words essentially assigned one of a finite number of word-sense representations? For one, we find that the contextualized representations of all words are not isotropic in any layer of the contextualizing model. While representations of the same word in different contexts still have a greater cosine similarity than those of two different words, this self-similarity is much lower in upper layers. This suggests that upper layers of contextualizing models produce more context-specific representations, much like how upper layers of LSTMs produce more task-specific representations. In all layers of ELMo, BERT, and GPT-2, on average, less than 5{\%} of the variance in a word{'}s contextualized representations can be explained by a static embedding for that word, providing some justification for the success of contextualized representations.}
}
@inbook{tf_idf,
	title        = {A statistical interpretation of term specificity and its application in retrieval},
	author       = {Sparck Jones, Karen},
	year         = 1988,
	booktitle    = {Document Retrieval Systems},
	publisher    = {Taylor Graham Publishing},
	address      = {GBR},
	pages        = {132–142},
	isbn         = {0947568212},
	numpages     = 11
}
@misc{harris1954distributional,
	title        = {Distributional Structure},
	author       = {Harris, ZS},
	year         = 1954,
	publisher    = {Word}
}
@inproceedings{weaver-1952-translation,
	title        = {Translation},
	author       = {Weaver, Warren},
	year         = 1952,
	month        = {17-20 } # jun,
	booktitle    = {Proceedings of the Conference on Mechanical Translation},
	address      = {Massachusetts Institute of Technology},
	url          = {https://aclanthology.org/1952.earlymt-1.1}
}
@article{shannon_mi,
	title        = {A Mathematical Theory of Communication},
	author       = {Shannon, Claude Elwood},
	year         = 1948,
	journal      = {The Bell System Technical Journal},
	volume       = 27,
	pages        = {379--423},
	url          = {http://plan9.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf},
	urldate      = {2003-04-22},
	added-at     = {2021-09-19T18:40:37.000+0200},
	biburl       = {https://www.bibsonomy.org/bibtex/29f88587b33c82f692b61d129eb2f2517/steschum},
	interhash    = {754130207906fcec16a53d330eeff348},
	intrahash    = {9f88587b33c82f692b61d129eb2f2517},
	keywords     = {imported},
	timestamp    = {2021-09-19T18:41:56.000+0200}
}
@incollection{alexnet,
	title        = {ImageNet Classification with Deep Convolutional Neural Networks},
	author       = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	year         = 2012,
	booktitle    = {Advances in Neural Information Processing Systems 25},
	publisher    = {Curran Associates, Inc.},
	pages        = {1097--1105},
	url          = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
	added-at     = {2016-11-14T12:05:24.000+0100},
	biburl       = {https://www.bibsonomy.org/bibtex/2886c491fe45049fee3c9660df30bb5c4/albinzehe},
	editor       = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	interhash    = {74bbb5dea5afb1b088bd10e317f1f0d2},
	intrahash    = {886c491fe45049fee3c9660df30bb5c4},
	keywords     = {cnn deeplearning ma-zehe neuralnet},
	timestamp    = {2016-11-14T12:05:24.000+0100}
}
@inproceedings{cho-etal-2014-learning,
	title        = {Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation},
	author       = {Cho, Kyunghyun  and van Merri{\"e}nboer, Bart  and Gulcehre, Caglar  and Bahdanau, Dzmitry  and Bougares, Fethi  and Schwenk, Holger  and Bengio, Yoshua},
	year         = 2014,
	month        = oct,
	booktitle    = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
	publisher    = {Association for Computational Linguistics},
	address      = {Doha, Qatar},
	pages        = {1724--1734},
	doi          = {10.3115/v1/D14-1179},
	url          = {https://aclanthology.org/D14-1179},
	editor       = {Moschitti, Alessandro  and Pang, Bo  and Daelemans, Walter}
}
@misc{word2vec,
	title        = {Efficient Estimation of Word Representations in Vector Space},
	author       = {Tomas Mikolov and Kai Chen and Greg S. Corrado and Jeffrey Dean},
	year         = 2013,
	url          = {http://arxiv.org/abs/1301.3781}
}
}
@inproceedings{pmlr-vR5-morin05a,
	title        = {Hierarchical Probabilistic Neural Network Language Model},
	author       = {Morin, Frederic and Bengio, Yoshua},
	year         = 2005,
	month        = {06--08 Jan},
	booktitle    = {Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = {R5},
	pages        = {246--252},
	url          = {https://proceedings.mlr.press/r5/morin05a.html},
	note         = {Reissued by PMLR on 30 March 2021.},
	editor       = {Cowell, Robert G. and Ghahramani, Zoubin},
	pdf          = {http://proceedings.mlr.press/r5/morin05a/morin05a.pdf}
}
@inproceedings{pennington-etal-2014-glove,
	title        = {{G}lo{V}e: Global Vectors for Word Representation},
	author       = {Pennington, Jeffrey  and Socher, Richard  and Manning, Christopher},
	year         = 2014,
	month        = oct,
	booktitle    = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
	publisher    = {Association for Computational Linguistics},
	address      = {Doha, Qatar},
	pages        = {1532--1543},
	doi          = {10.3115/v1/D14-1162},
	url          = {https://aclanthology.org/D14-1162},
	editor       = {Moschitti, Alessandro  and Pang, Bo  and Daelemans, Walter}
}
@inproceedings{lstm_w2v,
	title        = {Research on Patent Text Classification Based on Word2Vec and LSTM},
	author       = {Xiao, Lizhong and Wang, Guangzhong and Zuo, Yang},
	year         = 2018,
	booktitle    = {2018 11th International Symposium on Computational Intelligence and Design (ISCID)},
	volume       = {01},
	number       = {},
	pages        = {71--74},
	doi          = {10.1109/ISCID.2018.00023},
	keywords     = {Computational intelligence;Handheld computers;security field;text categorization;Word2Vec;long and short-term memory networks;stop words}
}
@article{MUHAMMAD2021728,
	title        = {Sentiment Analysis Using Word2vec And Long Short-Term Memory (LSTM) For Indonesian Hotel Reviews},
	author       = {Putra Fissabil Muhammad and Retno Kusumaningrum and Adi Wibowo},
	year         = 2021,
	journal      = {Procedia Computer Science},
	volume       = 179,
	pages        = {728--735},
	doi          = {https://doi.org/10.1016/j.procs.2021.01.061},
	issn         = {1877-0509},
	url          = {https://www.sciencedirect.com/science/article/pii/S1877050921000752},
	note         = {5th International Conference on Computer Science and Computational Intelligence 2020},
	keywords     = {Hotel Reviews, sentiment analysis, Word2Vec, Long-short term memory}
}
@article{tl_survey,
	title        = {A Survey on Transfer Learning},
	author       = {Pan, Sinno Jialin and Yang, Qiang},
	year         = 2010,
	journal      = {IEEE Transactions on Knowledge and Data Engineering},
	volume       = 22,
	number       = 10,
	pages        = {1345--1359},
	doi          = {10.1109/TKDE.2009.191},
	keywords     = {Machine learning;Training data;Data mining;Knowledge transfer;Space technology;Knowledge engineering;Machine learning algorithms;Labeling;Learning systems;Testing;Transfer learning;survey;machine learning;data mining.}
}
@inproceedings{panchendrarajan-amaresan-2018-bidirectional,
	title        = {Bidirectional {LSTM}-{CRF} for Named Entity Recognition},
	author       = {Panchendrarajan, Rrubaa  and Amaresan, Aravindh},
	year         = 2018,
	month        = {1{--}3 } # dec,
	booktitle    = {Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation},
	publisher    = {Association for Computational Linguistics},
	address      = {Hong Kong},
	url          = {https://aclanthology.org/Y18-1061},
	editor       = {Politzer-Ahles, Stephen  and Hsu, Yu-Yin  and Huang, Chu-Ren  and Yao, Yao}
}
@article{zar2005spearman,
	title        = {Spearman rank correlation},
	author       = {Zar, Jerrold H},
	year         = 2005,
	journal      = {Encyclopedia of Biostatistics},
	publisher    = {Wiley Online Library},
	volume       = 7
}
@inproceedings{mu2018allbutthetop,
	title        = {All-but-the-Top: Simple and Effective Postprocessing for Word Representations},
	author       = {Jiaqi Mu and Pramod Viswanath},
	year         = 2018,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=HkuGJ3kCb}
}
@inproceedings{li-etal-2020-sentence,
	title        = {On the Sentence Embeddings from Pre-trained Language Models},
	author       = {Li, Bohan  and Zhou, Hao  and He, Junxian  and Wang, Mingxuan  and Yang, Yiming  and Li, Lei},
	year         = 2020,
	month        = nov,
	booktitle    = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {9119--9130},
	doi          = {10.18653/v1/2020.emnlp-main.733},
	url          = {https://aclanthology.org/2020.emnlp-main.733},
	editor       = {Webber, Bonnie  and Cohn, Trevor  and He, Yulan  and Liu, Yang},
	abstract     = {Pre-trained contextual representations like BERT have achieved great success in natural language processing. However, the sentence embeddings from the pre-trained language models without fine-tuning have been found to poorly capture semantic meaning of sentences. In this paper, we argue that the semantic information in the BERT embeddings is not fully exploited. We first reveal the theoretical connection between the masked language model pre-training objective and the semantic similarity task theoretically, and then analyze the BERT sentence embeddings empirically. We find that BERT always induces a non-smooth anisotropic semantic space of sentences, which harms its performance of semantic similarity. To address this issue, we propose to transform the anisotropic sentence embedding distribution to a smooth and isotropic Gaussian distribution through normalizing flows that are learned with an unsupervised objective. Experimental results show that our proposed BERT-flow method obtains significant performance gains over the state-of-the-art sentence embeddings on a variety of semantic textual similarity tasks. The code is available at \url{https://github.com/bohanli/BERT-flow}.}
}
@inproceedings{arora2017a,
	title        = {A Simple but Tough-to-Beat Baseline for Sentence Embeddings},
	author       = {Sanjeev Arora and Yingyu Liang and Tengyu Ma},
	year         = 2017,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=SyK00v5xx}
}
@inproceedings{conneau-etal-2017-supervised,
	title        = {Supervised Learning of Universal Sentence Representations from Natural Language Inference Data},
	author       = {Conneau, Alexis  and Kiela, Douwe  and Schwenk, Holger  and Barrault, Lo{\"\i}c  and Bordes, Antoine},
	year         = 2017,
	month        = sep,
	booktitle    = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
	publisher    = {Association for Computational Linguistics},
	address      = {Copenhagen, Denmark},
	pages        = {670--680},
	doi          = {10.18653/v1/D17-1070},
	url          = {https://aclanthology.org/D17-1070},
	editor       = {Palmer, Martha  and Hwa, Rebecca  and Riedel, Sebastian},
	abstract     = {Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available.}
}
@inproceedings{cer-etal-2018-universal,
	title        = {Universal Sentence Encoder for {E}nglish},
	author       = {Cer, Daniel  and Yang, Yinfei  and Kong, Sheng-yi  and Hua, Nan  and Limtiaco, Nicole  and St. John, Rhomni  and Constant, Noah  and Guajardo-Cespedes, Mario  and Yuan, Steve  and Tar, Chris  and Strope, Brian  and Kurzweil, Ray},
	year         = 2018,
	month        = nov,
	booktitle    = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
	publisher    = {Association for Computational Linguistics},
	address      = {Brussels, Belgium},
	pages        = {169--174},
	doi          = {10.18653/v1/D18-2029},
	url          = {https://aclanthology.org/D18-2029},
	editor       = {Blanco, Eduardo  and Lu, Wei},
	abstract     = {We present easy-to-use TensorFlow Hub sentence embedding models having good task transfer performance. Model variants allow for trade-offs between accuracy and compute resources. We report the relationship between model complexity, resources, and transfer performance. Comparisons are made with baselines without transfer learning and to baselines that incorporate word-level transfer. Transfer learning using sentence-level embeddings is shown to outperform models without transfer learning and often those that use only word-level transfer. We show good transfer task performance with minimal training data and obtain encouraging results on word embedding association tests (WEAT) of model bias.}
}
@inproceedings{hill-etal-2016-learning,
	title        = {Learning Distributed Representations of Sentences from Unlabelled Data},
	author       = {Hill, Felix  and Cho, Kyunghyun  and Korhonen, Anna},
	year         = 2016,
	month        = jun,
	booktitle    = {Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies},
	publisher    = {Association for Computational Linguistics},
	address      = {San Diego, California},
	pages        = {1367--1377},
	doi          = {10.18653/v1/N16-1162},
	url          = {https://aclanthology.org/N16-1162},
	editor       = {Knight, Kevin  and Nenkova, Ani  and Rambow, Owen}
}
@inproceedings{schwenk-douze-2017-learning,
	title        = {Learning Joint Multilingual Sentence Representations with Neural Machine Translation},
	author       = {Schwenk, Holger  and Douze, Matthijs},
	year         = 2017,
	month        = aug,
	booktitle    = {Proceedings of the 2nd Workshop on Representation Learning for {NLP}},
	publisher    = {Association for Computational Linguistics},
	address      = {Vancouver, Canada},
	pages        = {157--167},
	doi          = {10.18653/v1/W17-2619},
	url          = {https://aclanthology.org/W17-2619},
	editor       = {Blunsom, Phil  and Bordes, Antoine  and Cho, Kyunghyun  and Cohen, Shay  and Dyer, Chris  and Grefenstette, Edward  and Hermann, Karl Moritz  and Rimell, Laura  and Weston, Jason  and Yih, Scott},
	abstract     = {In this paper, we use the framework of neural machine translation to learn joint sentence representations across six very different languages. Our aim is that a representation which is independent of the language, is likely to capture the underlying semantics. We define a new cross-lingual similarity measure, compare up to 1.4M sentence representations and study the characteristics of close sentences. We provide experimental evidence that sentences that are close in embedding space are indeed semantically highly related, but often have quite different structure and syntax. These relations also hold when comparing sentences in different languages.}
}
@misc{Sutton_2019,
	title        = {The Bitter Lesson},
	author       = {Sutton, Rich},
	year         = 2019,
	month        = {Mar},
	journal      = {Incomplete Ideas},
	url          = {https://incompleteideas.net/IncIdeas/BitterLesson.html}
}
@inproceedings{Zhang2020BERTScore,
	title        = {BERTScore: Evaluating Text Generation with BERT},
	author       = {Tianyi Zhang and Varsha Kishore and Felix Wu and Kilian Q. Weinberger and Yoav Artzi},
	year         = 2020,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=SkeHuCVFDr}
}
@inproceedings{reimers-gurevych-2019-sentence,
	title        = {Sentence-{BERT}: Sentence Embeddings using {S}iamese {BERT}-Networks},
	author       = {Reimers, Nils  and Gurevych, Iryna},
	year         = 2019,
	month        = nov,
	booktitle    = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
	publisher    = {Association for Computational Linguistics},
	address      = {Hong Kong, China},
	pages        = {3982--3992},
	doi          = {10.18653/v1/D19-1410},
	url          = {https://aclanthology.org/D19-1410},
	editor       = {Inui, Kentaro  and Jiang, Jing  and Ng, Vincent  and Wan, Xiaojun},
	abstract     = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.}
}
@inproceedings{bommasani-etal-2020-interpreting,
	title        = {{I}nterpreting {P}retrained {C}ontextualized {R}epresentations via {R}eductions to {S}tatic {E}mbeddings},
	author       = {Bommasani, Rishi  and Davis, Kelly  and Cardie, Claire},
	year         = 2020,
	month        = jul,
	booktitle    = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {4758--4781},
	doi          = {10.18653/v1/2020.acl-main.431},
	url          = {https://aclanthology.org/2020.acl-main.431},
	editor       = {Jurafsky, Dan  and Chai, Joyce  and Schluter, Natalie  and Tetreault, Joel},
	abstract     = {Contextualized representations (e.g. ELMo, BERT) have become the default pretrained representations for downstream NLP applications. In some settings, this transition has rendered their static embedding predecessors (e.g. Word2Vec, GloVe) obsolete. As a side-effect, we observe that older interpretability methods for static embeddings {---} while more diverse and mature than those available for their dynamic counterparts {---} are underutilized in studying newer contextualized representations. Consequently, we introduce simple and fully general methods for converting from contextualized representations to static lookup-table embeddings which we apply to 5 popular pretrained models and 9 sets of pretrained weights. Our analysis of the resulting static embeddings notably reveals that pooling over many contexts significantly improves representational quality under intrinsic evaluation. Complementary to analyzing representational quality, we consider social biases encoded in pretrained representations with respect to gender, race/ethnicity, and religion and find that bias is encoded disparately across pretrained models and internal layers even for models with the same training data. Concerningly, we find dramatic inconsistencies between social bias estimators for word embeddings.}
}
@inproceedings{martin-etal-2020-camembert,
	title        = {{C}amem{BERT}: a Tasty {F}rench Language Model},
	author       = {Martin, Louis  and Muller, Benjamin  and Ortiz Su{\'a}rez, Pedro Javier  and Dupont, Yoann  and Romary, Laurent  and de la Clergerie, {\'E}ric  and Seddah, Djam{\'e}  and Sagot, Beno{\^\i}t},
	year         = 2020,
	month        = jul,
	booktitle    = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {7203--7219},
	doi          = {10.18653/v1/2020.acl-main.645},
	url          = {https://aclanthology.org/2020.acl-main.645},
	editor       = {Jurafsky, Dan  and Chai, Joyce  and Schluter, Natalie  and Tetreault, Joel},
	abstract     = {Pretrained language models are now ubiquitous in Natural Language Processing. Despite their success, most available models have either been trained on English data or on the concatenation of data in multiple languages. This makes practical use of such models {--}in all languages except English{--} very limited. In this paper, we investigate the feasibility of training monolingual Transformer-based language models for other languages, taking French as an example and evaluating our language models on part-of-speech tagging, dependency parsing, named entity recognition and natural language inference tasks. We show that the use of web crawled data is preferable to the use of Wikipedia data. More surprisingly, we show that a relatively small web crawled dataset (4GB) leads to results that are as good as those obtained using larger datasets (130+GB). Our best performing model CamemBERT reaches or improves the state of the art in all four downstream tasks.}
}
@article{owens_gpu,
	title        = {GPU Computing},
	author       = {Owens, John D. and Houston, Mike and Luebke, David and Green, Simon and Stone, John E. and Phillips, James C.},
	year         = 2008,
	journal      = {Proceedings of the IEEE},
	volume       = 96,
	number       = 5,
	pages        = {879--899},
	doi          = {10.1109/JPROC.2008.917757},
	keywords     = {Graphics;Central Processing Unit;Physics computing;Engines;Arithmetic;Bandwidth;Microprocessors;Hardware;Computational biophysics;Performance gain;General-purpose computing on the graphics processing unit (GPGPU);GPU computing;parallel computing}
}
@article{bojanowski-etal-2017-enriching,
	title        = {Enriching Word Vectors with Subword Information},
	author       = {Bojanowski, Piotr  and Grave, Edouard  and Joulin, Armand  and Mikolov, Tomas},
	year         = 2017,
	journal      = {Transactions of the Association for Computational Linguistics},
	publisher    = {MIT Press},
	address      = {Cambridge, MA},
	volume       = 5,
	pages        = {135--146},
	doi          = {10.1162/tacl_a_00051},
	url          = {https://aclanthology.org/Q17-1010},
	editor       = {Lee, Lillian  and Johnson, Mark  and Toutanova, Kristina},
	abstract     = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.}
}
@inproceedings{billingsley-curran-2012-improvements,
	title        = {Improvements to Training an {RNN} parser},
	author       = {Billingsley, Richard  and Curran, James},
	year         = 2012,
	month        = dec,
	booktitle    = {Proceedings of {COLING} 2012},
	publisher    = {The COLING 2012 Organizing Committee},
	address      = {Mumbai, India},
	pages        = {279--294},
	url          = {https://aclanthology.org/C12-1018},
	editor       = {Kay, Martin  and Boitet, Christian}
}
@article{lda,
	title        = {Latent dirichlet allocation},
	author       = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
	year         = 2003,
	month        = {mar},
	journal      = {J. Mach. Learn. Res.},
	publisher    = {JMLR.org},
	volume       = 3,
	number       = {null},
	pages        = {993–1022},
	issn         = {1532-4435},
	issue_date   = {3/1/2003},
	abstract     = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
	numpages     = 30
}
@article{churchill2022evolution,
	title        = {The evolution of topic modeling},
	author       = {Churchill, Rob and Singh, Lisa},
	year         = 2022,
	journal      = {ACM Computing Surveys},
	publisher    = {ACM New York, NY},
	volume       = 54,
	number       = {10s},
	pages        = {1--35}
}
@article{deerwester1990indexing,
	title        = {Indexing by latent semantic analysis},
	author       = {Deerwester, Scott and Dumais, Susan T and Furnas, George W and Landauer, Thomas K and Harshman, Richard},
	year         = 1990,
	journal      = {Journal of the American society for information science},
	publisher    = {Wiley Online Library},
	volume       = 41,
	number       = 6,
	pages        = {391--407}
}
@inproceedings{ramos2003using,
	title        = {Using tf-idf to determine word relevance in document queries},
	author       = {Ramos, Juan and others},
	year         = 2003,
	booktitle    = {Proceedings of the first instructional conference on machine learning},
	volume       = 242,
	number       = 1,
	pages        = {29--48},
	organization = {Citeseer}
}
@inproceedings{He_2020_CVPR,
	title        = {Momentum Contrast for Unsupervised Visual Representation Learning},
	author       = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
	year         = 2020,
	month        = {June},
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}
}
@misc{oord2019representationlearningcontrastivepredictive,
	title        = {Representation Learning with Contrastive Predictive Coding},
	author       = {Aaron van den Oord and Yazhe Li and Oriol Vinyals},
	year         = 2019,
	url          = {https://arxiv.org/abs/1807.03748},
	eprint       = {1807.03748},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{li2023generaltextembeddingsmultistage,
	title        = {Towards General Text Embeddings with Multi-stage Contrastive Learning},
	author       = {Zehan Li and Xin Zhang and Yanzhao Zhang and Dingkun Long and Pengjun Xie and Meishan Zhang},
	year         = 2023,
	url          = {https://arxiv.org/abs/2308.03281},
	eprint       = {2308.03281},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@misc{Duquenne:2023:sonar_arxiv,
	title        = {{SONAR:} Sentence-Level Multimodal and Language-Agnostic Representations},
	author       = {Paul-Ambroise Duquenne and Holger Schwenk and Benoit Sagot},
	year         = 2023,
	publisher    = {arXiv},
	url          = {https://arxiv.org/abs/2308.11466}
}
@article{interpretability_survey,
	title        = {Post-hoc Interpretability for Neural NLP: A Survey},
	author       = {Madsen, Andreas and Reddy, Siva and Chandar, Sarath},
	year         = 2022,
	month        = {dec},
	journal      = {ACM Comput. Surv.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 55,
	number       = 8,
	doi          = {10.1145/3546577},
	issn         = {0360-0300},
	url          = {https://doi.org/10.1145/3546577},
	issue_date   = {August 2023},
	abstract     = {Neural networks for NLP are becoming increasingly complex and widespread, and there is a growing concern if these models are responsible to use. Explaining models helps to address the safety and ethical concerns and is essential for accountability. Interpretability serves to provide these explanations in terms that are understandable to humans. Additionally, post-hoc methods provide explanations after a model is learned and are generally model-agnostic. This survey provides a categorization of how recent post-hoc interpretability methods communicate explanations to humans, it discusses each method in-depth, and how they are validated, as the latter is often a common concern.},
	articleno    = 155,
	numpages     = 42,
	keywords     = {Interpretability, transparency, post-hoc explanations}
}
@inproceedings{park2024the,
	title        = {The Linear Representation Hypothesis and the Geometry of Large Language Models},
	author       = {Kiho Park and Yo Joong Choe and Victor Veitch},
	year         = 2024,
	booktitle    = {Forty-first International Conference on Machine Learning},
	url          = {https://openreview.net/forum?id=UGpGkLzwpP}
}
@inproceedings{feng-etal-2022-language,
	title        = {Language-agnostic {BERT} Sentence Embedding},
	author       = {Feng, Fangxiaoyu  and Yang, Yinfei  and Cer, Daniel  and Arivazhagan, Naveen  and Wang, Wei},
	year         = 2022,
	month        = may,
	booktitle    = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Dublin, Ireland},
	pages        = {878--891},
	doi          = {10.18653/v1/2022.acl-long.62},
	url          = {https://aclanthology.org/2022.acl-long.62},
	editor       = {Muresan, Smaranda  and Nakov, Preslav  and Villavicencio, Aline},
	abstract     = {While BERT is an effective method for learning monolingual sentence embeddings for semantic similarity and embedding based transfer learning BERT based cross-lingual sentence embeddings have yet to be explored. We systematically investigate methods for learning multilingual sentence embeddings by combining the best methods for learning monolingual and cross-lingual representations including: masked language modeling (MLM), translation language modeling (TLM), dual encoder translation ranking, and additive margin softmax. We show that introducing a pre-trained multilingual language model dramatically reduces the amount of parallel training data required to achieve good performance by 80{\%}. Composing the best of these methods produces a model that achieves 83.7{\%} bi-text retrieval accuracy over 112 languages on Tatoeba, well above the 65.5{\%} achieved by LASER, while still performing competitively on monolingual transfer learning benchmarks. Parallel data mined from CommonCrawl using our best model is shown to train competitive NMT models for en-zh and en-de. We publicly release our best multilingual sentence embedding model for 109+ languages at \url{https://tfhub.dev/google/LaBSE}.}
}
@article{10.1145/3593590,
	title        = {Contrastive Learning Models for Sentence Representations},
	author       = {Xu, Lingling and Xie, Haoran and Li, Zongxi and Wang, Fu Lee and Wang, Weiming and Li, Qing},
	year         = 2023,
	month        = {jun},
	journal      = {ACM Trans. Intell. Syst. Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 14,
	number       = 4,
	doi          = {10.1145/3593590},
	issn         = {2157-6904},
	url          = {https://doi.org/10.1145/3593590},
	issue_date   = {August 2023},
	abstract     = {Sentence representation learning is a crucial task in natural language processing, as the quality of learned representations directly influences downstream tasks, such as sentence classification and sentiment analysis. Transformer-based pretrained language models such as bidirectional encoder representations from transformers (BERT) have been extensively applied to various natural language processing tasks, and have exhibited moderately good performance. However, the anisotropy of the learned embedding space prevents BERT sentence embeddings from achieving good results in the semantic textual similarity tasks. It has been shown that contrastive learning can alleviate the anisotropy problem and significantly improve sentence representation performance. Therefore, there has been a surge in the development of models that utilize contrastive learning to fine-tune BERT-like pretrained language models to learn sentence representations. But no systematic review of contrastive learning models for sentence representations has been conducted. To fill this gap, this article summarizes and categorizes the contrastive learning based sentence representation models, common evaluation tasks for assessing the quality of learned representations, and future research directions. Furthermore, we select several representative models for exhaustive experiments to illustrate the quantitative improvement of various strategies on sentence representations.},
	articleno    = 67,
	numpages     = 34,
	keywords     = {Sentence representation learning, contrastive learning, Data Augmentation, BERT}
}
@inproceedings{simclr,
	title        = {A simple framework for contrastive learning of visual representations},
	author       = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	year         = 2020,
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
	publisher    = {JMLR.org},
	series       = {ICML'20},
	abstract     = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by Sim-CLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100\texttimes{} fewer labels.},
	articleno    = 149,
	numpages     = 11
}
@inproceedings{gao2018representation,
	title        = {Representation Degeneration Problem in Training Natural Language Generation Models},
	author       = {Jun Gao and Di He and Xu Tan and Tao Qin and Liwei Wang and Tieyan Liu},
	year         = 2019,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=SkEYojRqtm}
}
@inproceedings{jing2022understanding,
	title        = {Understanding Dimensional Collapse in Contrastive Self-supervised Learning},
	author       = {Li Jing and Pascal Vincent and Yann LeCun and Yuandong Tian},
	year         = 2022,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=YevsQ05DEN7}
}
@inproceedings{pmlr-v119-wang20k,
	title        = {Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere},
	author       = {Wang, Tongzhou and Isola, Phillip},
	year         = 2020,
	month        = {13--18 Jul},
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = 119,
	pages        = {9929--9939},
	url          = {https://proceedings.mlr.press/v119/wang20k.html},
	editor       = {III, Hal Daumé and Singh, Aarti},
	pdf          = {http://proceedings.mlr.press/v119/wang20k/wang20k.pdf},
	abstract     = {Contrastive representation learning has been outstandingly successful in practice. In this work, we identify two key properties related to the contrastive loss: (1) alignment (closeness) of features from positive pairs, and (2) uniformity of the induced distribution of the (normalized) features on the hypersphere. We prove that, asymptotically, the contrastive loss optimizes these properties, and analyze their positive effects on downstream tasks. Empirically, we introduce an optimizable metric to quantify each property. Extensive experiments on standard vision and language datasets confirm the strong agreement between both metrics and downstream task performance. Directly optimizing for these two metrics leads to representations with comparable or better performance at downstream tasks than contrastive learning.}
}
@inproceedings{yu-etal-2022-rare,
	title        = {Rare Tokens Degenerate All Tokens: Improving Neural Text Generation via Adaptive Gradient Gating for Rare Token Embeddings},
	author       = {Yu, Sangwon  and Song, Jongyoon  and Kim, Heeseung  and Lee, Seongmin  and Ryu, Woo-Jong  and Yoon, Sungroh},
	year         = 2022,
	month        = may,
	booktitle    = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Dublin, Ireland},
	pages        = {29--45},
	doi          = {10.18653/v1/2022.acl-long.3},
	url          = {https://aclanthology.org/2022.acl-long.3},
	editor       = {Muresan, Smaranda  and Nakov, Preslav  and Villavicencio, Aline},
	abstract     = {Recent studies have determined that the learned token embeddings of large-scale neural language models are degenerated to be anisotropic with a narrow-cone shape. This phenomenon, called the representation degeneration problem, facilitates an increase in the overall similarity between token embeddings that negatively affect the performance of the models. Although the existing methods that address the degeneration problem based on observations of the phenomenon triggered by the problem improves the performance of the text generation, the training dynamics of token embeddings behind the degeneration problem are still not explored. In this study, we analyze the training dynamics of the token embeddings focusing on rare token embedding. We demonstrate that the specific part of the gradient for rare token embeddings is the key cause of the degeneration problem for all tokens during training stage. Based on the analysis, we propose a novel method called, adaptive gradient gating(AGG). AGG addresses the degeneration problem by gating the specific part of the gradient for rare token embeddings. Experimental results from language modeling, word similarity, and machine translation tasks quantitatively and qualitatively verify the effectiveness of AGG.}
}
@inproceedings{rajaee-pilehvar-2021-cluster,
	title        = {A Cluster-based Approach for Improving Isotropy in Contextual Embedding Space},
	author       = {Rajaee, Sara  and Pilehvar, Mohammad Taher},
	year         = 2021,
	month        = aug,
	booktitle    = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {575--584},
	doi          = {10.18653/v1/2021.acl-short.73},
	url          = {https://aclanthology.org/2021.acl-short.73},
	editor       = {Zong, Chengqing  and Xia, Fei  and Li, Wenjie  and Navigli, Roberto},
	abstract     = {The representation degeneration problem in Contextual Word Representations (CWRs) hurts the expressiveness of the embedding space by forming an anisotropic cone where even unrelated words have excessively positive correlations. Existing techniques for tackling this issue require a learning process to re-train models with additional objectives and mostly employ a global assessment to study isotropy. Our quantitative analysis over isotropy shows that a local assessment could be more accurate due to the clustered structure of CWRs. Based on this observation, we propose a local cluster-based method to address the degeneration issue in contextual embedding spaces. We show that in clusters including punctuations and stop words, local dominant directions encode structural information, removing which can improve CWRs performance on semantic tasks. Moreover, we find that tense information in verb representations dominates sense semantics. We show that removing dominant directions of verb representations can transform the space to better suit semantic applications. Our experiments demonstrate that the proposed cluster-based method can mitigate the degeneration problem on multiple tasks.}
}
@inproceedings{puccetti-etal-2022-outlier,
	title        = {Outlier Dimensions that Disrupt Transformers are Driven by Frequency},
	author       = {Puccetti, Giovanni  and Rogers, Anna  and Drozd, Aleksandr  and Dell{'}Orletta, Felice},
	year         = 2022,
	month        = dec,
	booktitle    = {Findings of the Association for Computational Linguistics: EMNLP 2022},
	publisher    = {Association for Computational Linguistics},
	address      = {Abu Dhabi, United Arab Emirates},
	pages        = {1286--1304},
	doi          = {10.18653/v1/2022.findings-emnlp.93},
	url          = {https://aclanthology.org/2022.findings-emnlp.93},
	editor       = {Goldberg, Yoav  and Kozareva, Zornitsa  and Zhang, Yue},
	abstract     = {While Transformer-based language models are generally very robust to pruning, there is the recently discovered outlier phenomenon: disabling only 48 out of 110M parameters in BERT-base drops its performance by nearly 30{\%} on MNLI. We replicate the original evidence for the outlier phenomenon and we link it to the geometry of the embedding space. We find that in both BERT and RoBERTa the magnitude of hidden state coefficients corresponding to outlier dimensions correlate with the frequencies of encoded tokens in pre-training data, and they also contribute to the {``}vertical{''} self-attention pattern enabling the model to focus on the special tokens. This explains the drop in performance from disabling the outliers, and it suggests that to decrease anisotopicity in future models we need pre-training schemas that would better take into account the skewed token distributions.}
}
@inproceedings{Wang2020Improving,
	title        = {Improving Neural Language Generation with Spectrum Control},
	author       = {Lingxiao Wang and Jing Huang and Kevin Huang and Ziniu Hu and Guangtao Wang and Quanquan Gu},
	year         = 2020,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=ByxY8CNtvr}
}
@inproceedings{gao-etal-2021-simcse,
	title        = {{S}im{CSE}: Simple Contrastive Learning of Sentence Embeddings},
	author       = {Gao, Tianyu  and Yao, Xingcheng  and Chen, Danqi},
	year         = 2021,
	month        = nov,
	booktitle    = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	publisher    = {Association for Computational Linguistics},
	address      = {Online and Punta Cana, Dominican Republic},
	pages        = {6894--6910},
	doi          = {10.18653/v1/2021.emnlp-main.552},
	url          = {https://aclanthology.org/2021.emnlp-main.552},
	editor       = {Moens, Marie-Francine  and Huang, Xuanjing  and Specia, Lucia  and Yih, Scott Wen-tau},
	abstract     = {This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework, by using {``}entailment{''} pairs as positives and {``}contradiction{''} pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3{\%} and 81.6{\%} Spearman{'}s correlation respectively, a 4.2{\%} and 2.2{\%} improvement compared to previous best results. We also show{---}both theoretically and empirically{---}that contrastive learning objective regularizes pre-trained embeddings{'} anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available.}
}
@inproceedings{yan-etal-2021-consert,
	title        = {{C}on{SERT}: A Contrastive Framework for Self-Supervised Sentence Representation Transfer},
	author       = {Yan, Yuanmeng  and Li, Rumei  and Wang, Sirui  and Zhang, Fuzheng  and Wu, Wei  and Xu, Weiran},
	year         = 2021,
	month        = aug,
	booktitle    = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {5065--5075},
	doi          = {10.18653/v1/2021.acl-long.393},
	url          = {https://aclanthology.org/2021.acl-long.393},
	editor       = {Zong, Chengqing  and Xia, Fei  and Li, Wenjie  and Navigli, Roberto},
	abstract     = {Learning high-quality sentence representations benefits a wide range of natural language processing tasks. Though BERT-based pre-trained language models achieve high performance on many downstream tasks, the native derived sentence representations are proved to be collapsed and thus produce a poor performance on the semantic textual similarity (STS) tasks. In this paper, we present ConSERT, a Contrastive Framework for Self-Supervised SEntence Representation Transfer, that adopts contrastive learning to fine-tune BERT in an unsupervised and effective way. By making use of unlabeled texts, ConSERT solves the collapse issue of BERT-derived sentence representations and make them more applicable for downstream tasks. Experiments on STS datasets demonstrate that ConSERT achieves an 8{\%} relative improvement over the previous state-of-the-art, even comparable to the supervised SBERT-NLI. And when further incorporating NLI supervision, we achieve new state-of-the-art performance on STS tasks. Moreover, ConSERT obtains comparable results with only 1000 samples available, showing its robustness in data scarcity scenarios.}
}
@inproceedings{rajaee-pilehvar-2022-isotropy,
	title        = {An Isotropy Analysis in the Multilingual {BERT} Embedding Space},
	author       = {Rajaee, Sara  and Pilehvar, Mohammad Taher},
	year         = 2022,
	month        = may,
	booktitle    = {Findings of the Association for Computational Linguistics: ACL 2022},
	publisher    = {Association for Computational Linguistics},
	address      = {Dublin, Ireland},
	pages        = {1309--1316},
	doi          = {10.18653/v1/2022.findings-acl.103},
	url          = {https://aclanthology.org/2022.findings-acl.103},
	editor       = {Muresan, Smaranda  and Nakov, Preslav  and Villavicencio, Aline},
	abstract     = {Several studies have explored various advantages of multilingual pre-trained models (such as multilingual BERT) in capturing shared linguistic knowledge. However, less attention has been paid to their limitations. In this paper, we investigate the multilingual BERT for two known issues of the monolingual models: anisotropic embedding space and outlier dimensions. We show that, unlike its monolingual counterpart, the multilingual BERT model exhibits no outlier dimension in its representations while it has a highly anisotropic space. There are a few dimensions in the monolingual BERT with high contributions to the anisotropic distribution. However, we observe no such dimensions in the multilingual BERT. Furthermore, our experimental results demonstrate that increasing the isotropy of multilingual space can significantly improve its representation power and performance, similarly to what had been observed for monolingual CWRs on semantic similarity tasks. Our analysis indicates that, despite having different degenerated directions, the embedding spaces in various languages tend to be partially similar with respect to their structures.}
}
@inproceedings{el-boukkouri-etal-2020-characterbert,
	title        = {{C}haracter{BERT}: Reconciling {ELM}o and {BERT} for Word-Level Open-Vocabulary Representations From Characters},
	author       = {El Boukkouri, Hicham  and Ferret, Olivier  and Lavergne, Thomas  and Noji, Hiroshi  and Zweigenbaum, Pierre  and Tsujii, Jun{'}ichi},
	year         = 2020,
	month        = dec,
	booktitle    = {Proceedings of the 28th International Conference on Computational Linguistics},
	publisher    = {International Committee on Computational Linguistics},
	address      = {Barcelona, Spain (Online)},
	pages        = {6903--6915},
	doi          = {10.18653/v1/2020.coling-main.609},
	url          = {https://aclanthology.org/2020.coling-main.609},
	editor       = {Scott, Donia  and Bel, Nuria  and Zong, Chengqing},
	abstract     = {Due to the compelling improvements brought by BERT, many recent representation models adopted the Transformer architecture as their main building block, consequently inheriting the wordpiece tokenization system despite it not being intrinsically linked to the notion of Transformers. While this system is thought to achieve a good balance between the flexibility of characters and the efficiency of full words, using predefined wordpiece vocabularies from the general domain is not always suitable, especially when building models for specialized domains (e.g., the medical domain). Moreover, adopting a wordpiece tokenization shifts the focus from the word level to the subword level, making the models conceptually more complex and arguably less convenient in practice. For these reasons, we propose CharacterBERT, a new variant of BERT that drops the wordpiece system altogether and uses a Character-CNN module instead to represent entire words by consulting their characters. We show that this new model improves the performance of BERT on a variety of medical domain tasks while at the same time producing robust, word-level, and open-vocabulary representations.}
}
@article{clark-etal-2022-canine,
	title        = {Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation},
	author       = {Clark, Jonathan H.  and Garrette, Dan  and Turc, Iulia  and Wieting, John},
	year         = 2022,
	journal      = {Transactions of the Association for Computational Linguistics},
	publisher    = {MIT Press},
	address      = {Cambridge, MA},
	volume       = 10,
	pages        = {73--91},
	doi          = {10.1162/tacl_a_00448},
	url          = {https://aclanthology.org/2022.tacl-1.5},
	editor       = {Roark, Brian  and Nenkova, Ani},
	abstract     = {Pipelined NLP systems have largely been superseded by end-to-end neural modeling, yet nearly all commonly used models still require an explicit tokenization step. While recent tokenization approaches based on data-derived subword lexicons are less brittle than manually engineered tokenizers, these techniques are not equally suited to all languages, and the use of any fixed vocabulary may limit a model{'}s ability to adapt. In this paper, we present Canine, a neural encoder that operates directly on character sequences{---}without explicit tokenization or vocabulary{---}and a pre-training strategy that operates either directly on characters or optionally uses subwords as a soft inductive bias. To use its finer-grained input effectively and efficiently, Canine combines downsampling, which reduces the input sequence length, with a deep transformer stack, which encodes context. Canine outperforms a comparable mBert model by 5.7 F1 on TyDi QA, a challenging multilingual benchmark, despite having fewer model parameters.}
}
@inproceedings{godey-etal-2022-manta,
	title        = {{MANT}a: Efficient Gradient-Based Tokenization for End-to-End Robust Language Modeling},
	author       = {Godey, Nathan  and Castagn{\'e}, Roman  and de la Clergerie, {\'E}ric  and Sagot, Beno{\^\i}t},
	year         = 2022,
	month        = dec,
	booktitle    = {Findings of the Association for Computational Linguistics: EMNLP 2022},
	publisher    = {Association for Computational Linguistics},
	address      = {Abu Dhabi, United Arab Emirates},
	pages        = {2859--2870},
	doi          = {10.18653/v1/2022.findings-emnlp.207},
	url          = {https://aclanthology.org/2022.findings-emnlp.207},
	editor       = {Goldberg, Yoav  and Kozareva, Zornitsa  and Zhang, Yue},
	abstract     = {Static subword tokenization algorithms have been an essential component of recent works on language modeling. However, their static nature results in important flaws that degrade the models{'} downstream performance and robustness. In this work, we propose MANTa, a Module for Adaptive Neural TokenizAtion. MANTa is a differentiable tokenizer trained end-to-end with the language model. The resulting system offers a trade-off between the expressiveness of byte-level models and the speed of models trained using subword tokenization. In addition, our tokenizer is highly explainable since it produces an explicit segmentation of sequences into blocks. We evaluate our pre-trained model on several English datasets from different domains as well as on synthetic noise. We find that MANTa improves robustness to character perturbations and out-of-domain data. We then show that MANTa performs comparably to other models on the general-domain GLUE benchmark. Finally, we show that it is considerably faster than strictly byte-level models.}
}
@inproceedings{wang-etal-2018-glue,
	title        = {{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
	author       = {Wang, Alex  and Singh, Amanpreet  and Michael, Julian  and Hill, Felix  and Levy, Omer  and Bowman, Samuel},
	year         = 2018,
	month        = nov,
	booktitle    = {Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}},
	publisher    = {Association for Computational Linguistics},
	address      = {Brussels, Belgium},
	pages        = {353--355},
	doi          = {10.18653/v1/W18-5446},
	url          = {https://aclanthology.org/W18-5446},
	editor       = {Linzen, Tal  and Chrupa{\l}a, Grzegorz  and Alishahi, Afra},
	abstract     = {Human ability to understand language is \textit{general, flexible, and robust}. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.}
}
@inproceedings{bookcorpus,
	title        = {Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books},
	author       = {Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
	year         = 2015,
	month        = {December},
	booktitle    = {The IEEE International Conference on Computer Vision (ICCV)}
}
@article{2020t5,
	title        = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
	author       = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
	year         = 2020,
	journal      = {Journal of Machine Learning Research},
	volume       = 21,
	number       = 140,
	pages        = {1--67},
	url          = {http://jmlr.org/papers/v21/20-074.html}
}
@article{xue-etal-2022-byt5,
	title        = {{B}y{T}5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models},
	author       = {Xue, Linting  and Barua, Aditya  and Constant, Noah  and Al-Rfou, Rami  and Narang, Sharan  and Kale, Mihir  and Roberts, Adam  and Raffel, Colin},
	year         = 2022,
	journal      = {Transactions of the Association for Computational Linguistics},
	publisher    = {MIT Press},
	address      = {Cambridge, MA},
	volume       = 10,
	pages        = {291--306},
	doi          = {10.1162/tacl_a_00461},
	url          = {https://aclanthology.org/2022.tacl-1.17},
	editor       = {Roark, Brian  and Nenkova, Ani},
	abstract     = {Most widely used pre-trained language models operate on sequences of tokens corresponding to word or subword units. By comparison, token-free models that operate directly on raw text (bytes or characters) have many benefits: They can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines. Because byte or character sequences are longer than token sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.1}
}
@inproceedings{merity2017pointer,
	title        = {Pointer Sentinel Mixture Models},
	author       = {Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
	year         = 2017,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=Byj72udxe}
}
@inproceedings{gupta-etal-2015-distributional,
	title        = {Distributional vectors encode referential attributes},
	author       = {Gupta, Abhijeet  and Boleda, Gemma  and Baroni, Marco  and Pad{\'o}, Sebastian},
	year         = 2015,
	month        = sep,
	booktitle    = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
	publisher    = {Association for Computational Linguistics},
	address      = {Lisbon, Portugal},
	pages        = {12--21},
	doi          = {10.18653/v1/D15-1002},
	url          = {https://aclanthology.org/D15-1002},
	editor       = {M{\`a}rquez, Llu{\'\i}s  and Callison-Burch, Chris  and Su, Jian}
}
@inproceedings{kohn-2015-whats,
	title        = {What{'}s in an Embedding? Analyzing Word Embeddings through Multilingual Evaluation},
	author       = {K{\"o}hn, Arne},
	year         = 2015,
	month        = sep,
	booktitle    = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
	publisher    = {Association for Computational Linguistics},
	address      = {Lisbon, Portugal},
	pages        = {2067--2073},
	doi          = {10.18653/v1/D15-1246},
	url          = {https://aclanthology.org/D15-1246},
	editor       = {M{\`a}rquez, Llu{\'\i}s  and Callison-Burch, Chris  and Su, Jian}
}
@inproceedings{shi-etal-2016-string,
	title        = {Does String-Based Neural {MT} Learn Source Syntax?},
	author       = {Shi, Xing  and Padhi, Inkit  and Knight, Kevin},
	year         = 2016,
	month        = nov,
	booktitle    = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
	publisher    = {Association for Computational Linguistics},
	address      = {Austin, Texas},
	pages        = {1526--1534},
	doi          = {10.18653/v1/D16-1159},
	url          = {https://aclanthology.org/D16-1159},
	editor       = {Su, Jian  and Duh, Kevin  and Carreras, Xavier}
}
@inproceedings{ijcai2018p796,
	title        = {Visualisation and 'Diagnostic Classifiers' Reveal how Recurrent and Recursive Neural Networks Process Hierarchical Structure (Extended Abstract)},
	author       = {Dieuwke Hupkes and Willem Zuidema},
	year         = 2018,
	month        = 7,
	booktitle    = {Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, {IJCAI-18}},
	publisher    = {International Joint Conferences on Artificial Intelligence Organization},
	pages        = {5617--5621},
	doi          = {10.24963/ijcai.2018/796},
	url          = {https://doi.org/10.24963/ijcai.2018/796}
}
@inproceedings{conneau-etal-2018-cram,
	title        = {What you can cram into a single {\$}{\&}!{\#}* vector: Probing sentence embeddings for linguistic properties},
	author       = {Conneau, Alexis  and Kruszewski, German  and Lample, Guillaume  and Barrault, Lo{\"\i}c  and Baroni, Marco},
	year         = 2018,
	month        = jul,
	booktitle    = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Melbourne, Australia},
	pages        = {2126--2136},
	doi          = {10.18653/v1/P18-1198},
	url          = {https://aclanthology.org/P18-1198},
	editor       = {Gurevych, Iryna  and Miyao, Yusuke},
	abstract     = {Although much effort has recently been devoted to training high-quality sentence embeddings, we still have a poor understanding of what they are capturing. {``}Downstream{''} tasks, often based on sentence classification, are commonly used to evaluate the quality of sentence representations. The complexity of the tasks makes it however difficult to infer what kind of information is present in the representations. We introduce here 10 probing tasks designed to capture simple linguistic features of sentences, and we use them to study embeddings generated by three different encoders trained in eight distinct ways, uncovering intriguing properties of both encoders and training methods.}
}
@misc{su2021whiteningsentencerepresentationsbetter,
	title        = {Whitening Sentence Representations for Better Semantics and Faster Retrieval},
	author       = {Jianlin Su and Jiarun Cao and Weijie Liu and Yangyiwen Ou},
	year         = 2021,
	url          = {https://arxiv.org/abs/2103.15316},
	eprint       = {2103.15316},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@misc{wav2vec,
	title        = {wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations},
	author       = {Alexei Baevski and Henry Zhou and Abdelrahman Mohamed and Michael Auli},
	year         = 2020,
	url          = {https://arxiv.org/abs/2006.11477},
	eprint       = {2006.11477},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@inproceedings{thukral-etal-2021-probing,
	title        = {Probing Language Models for Understanding of Temporal Expressions},
	author       = {Thukral, Shivin  and Kukreja, Kunal  and Kavouras, Christian},
	year         = 2021,
	month        = nov,
	booktitle    = {Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},
	publisher    = {Association for Computational Linguistics},
	address      = {Punta Cana, Dominican Republic},
	pages        = {396--406},
	doi          = {10.18653/v1/2021.blackboxnlp-1.31},
	url          = {https://aclanthology.org/2021.blackboxnlp-1.31},
	editor       = {Bastings, Jasmijn  and Belinkov, Yonatan  and Dupoux, Emmanuel  and Giulianelli, Mario  and Hupkes, Dieuwke  and Pinter, Yuval  and Sajjad, Hassan},
	abstract     = {We present three Natural Language Inference (NLI) challenge sets that can evaluate NLI models on their understanding of temporal expressions. More specifically, we probe these models for three temporal properties: (a) the order between points in time, (b) the duration between two points in time, (c) the relation between the magnitude of times specified in different units. We find that although large language models fine-tuned on MNLI have some basic perception of the order between points in time, at large, these models do not have a thorough understanding of the relation between temporal expressions.}
}
@misc{ngo2024languagemodelshearprobing,
	title        = {What Do Language Models Hear? Probing for Auditory Representations in Language Models},
	author       = {Jerry Ngo and Yoon Kim},
	year         = 2024,
	url          = {https://arxiv.org/abs/2402.16998},
	eprint       = {2402.16998},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@inproceedings{zhao-etal-2018-learning,
	title        = {Learning Gender-Neutral Word Embeddings},
	author       = {Zhao, Jieyu  and Zhou, Yichao  and Li, Zeyu  and Wang, Wei  and Chang, Kai-Wei},
	year         = 2018,
	month        = oct # {-} # nov,
	booktitle    = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	publisher    = {Association for Computational Linguistics},
	address      = {Brussels, Belgium},
	pages        = {4847--4853},
	doi          = {10.18653/v1/D18-1521},
	url          = {https://aclanthology.org/D18-1521},
	editor       = {Riloff, Ellen  and Chiang, David  and Hockenmaier, Julia  and Tsujii, Jun{'}ichi},
	abstract     = {Word embedding models have become a fundamental component in a wide range of Natural Language Processing (NLP) applications. However, embeddings trained on human-generated corpora have been demonstrated to inherit strong gender stereotypes that reflect social constructs. To address this concern, in this paper, we propose a novel training procedure for learning gender-neutral word embeddings. Our approach aims to preserve gender information in certain dimensions of word vectors while compelling other dimensions to be free of gender influence. Based on the proposed method, we generate a Gender-Neutral variant of GloVe (GN-GloVe). Quantitative and qualitative experiments demonstrate that GN-GloVe successfully isolates gender information without sacrificing the functionality of the embedding model.}
}
@inproceedings{ravfogel-etal-2020-null,
	title        = {Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection},
	author       = {Ravfogel, Shauli  and Elazar, Yanai  and Gonen, Hila  and Twiton, Michael  and Goldberg, Yoav},
	year         = 2020,
	month        = jul,
	booktitle    = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {7237--7256},
	doi          = {10.18653/v1/2020.acl-main.647},
	url          = {https://aclanthology.org/2020.acl-main.647},
	editor       = {Jurafsky, Dan  and Chai, Joyce  and Schluter, Natalie  and Tetreault, Joel},
	abstract     = {The ability to control for the kinds of information encoded in neural representation has a variety of use cases, especially in light of the challenge of interpreting these models. We present Iterative Null-space Projection (INLP), a novel method for removing information from neural representations. Our method is based on repeated training of linear classifiers that predict a certain property we aim to remove, followed by projection of the representations on their null-space. By doing so, the classifiers become oblivious to that target property, making it hard to linearly separate the data according to it. While applicable for multiple uses, we evaluate our method on bias and fairness use-cases, and show that our method is able to mitigate bias in word embeddings, as well as to increase fairness in a setting of multi-class classification.}
}
@inproceedings{peters-etal-2019-knowledge,
	title        = {Knowledge Enhanced Contextual Word Representations},
	author       = {Peters, Matthew E.  and Neumann, Mark  and Logan, Robert  and Schwartz, Roy  and Joshi, Vidur  and Singh, Sameer  and Smith, Noah A.},
	year         = 2019,
	month        = nov,
	booktitle    = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
	publisher    = {Association for Computational Linguistics},
	address      = {Hong Kong, China},
	pages        = {43--54},
	doi          = {10.18653/v1/D19-1005},
	url          = {https://aclanthology.org/D19-1005},
	editor       = {Inui, Kentaro  and Jiang, Jing  and Ng, Vincent  and Wan, Xiaojun},
	abstract     = {Contextual word representations, typically trained on unstructured, unlabeled text, do not contain any explicit grounding to real world entities and are often unable to remember facts about those entities. We propose a general method to embed multiple knowledge bases (KBs) into large scale models, and thereby enhance their representations with structured, human-curated knowledge. For each KB, we first use an integrated entity linker to retrieve relevant entity embeddings, then update contextual word representations via a form of word-to-entity attention. In contrast to previous approaches, the entity linkers and self-supervised language modeling objective are jointly trained end-to-end in a multitask setting that combines a small amount of entity linking supervision with a large amount of raw text. After integrating WordNet and a subset of Wikipedia into BERT, the knowledge enhanced BERT (KnowBert) demonstrates improved perplexity, ability to recall facts as measured in a probing task and downstream performance on relationship extraction, entity typing, and word sense disambiguation. KnowBert{'}s runtime is comparable to BERT{'}s and it scales to large KBs.}
}
@inproceedings{youssef-etal-2023-give,
	title        = {Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models},
	author       = {Youssef, Paul  and Kora{\c{s}}, Osman  and Li, Meijie  and Schl{\"o}tterer, J{\"o}rg  and Seifert, Christin},
	year         = 2023,
	month        = dec,
	booktitle    = {Findings of the Association for Computational Linguistics: EMNLP 2023},
	publisher    = {Association for Computational Linguistics},
	address      = {Singapore},
	pages        = {15588--15605},
	doi          = {10.18653/v1/2023.findings-emnlp.1043},
	url          = {https://aclanthology.org/2023.findings-emnlp.1043},
	editor       = {Bouamor, Houda  and Pino, Juan  and Bali, Kalika},
	abstract     = {Pre-trained Language Models (PLMs) are trained on vast unlabeled data, rich in world knowledge. This fact has sparked the interest of the community in quantifying the amount of factual knowledge present in PLMs, as this explains their performance on downstream tasks, and potentially justifies their use as knowledge bases. In this work, we survey methods and datasets that are used to probe PLMs for factual knowledge. Our contributions are: (1) We propose a categorization scheme for factual probing methods that is based on how their inputs, outputs and the probed PLMs are adapted; (2) We provide an overview of the datasets used for factual probing; (3) We synthesize insights about knowledge retention and prompt optimization in PLMs, analyze obstacles to adopting PLMs as knowledge bases and outline directions for future work.}
}
@inproceedings{caselli-etal-2022-time,
	title        = {How about Time? Probing a Multilingual Language Model for Temporal Relations},
	author       = {Caselli, Tommaso  and Dini, Irene  and Dell{'}Orletta, Felice},
	year         = 2022,
	month        = oct,
	booktitle    = {Proceedings of the 29th International Conference on Computational Linguistics},
	publisher    = {International Committee on Computational Linguistics},
	address      = {Gyeongju, Republic of Korea},
	pages        = {3197--3209},
	url          = {https://aclanthology.org/2022.coling-1.283},
	editor       = {Calzolari, Nicoletta  and Huang, Chu-Ren  and Kim, Hansaem  and Pustejovsky, James  and Wanner, Leo  and Choi, Key-Sun  and Ryu, Pum-Mo  and Chen, Hsin-Hsi  and Donatelli, Lucia  and Ji, Heng  and Kurohashi, Sadao  and Paggio, Patrizia  and Xue, Nianwen  and Kim, Seokhwan  and Hahm, Younggyun  and He, Zhong  and Lee, Tony Kyungil  and Santus, Enrico  and Bond, Francis  and Na, Seung-Hoon},
	abstract     = {This paper presents a comprehensive set of probing experiments using a multilingual language model, XLM-R, for temporal relation classification between events in four languages. Results show an advantage of contextualized embeddings over static ones and a detrimen- tal role of sentence level embeddings. While obtaining competitive results against state-of-the-art systems, our probes indicate a lack of suitable encoded information to properly address this task.}
}
@article{HuBERT,
	title        = {HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units},
	author       = {Hsu, Wei-Ning and Bolte, Benjamin and Tsai, Yao-Hung Hubert and Lakhotia, Kushal and Salakhutdinov, Ruslan and Mohamed, Abdelrahman},
	year         = 2021,
	month        = {oct},
	journal      = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
	publisher    = {IEEE Press},
	volume       = 29,
	pages        = {3451–3460},
	doi          = {10.1109/TASLP.2021.3122291},
	issn         = {2329-9290},
	url          = {https://doi.org/10.1109/TASLP.2021.3122291},
	issue_date   = 2021,
	abstract     = {Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960 h) and Libri-light (60,000 h) benchmarks with 10 min, 1 h, 10 h, 100 h, and 960 h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.<xref ref-type="fn" rid="fn1"><sup>1</sup></xref><xref ref-type="fn" rid="fn2"><sup>2</sup></xref>},
	numpages     = 10
}
@inproceedings{radford2022whisper,
	title        = {Robust Speech Recognition via Large-Scale Weak Supervision},
	author       = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and Mcleavey, Christine and Sutskever, Ilya},
	year         = 2023,
	month        = {23--29 Jul},
	booktitle    = {Proceedings of the 40th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = 202,
	pages        = {28492--28518},
	url          = {https://proceedings.mlr.press/v202/radford23a.html},
	editor       = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
	pdf          = {https://proceedings.mlr.press/v202/radford23a/radford23a.pdf},
	abstract     = {We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results without the need for any dataset specific fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.}
}
@inproceedings{commonvoice:2020,
	title        = {Common Voice: A Massively-Multilingual Speech Corpus},
	author       = {Ardila, Rosana  and Branson, Megan  and Davis, Kelly  and Kohler, Michael  and Meyer, Josh  and Henretty, Michael  and Morais, Reuben  and Saunders, Lindsay  and Tyers, Francis  and Weber, Gregor},
	year         = 2020,
	month        = may,
	booktitle    = {Proceedings of the Twelfth Language Resources and Evaluation Conference},
	publisher    = {European Language Resources Association},
	address      = {Marseille, France},
	pages        = {4218--4222},
	isbn         = {979-10-95546-34-4},
	url          = {https://aclanthology.org/2020.lrec-1.520},
	editor       = {Calzolari, Nicoletta  and B{\'e}chet, Fr{\'e}d{\'e}ric  and Blache, Philippe  and Choukri, Khalid  and Cieri, Christopher  and Declerck, Thierry  and Goggi, Sara  and Isahara, Hitoshi  and Maegaard, Bente  and Mariani, Joseph  and Mazo, H{\'e}l{\`e}ne  and Moreno, Asuncion  and Odijk, Jan  and Piperidis, Stelios},
	abstract     = {The Common Voice corpus is a massively-multilingual collection of transcribed speech intended for speech technology research and development. Common Voice is designed for Automatic Speech Recognition purposes but can be useful in other domains (e.g. language identification). To achieve scale and sustainability, the Common Voice project employs crowdsourcing for both data collection and data validation. The most recent release includes 29 languages, and as of November 2019 there are a total of 38 languages collecting data. Over 50,000 individuals have participated so far, resulting in 2,500 hours of collected audio. To our knowledge this is the largest audio corpus in the public domain for speech recognition, both in terms of number of hours and number of languages. As an example use case for Common Voice, we present speech recognition experiments using Mozilla{'}s DeepSpeech Speech-to-Text toolkit. By applying transfer learning from a source English model, we find an average Character Error Rate improvement of 5.99 {\mbox{$\pm$}} 5.48 for twelve target languages (German, French, Italian, Turkish, Catalan, Slovenian, Welsh, Irish, Breton, Tatar, Chuvash, and Kabyle). For most of these languages, these are the first ever published results on end-to-end Automatic Speech Recognition.},
	language     = {English}
}
@misc{Wu2020VisualTT,
	title        = {Visual Transformers: Token-based Image Representation and Processing for Computer Vision},
	author       = {Bichen Wu and Chenfeng Xu and Xiaoliang Dai and Alvin Wan and Peizhao Zhang and Zhicheng Yan and Masayoshi Tomizuka and Joseph Gonzalez and Kurt Keutzer and Peter Vajda},
	year         = 2020,
	url          = {https://arxiv.org/abs/2006.03677},
	eprint       = {2006.03677},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}
@inproceedings{beit-2021,
	title        = {{BE}iT: {BERT} Pre-Training of Image Transformers},
	author       = {Hangbo Bao and Li Dong and Songhao Piao and Furu Wei},
	year         = 2022,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=p-BhZSz59o4}
}
@inproceedings{segformer21,
	title        = {SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers},
	author       = {Enze Xie and Wenhai Wang and Zhiding Yu and Anima Anandkumar and Jose M. Alvarez and Ping Luo},
	year         = 2021,
	booktitle    = {Advances in Neural Information Processing Systems},
	url          = {https://openreview.net/forum?id=OG18MI5TRL},
	editor       = {A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan}
}
@inproceedings{pmlr-v139-touvron21a,
	title        = {Training data-efficient image transformers and distillation through attention},
	author       = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jegou, Herve},
	year         = 2021,
	month        = {18--24 Jul},
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = 139,
	pages        = {10347--10357},
	url          = {https://proceedings.mlr.press/v139/touvron21a.html},
	editor       = {Meila, Marina and Zhang, Tong},
	pdf          = {http://proceedings.mlr.press/v139/touvron21a/touvron21a.pdf},
	abstract     = {Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. These high-performing vision transformers are pre-trained with hundreds of millions of images using a large infrastructure, thereby limiting their adoption. In this work, we produce competitive convolution-free transformers trained on ImageNet only using a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop) on ImageNet with no external data. We also introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention, typically from a convnet teacher. The learned transformers are competitive (85.2% top-1 acc.) with the state of the art on ImageNet, and similarly when transferred to other tasks. We will share our code and models.}
}
@article{imagenet15russakovsky,
	title        = {ImageNet Large Scale Visual Recognition Challenge},
	author       = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
	year         = 2015,
	journal      = {International Journal of Computer Vision},
	volume       = 115,
	number       = 3,
	pages        = {211--252},
	doi          = {10.1007/s11263-015-0816-y},
	isbn         = {1573-1405},
	url          = {https://doi.org/10.1007/s11263-015-0816-y},
	abstract     = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
	date         = {2015/12/01},
	date-added   = {2024-07-25 11:25:58 +0200},
	date-modified = {2024-07-25 11:25:58 +0200},
	id           = {Russakovsky2015},
	bdsk-url-1   = {https://doi.org/10.1007/s11263-015-0816-y}
}
@inproceedings{he2016deep,
	title        = {Deep Residual Learning for Image Recognition},
	author       = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year         = 2016,
	booktitle    = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	volume       = {},
	number       = {},
	pages        = {770--778},
	doi          = {10.1109/CVPR.2016.90},
	keywords     = {Training;Degradation;Complexity theory;Image recognition;Neural networks;Visualization;Image segmentation}
}
@inproceedings{Tan2019EfficientNetRM,
	title        = {{E}fficient{N}et: Rethinking Model Scaling for Convolutional Neural Networks},
	author       = {Tan, Mingxing and Le, Quoc},
	year         = 2019,
	month        = {09--15 Jun},
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = 97,
	pages        = {6105--6114},
	url          = {https://proceedings.mlr.press/v97/tan19a.html},
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	pdf          = {http://proceedings.mlr.press/v97/tan19a/tan19a.pdf},
	abstract     = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are given. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves stateof-the-art 84.4% top-1 / 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet (Huang et al., 2018). Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flower (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.}
}
@inproceedings{wu2021cvt,
	title        = {CvT: Introducing Convolutions to Vision Transformers},
	author       = {Wu, Haiping and Xiao, Bin and Codella, Noel and Liu, Mengchen and Dai, Xiyang and Yuan, Lu and Zhang, Lei},
	year         = 2021,
	booktitle    = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
	volume       = {},
	number       = {},
	pages        = {22--31},
	doi          = {10.1109/ICCV48922.2021.00009},
	keywords     = {Convolutional codes;Image resolution;Image recognition;Computer architecture;Performance gain;Transformers;Distortion;Recognition and classification}
}
@inproceedings{liu2022convnet,
	title        = {A ConvNet for the 2020s},
	author       = {Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
	year         = 2022,
	month        = {June},
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	pages        = {11976--11986}
}
@article{guo2022visual,
	title        = {Visual attention network},
	author       = {Guo, Meng-Hao and Lu, Cheng-Ze and Liu, Zheng-Ning and Cheng, Ming-Ming and Hu, Shi-Min},
	year         = 2023,
	journal      = {Computational Visual Media},
	volume       = 9,
	number       = 4,
	pages        = {733--752},
	doi          = {10.1007/s41095-023-0364-2},
	isbn         = {2096-0662},
	url          = {https://doi.org/10.1007/s41095-023-0364-2},
	abstract     = {While originally designed for natural language processing tasks, the self-attention mechanism has recently taken various computer vision areas by storm. However, the 2D nature of images brings three challenges for applying self-attention in computer vision: (1) treating images as 1D sequences neglects their 2D structures; (2) the quadratic complexity is too expensive for high-resolution images; (3) it only captures spatial adaptability but ignores channel adaptability. In this paper, we propose a novel linear attention named large kernel attention (LKA) to enable self-adaptive and long-range correlations in self-attention while avoiding its shortcomings. Furthermore, we present a neural network based on LKA, namely Visual Attention Network (VAN). While extremely simple, VAN achieves comparable results with similar size convolutional neural networks (CNNs) and vision transformers (ViTs) in various tasks, including image classification, object detection, semantic segmentation, panoptic segmentation, pose estimation, etc. For example, VAN-B6 achieves 87.8{\%} accuracy on ImageNet benchmark, and sets new state-of-the-art performance (58.2 PQ) for panoptic segmentation. Besides, VAN-B2 surpasses Swin-T 4 mIoU (50.1 vs. 46.1) for semantic segmentation on ADE20K benchmark, 2.6 AP (48.8 vs. 46.2) for object detection on COCO dataset. It provides a novel method and a simple yet strong baseline for the community. The code is available at https://github.com/Visual-Attention-Network.},
	date         = {2023/12/01},
	date-added   = {2024-07-25 14:14:32 +0200},
	date-modified = {2024-07-25 14:14:32 +0200},
	id           = {Guo2023},
	bdsk-url-1   = {https://doi.org/10.1007/s41095-023-0364-2}
}
@inproceedings{pmlr-v37-ioffe15,
	title        = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
	author       = {Ioffe, Sergey and Szegedy, Christian},
	year         = 2015,
	month        = {07--09 Jul},
	booktitle    = {Proceedings of the 32nd International Conference on Machine Learning},
	publisher    = {PMLR},
	address      = {Lille, France},
	series       = {Proceedings of Machine Learning Research},
	volume       = 37,
	pages        = {448--456},
	url          = {https://proceedings.mlr.press/v37/ioffe15.html},
	editor       = {Bach, Francis and Blei, David},
	pdf          = {http://proceedings.mlr.press/v37/ioffe15.pdf},
	abstract     = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.}
}
@inproceedings{devlin-etal-2019-bert,
	title        = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	author       = {Devlin, Jacob  and Chang, Ming-Wei  and Lee, Kenton  and Toutanova, Kristina},
	year         = 2019,
	month        = jun,
	booktitle    = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Minneapolis, Minnesota},
	pages        = {4171--4186},
	doi          = {10.18653/v1/N19-1423},
	url          = {https://aclanthology.org/N19-1423},
	editor       = {Burstein, Jill  and Doran, Christy  and Solorio, Thamar},
	abstract     = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}
}
@misc{roberta,
	title        = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
	author       = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
	year         = 2019,
	url          = {https://arxiv.org/abs/1907.11692},
	eprint       = {1907.11692},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@article{gpt2,
	title        = {Language Models are Unsupervised Multitask Learners},
	author       = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	year         = 2019
}
@inproceedings{clark-etal-2019-bert,
	title        = {What Does {BERT} Look at? An Analysis of {BERT}{'}s Attention},
	author       = {Clark, Kevin  and Khandelwal, Urvashi  and Levy, Omer  and Manning, Christopher D.},
	year         = 2019,
	month        = aug,
	booktitle    = {Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
	publisher    = {Association for Computational Linguistics},
	address      = {Florence, Italy},
	pages        = {276--286},
	doi          = {10.18653/v1/W19-4828},
	url          = {https://aclanthology.org/W19-4828},
	editor       = {Linzen, Tal  and Chrupa{\l}a, Grzegorz  and Belinkov, Yonatan  and Hupkes, Dieuwke},
	abstract     = {Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT{'}s attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT{'}s attention.}
}
@inproceedings{sellam2021multiberts,
	title        = {The Multi{BERT}s: {BERT} Reproductions for Robustness Analysis},
	author       = {Thibault Sellam and Steve Yadlowsky and Ian Tenney and Jason Wei and Naomi Saphra and Alexander D'Amour and Tal Linzen and Jasmijn Bastings and Iulia Raluca Turc and Jacob Eisenstein and Dipanjan Das and Ellie Pavlick},
	year         = 2022,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=K0E_F0gFDgA}
}
@inproceedings{bihani-rayz-2021-low,
	title        = {Low Anisotropy Sense Retrofitting ({LAS}e{R}) : Towards Isotropic and Sense Enriched Representations},
	author       = {Bihani, Geetanjali  and Rayz, Julia},
	year         = 2021,
	month        = jun,
	booktitle    = {Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {81--95},
	doi          = {10.18653/v1/2021.deelio-1.9},
	url          = {https://aclanthology.org/2021.deelio-1.9},
	editor       = {Agirre, Eneko  and Apidianaki, Marianna  and Vuli{\'c}, Ivan},
	abstract     = {Contextual word representation models have shown massive improvements on a multitude of NLP tasks, yet their word sense disambiguation capabilities remain poorly explained. To address this gap, we assess whether contextual word representations extracted from deep pretrained language models create distinguishable representations for different senses of a given word. We analyze the representation geometry and find that most layers of deep pretrained language models create highly anisotropic representations, pointing towards the existence of representation degeneration problem in contextual word representations. After accounting for anisotropy, our study further reveals that there is variability in sense learning capabilities across different language models. Finally, we propose LASeR, a {`}Low Anisotropy Sense Retrofitting{'} approach that renders off-the-shelf representations isotropic and semantically more meaningful, resolving the representation degeneration problem as a post-processing step, and conducting sense-enrichment of contextualized representations extracted from deep neural language models.}
}
@inproceedings{hämmerl2023exploring,
	title        = {Exploring Anisotropy and Outliers in Multilingual Language Models for Cross-Lingual Semantic Sentence Similarity},
	author       = {H{\"a}mmerl, Katharina  and Fastowski, Alina  and Libovick{\'y}, Jind{\v{r}}ich  and Fraser, Alexander},
	year         = 2023,
	month        = jul,
	booktitle    = {Findings of the Association for Computational Linguistics: ACL 2023},
	publisher    = {Association for Computational Linguistics},
	address      = {Toronto, Canada},
	pages        = {7023--7037},
	doi          = {10.18653/v1/2023.findings-acl.439},
	url          = {https://aclanthology.org/2023.findings-acl.439},
	editor       = {Rogers, Anna  and Boyd-Graber, Jordan  and Okazaki, Naoaki},
	abstract     = {Previous work has shown that the representations output by contextual language models are more anisotropic than static type embeddings, and typically display outlier dimensions. This seems to be true for both monolingual and multilingual models, although much less work has been done on the multilingual context. Why these outliers occur and how they affect the representations is still an active area of research. We investigate outlier dimensions and their relationship to anisotropy in multiple pre-trained multilingual language models. We focus on cross-lingual semantic similarity tasks, as these are natural tasks for evaluating multilingual representations. Specifically, we examine sentence representations. Sentence transformers which are fine-tuned on parallel resources (that are not always available) perform better on this task, and we show that their representations are more isotropic. However, we aim to improve multilingual representations in general. We investigate how much of the performance difference can be made up by only transforming the embedding space without fine-tuning, and visualise the resulting spaces. We test different operations: Removing individual outlier dimensions, cluster-based isotropy enhancement, and ZCA whitening. We publish our code for reproducibility.}
}
@inproceedings{machina-mercer-2024-anisotropy,
	title        = {Anisotropy is Not Inherent to Transformers},
	author       = {Machina, Anemily  and Mercer, Robert},
	year         = 2024,
	month        = jun,
	booktitle    = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Mexico City, Mexico},
	pages        = {4892--4907},
	doi          = {10.18653/v1/2024.naacl-long.274},
	url          = {https://aclanthology.org/2024.naacl-long.274},
	editor       = {Duh, Kevin  and Gomez, Helena  and Bethard, Steven},
	abstract     = {Isotropy is the property that embeddings are uniformly distributed around the origin. Previous work has shown that Transformer embedding spaces are anisotropic, which is called the representation degradation problem. This degradation has been assumed to be inherent to the standard language modeling tasks and to apply to all Transformer models regardless of their architecture. In this work we identify a set of Transformer models with isotropic embedding spaces, the large Pythia models. We examine the isotropy of Pythia models and explore how isotropy and anisotropy develop as a model is trained. We find that anisotropic models do not develop as previously theorized, using our own analysis to show that the large Pythia models optimize their final Layer Norm for isotropy, and provide reasoning why previous theoretical justifications for anisotropy were insufficient. The identification of a set of isotropic Transformer models calls previous assumptions into question, provides a set of models to contrast existing analysis, and should lead to deeper insight into isotropy.}
}
@inproceedings{ait-saada-nadif-2023-anisotropy,
	title        = {Is Anisotropy Truly Harmful? A Case Study on Text Clustering},
	author       = {Ait-Saada, Mira  and Nadif, Mohamed},
	year         = 2023,
	month        = jul,
	booktitle    = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Toronto, Canada},
	pages        = {1194--1203},
	doi          = {10.18653/v1/2023.acl-short.103},
	url          = {https://aclanthology.org/2023.acl-short.103},
	editor       = {Rogers, Anna  and Boyd-Graber, Jordan  and Okazaki, Naoaki},
	abstract     = {In the last few years, several studies have been devoted to dissecting dense text representations in order to understand their effectiveness and further improve their quality. Particularly, the anisotropy of such representations has been observed, which means that the directions of the word vectors are not evenly distributed across the space but rather concentrated in a narrow cone. This has led to several attempts to counteract this phenomenon both on static and contextualized text representations. However, despite this effort, there is no established relationship between anisotropy and performance. In this paper, we aim to bridge this gap by investigating the impact of different transformations on both the isotropy and the performance in order to assess the true impact of anisotropy. To this end, we rely on the clustering task as a means of evaluating the ability of text representations to produce meaningful groups. Thereby, we empirically show a limited impact of anisotropy on the expressiveness of sentence representations both in terms of directions and L2 closeness.}
}
@inproceedings{zhao-etal-2018-gender,
	title        = {Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods},
	author       = {Zhao, Jieyu  and Wang, Tianlu  and Yatskar, Mark  and Ordonez, Vicente  and Chang, Kai-Wei},
	year         = 2018,
	month        = jun,
	booktitle    = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {New Orleans, Louisiana},
	pages        = {15--20},
	doi          = {10.18653/v1/N18-2003},
	url          = {https://aclanthology.org/N18-2003},
	editor       = {Walker, Marilyn  and Ji, Heng  and Stent, Amanda},
	abstract     = {In this paper, we introduce a new benchmark for co-reference resolution focused on gender bias, WinoBias. Our corpus contains Winograd-schema style sentences with entities corresponding to people referred by their occupation (e.g. the nurse, the doctor, the carpenter). We demonstrate that a rule-based, a feature-rich, and a neural coreference system all link gendered pronouns to pro-stereotypical entities with higher accuracy than anti-stereotypical entities, by an average difference of 21.1 in F1 score. Finally, we demonstrate a data-augmentation approach that, in combination with existing word-embedding debiasing techniques, removes the bias demonstrated by these systems in WinoBias without significantly affecting their performance on existing datasets.}
}
@inproceedings{rudman2023stable,
	title        = {Stable Anisotropic Regularization},
	author       = {William Rudman and Carsten Eickhoff},
	year         = 2024,
	booktitle    = {The Twelfth International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=dbQH9AOVd5}
}
@misc{zhou2021freqbased,
	title        = {Frequency-based Distortions in Contextualized Word Embeddings},
	author       = {Kaitlyn Zhou and Kawin Ethayarajh and Dan Jurafsky},
	year         = 2021,
	url          = {https://arxiv.org/abs/2104.08465},
	eprint       = {2104.08465},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@misc{touvron2023llama,
	title        = {LLaMA: Open and Efficient Foundation Language Models},
	author       = {Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
	year         = 2023,
	url          = {https://arxiv.org/abs/2302.13971},
	eprint       = {2302.13971},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@misc{wang2020contextual,
	title        = {Contextual Temperature for Language Modeling},
	author       = {Pei-Hsin Wang and Sheng-Iou Hsieh and Shieh-Chieh Chang and Jia-Yu Pan and Yu-Ting Chen and Wei Wei and Da-Cheng Juan},
	year         = 2020,
	url          = {https://openreview.net/forum?id=H1x9004YPr}
}
@inproceedings{Radford2018ImprovingLU,
	title        = {Improving Language Understanding by Generative Pre-Training},
	author       = {Alec Radford and Karthik Narasimhan},
	year         = 2018,
	url          = {https://api.semanticscholar.org/CorpusID:49313245}
}
@inproceedings{fan-etal-2018-hierarchical,
	title        = {Hierarchical Neural Story Generation},
	author       = {Fan, Angela  and Lewis, Mike  and Dauphin, Yann},
	year         = 2018,
	month        = jul,
	booktitle    = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Melbourne, Australia},
	pages        = {889--898},
	doi          = {10.18653/v1/P18-1082},
	url          = {https://aclanthology.org/P18-1082},
	editor       = {Gurevych, Iryna  and Miyao, Yusuke},
	abstract     = {We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.}
}
@inproceedings{alibi,
	title        = {Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation},
	author       = {Ofir Press and Noah Smith and Mike Lewis},
	year         = 2022,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=R8sQPpGCv0}
}
@inproceedings{nucleus_sampling,
	title        = {The Curious Case of Neural Text Degeneration},
	author       = {Ari Holtzman and Jan Buys and Li Du and Maxwell Forbes and Yejin Choi},
	year         = 2020,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=rygGQyrFvH}
}
@article{rope,
	title        = {RoFormer: Enhanced transformer with Rotary Position Embedding},
	author       = {Jianlin Su and Murtadha Ahmed and Yu Lu and Shengfeng Pan and Wen Bo and Yunfeng Liu},
	year         = 2024,
	journal      = {Neurocomputing},
	volume       = 568,
	pages        = 127063,
	doi          = {https://doi.org/10.1016/j.neucom.2023.127063},
	issn         = {0925-2312},
	url          = {https://www.sciencedirect.com/science/article/pii/S0925231223011864},
	keywords     = {Pre-trained language models, Position information encoding, Pre-training, Natural language processing},
	abstract     = {Position encoding has recently been shown to be effective in transformer architecture. It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding (RoPE) to effectively leverage the positional information. Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in the self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Huggingface: https://huggingface.co/docs/transformers/model_doc/roformer.}
}
@article{elhage2021mathematical,
	title        = {A Mathematical Framework for Transformer Circuits},
	author       = {Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
	year         = 2021,
	journal      = {Transformer Circuits Thread},
	note         = {https://transformer-circuits.pub/2021/framework/index.html}
}
@article{lotr,
	title        = {Representing Spatial Structure Through Maps and Language: Lord of the Rings Encodes the Spatial Structure of Middle Earth},
	author       = {Louwerse, Max M. and Benesh, Nick},
	year         = 2012,
	journal      = {Cognitive Science},
	volume       = 36,
	number       = 8,
	pages        = {1556--1569},
	doi          = {https://doi.org/10.1111/cogs.12000},
	url          = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12000},
	keywords     = {Spatial cognition, Embodied cognition, Geographical structures, Cognitive maps, Mental representations, Symbol interdependency, Latent semantic analysis},
	eprint       = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.12000},
	abstract     = {Abstract Spatial mental representations can be derived from linguistic and non-linguistic sources of information. This study tested whether these representations could be formed from statistical linguistic frequencies of city names, and to what extent participants differed in their performance when they estimated spatial locations from language or maps. In a computational linguistic study, we demonstrated that co-occurrences of cities in Tolkien’s Lord of the Rings trilogy and The Hobbit predicted the authentic longitude and latitude of those cities in Middle Earth. In a human study, we showed that human spatial estimates of the location of cities were very similar regardless of whether participants read Tolkien’s texts or memorized a map of Middle Earth. However, text-based location estimates obtained from statistical linguistic frequencies better predicted the human text-based estimates than the human map-based estimates. These findings suggest that language encodes spatial structure of cities, and that human cognitive map representations can come from implicit statistical linguistic patterns, from explicit non-linguistic perceptual information, or from both.}
}
@inproceedings{faisal-anastasopoulos-2022-geographic,
	title        = {Geographic and Geopolitical Biases of Language Models},
	author       = {Faisal, Fahim  and Anastasopoulos, Antonios},
	year         = 2023,
	month        = dec,
	booktitle    = {Proceedings of the 3rd Workshop on Multi-lingual Representation Learning (MRL)},
	publisher    = {Association for Computational Linguistics},
	address      = {Singapore},
	pages        = {139--163},
	doi          = {10.18653/v1/2023.mrl-1.12},
	url          = {https://aclanthology.org/2023.mrl-1.12},
	editor       = {Ataman, Duygu}
}
@inproceedings{faisal-etal-2022-dataset,
	title        = {Dataset Geography: Mapping Language Data to Language Users},
	author       = {Faisal, Fahim  and Wang, Yinkai  and Anastasopoulos, Antonios},
	year         = 2022,
	month        = may,
	booktitle    = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Dublin, Ireland},
	pages        = {3381--3411},
	doi          = {10.18653/v1/2022.acl-long.239},
	url          = {https://aclanthology.org/2022.acl-long.239},
	editor       = {Muresan, Smaranda  and Nakov, Preslav  and Villavicencio, Aline},
	abstract     = {As language technologies become more ubiquitous, there are increasing efforts towards expanding the language diversity and coverage of natural language processing (NLP) systems. Arguably, the most important factor influencing the quality of modern NLP systems is data availability. In this work, we study the geographical representativeness of NLP datasets, aiming to quantify if and by how much do NLP datasets match the expected needs of the language speakers. In doing so, we use entity recognition and linking systems, also making important observations about their cross-lingual consistency and giving suggestions for more robust evaluation. Last, we explore some geographical and economic factors that may explain the observed dataset distributions.}
}
@inproceedings{gurnee2023language,
	title        = {Language Models Represent Space and Time},
	author       = {Wes Gurnee and Max Tegmark},
	year         = 2024,
	booktitle    = {The Twelfth International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=jE8xbmvFin}
}
@article{ridge,
	title        = {Ridge Regression: Biased Estimation for Nonorthogonal Problems},
	author       = {Hoerl, A. E. and Kennard, R. W.},
	year         = 1970,
	journal      = {Technometrics},
	volume       = 12,
	pages        = {55--67},
	added-at     = {2009-01-29T03:39:09.000+0100},
	keywords     = {ridge-regg},
	timestamp    = {2009-01-29T03:40:48.000+0100}
}
@misc{zhang2022opt,
	title        = {OPT: Open Pre-trained Transformer Language Models},
	author       = {Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},
	year         = 2022,
	url          = {https://arxiv.org/abs/2205.01068},
	eprint       = {2205.01068},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@inproceedings{pythia,
	title        = {Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling},
	author       = {Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin Gregory and Bradley, Herbie and O'Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, Usvsn Sai and Raff, Edward and Skowron, Aviya and Sutawika, Lintang and Van Der Wal, Oskar},
	year         = 2023,
	month        = {23--29 Jul},
	booktitle    = {Proceedings of the 40th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = 202,
	pages        = {2397--2430},
	url          = {https://proceedings.mlr.press/v202/biderman23a.html},
	editor       = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
	pdf          = {https://proceedings.mlr.press/v202/biderman23a/biderman23a.pdf},
	abstract     = {How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce <em>Pythia</em>, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend <em>Pythia</em> to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at https://github.com/EleutherAI/pythia.}
}
@software{gpt-neo,
	title        = {{GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow}},
	author       = {Black, Sid and Gao, Leo and Wang, Phil and Leahy, Connor and Biderman, Stella},
	year         = 2021,
	month        = mar,
	publisher    = {Zenodo},
	doi          = {10.5281/zenodo.5297715},
	url          = {https://doi.org/10.5281/zenodo.5297715},
	version      = {1.0}
}
@article{shliazhko2023mgpt,
	title        = {mGPT: Few-Shot Learners Go Multilingual},
	author       = {Shliazhko, Oleh and Fenogenova, Alena and Tikhonova, Maria and Kozlova, Anastasia and Mikhailov, Vladislav and Shavrina, Tatiana},
	year         = 2024,
	journal      = {Transactions of the Association for Computational Linguistics},
	publisher    = {MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…},
	volume       = 12,
	pages        = {58--79}
}
@inproceedings{electra,
	title        = {ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators},
	author       = {Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning},
	year         = 2020,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=r1xMH1BtvB}
}
@inproceedings{deberta,
	title        = {{\{}DEBERTA{\}}: {\{}DECODING{\}}-{\{}ENHANCED{\}} {\{}BERT{\}} {\{}WITH{\}} {\{}DISENTANGLED{\}} {\{}ATTENTION{\}}},
	author       = {Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
	year         = 2021,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=XPZIaotutsD}
}
% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@article{gao2020pile,
	title        = {The {P}ile: An 800{GB} dataset of diverse text for language modeling},
	author       = {Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2101.00027}
}

@ARTICLE {10148662,
author = {A. Sheth and K. Roy and M. Gaur},
journal = {IEEE Intelligent Systems},
title = {Neurosymbolic Artificial Intelligence (Why, What, and How)},
year = {2023},
volume = {38},
number = {03},
issn = {1941-1294},
pages = {56-62},
abstract = {Humans interact with the environment using a combination of perception—transforming sensory inputs from their environment into symbols, and cognition—mapping symbols to knowledge about the environment for supporting abstraction, reasoning by analogy, and long-term planning. Human perception-inspired machine perception, in the context of artificial intelligence (AI), refers to large-scale pattern recognition from raw data using neural networks trained using self-supervised learning objectives such as next-word prediction or object recognition. On the other hand, machine cognition encompasses more complex computations, such as using knowledge of the environment to guide reasoning, analogy, and long-term planning. Humans can also control and explain their cognitive functions. This seems to require the retention of symbolic mappings from perception outputs to knowledge about their environment. For example, humans can follow and explain the guidelines and safety constraints driving their decision making in safety-critical applications such as health care, criminal justice, and autonomous driving.},
keywords = {neural networks;symbols;self-supervised learning;medical services;cognition;human factors;safety},
doi = {10.1109/MIS.2023.3268724},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {may}
}

@inproceedings{
wei2022chain,
title={Chain of Thought Prompting Elicits Reasoning in Large Language Models},
author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and brian ichter and Fei Xia and Ed H. Chi and Quoc V Le and Denny Zhou},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=_VjQlMeSB_J}
}

@article{ecolo_llm,
	annote = {doi: 10.1021/acs.est.3c01106},
	author = {Rillig, Matthias C. and {\AA}gerstrand, Marlene and Bi, Mohan and Gould, Kenneth A. and Sauerland, Uli},
	date = {2023/03/07},
	date-added = {2024-09-28 11:06:15 +0200},
	date-modified = {2024-09-28 11:06:22 +0200},
	doi = {10.1021/acs.est.3c01106},
	isbn = {0013-936X},
	journal = {Environmental Science \& Technology},
	journal1 = {Environmental Science \& Technology},
	journal2 = {Environ. Sci. Technol.},
	month = {03},
	number = {9},
	pages = {3464--3466},
	publisher = {American Chemical Society},
	title = {Risks and Benefits of Large Language Models for the Environment},
	type = {doi: 10.1021/acs.est.3c01106},
	url = {https://doi.org/10.1021/acs.est.3c01106},
	volume = {57},
	year = {2023},
	year1 = {2023},
	bdsk-url-1 = {https://doi.org/10.1021/acs.est.3c01106}}



@inproceedings{conneau2018xnli,
	title        = {XNLI: Evaluating Cross-lingual Sentence Representations},
	author       = {Conneau, Alexis and Rinott, Ruty and Lample, Guillaume and Williams, Adina and Bowman, Samuel R. and Schwenk, Holger and Stoyanov, Veselin},
	year         = 2018,
	booktitle    = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	location     = {Brussels, Belgium},
	publisher    = {Association for Computational Linguistics}
}
@inproceedings{SciQ,
	title        = {Crowdsourcing Multiple Choice Science Questions},
	author       = {Johannes Welbl, Nelson F. Liu, Matt Gardner},
	year         = 2017,
	journal      = {arXiv:1707.06209v1}
}
@misc{clark2018think,
	title        = {Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},
	author       = {Peter Clark and Isaac Cowhey and Oren Etzioni and Tushar Khot and Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},
	year         = 2018,
	eprint       = {1803.05457},
	archiveprefix = {arXiv},
	primaryclass = {cs.AI}
}
@inproceedings{sanh2019distilbert,
	title        = {DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
	author       = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
	year         = 2019,
	booktitle    = {NeurIPS EMC2 Workshop}
}
@article{Fu_Zhou_Yang_Tang_Liu_Liu_Li_2021,
	title        = {LRC-BERT: Latent-representation Contrastive Knowledge Distillation for Natural Language Understanding},
	author       = {Fu, Hao and Zhou, Shaojun and Yang, Qihong and Tang, Junjie and Liu, Guiquan and Liu, Kaikui and Li, Xiaolong},
	year         = 2021,
	month        = {May},
	journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = 35,
	number       = 14,
	pages        = {12830--12838},
	doi          = {10.1609/aaai.v35i14.17518},
	url          = {https://ojs.aaai.org/index.php/AAAI/article/view/17518},
	abstractnote = {The pre-training models such as BERT have achieved great results in various natural language processing problems. However, a large number of parameters need significant amounts of memory and the consumption of inference time, which makes it difficult to deploy them on edge devices. In this work, we propose a knowledge distillation method LRC-BERT based on contrastive learning to fit the output of the intermediate layer from the angular distance aspect, which is not considered by the existing distillation methods. Furthermore, we introduce a gradient perturbation-based training architecture in the training phase to increase the robustness of LRC-BERT, which is the first attempt in knowledge distillation. Additionally, in order to better capture the distribution characteristics of the intermediate layer, we design a two-stage training method for the total distillation loss. Finally, by verifying 8 datasets on the General Language Understanding Evaluation (GLUE) benchmark, the performance of the proposed LRC-BERT exceeds the existing state-of-the-art methods, which proves the effectiveness of our method.}
}
@inproceedings{sun-etal-2020-contrastive,
	title        = {Contrastive Distillation on Intermediate Representations for Language Model Compression},
	author       = {Sun, Siqi  and Gan, Zhe  and Fang, Yuwei  and Cheng, Yu  and Wang, Shuohang  and Liu, Jingjing},
	year         = 2020,
	month        = nov,
	booktitle    = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {498--508},
	doi          = {10.18653/v1/2020.emnlp-main.36},
	url          = {https://aclanthology.org/2020.emnlp-main.36},
	editor       = {Webber, Bonnie  and Cohn, Trevor  and He, Yulan  and Liu, Yang},
	abstract     = {Existing language model compression methods mostly use a simple L{\_}2 loss to distill knowledge in the intermediate representations of a large BERT model to a smaller one. Although widely used, this objective by design assumes that all the dimensions of hidden representations are independent, failing to capture important structural knowledge in the intermediate layers of the teacher network. To achieve better distillation efficacy, we propose Contrastive Distillation on Intermediate Representations (CoDIR), a principled knowledge distillation framework where the student is trained to distill knowledge through intermediate layers of the teacher via a contrastive objective. By learning to distinguish positive sample from a large set of negative samples, CoDIR facilitates the student{'}s exploitation of rich information in teacher{'}s hidden layers. CoDIR can be readily applied to compress large-scale language models in both pre-training and finetuning stages, and achieves superb performance on the GLUE benchmark, outperforming state-of-the-art compression methods.}
}
@article{mgpt,
	title        = {{mGPT: Few-Shot Learners Go Multilingual}},
	author       = {Shliazhko, Oleh and Fenogenova, Alena and Tikhonova, Maria and Kozlova, Anastasia and Mikhailov, Vladislav and Shavrina, Tatiana},
	year         = 2024,
	month        = {01},
	journal      = {Transactions of the Association for Computational Linguistics},
	volume       = 12,
	pages        = {58--79},
	doi          = {10.1162/tacl_a_00633},
	issn         = {2307-387X},
	url          = {https://doi.org/10.1162/tacl\_a\_00633},
	abstract     = {{This paper introduces mGPT, a multilingual variant of GPT-3, pretrained on 61 languages from 25 linguistically diverse language families using Wikipedia and the C4 Corpus. We detail the design and pretraining procedure. The models undergo an intrinsic and extrinsic evaluation: language modeling in all languages, downstream evaluation on cross-lingual NLU datasets and benchmarks in 33 languages, and world knowledge probing in 23 languages. The in-context learning abilities are on par with the contemporaneous language models while covering a larger number of languages, including underrepresented and low-resource languages of the Commonwealth of Independent States and the indigenous peoples in Russia. The source code and the language models are publicly available under the MIT license.}},
	eprint       = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00633/2325676/tacl\_a\_00633.pdf}
}
@inproceedings{xue-etal-2021-mt5,
	title        = {m{T}5: A Massively Multilingual Pre-trained Text-to-Text Transformer},
	author       = {Xue, Linting  and Constant, Noah  and Roberts, Adam  and Kale, Mihir  and Al-Rfou, Rami  and Siddhant, Aditya  and Barua, Aditya  and Raffel, Colin},
	year         = 2021,
	month        = jun,
	booktitle    = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {483--498},
	doi          = {10.18653/v1/2021.naacl-main.41},
	url          = {https://aclanthology.org/2021.naacl-main.41},
	editor       = {Toutanova, Kristina  and Rumshisky, Anna  and Zettlemoyer, Luke  and Hakkani-Tur, Dilek  and Beltagy, Iz  and Bethard, Steven  and Cotterell, Ryan  and Chakraborty, Tanmoy  and Zhou, Yichao},
	abstract     = {The recent {``}Text-to-Text Transfer Transformer{''} (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent {``}accidental translation{''} in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.}
}
@misc{Hewitt_2019,
	title        = {Designing and Interpreting Probes},
	author       = {Hewitt, John},
	year         = 2019,
	month        = {Aug},
	url          = {https://nlp.stanford.edu/~johnhew/interpreting-probes.html}
}
@article{conneau2019unsupervised,
	title        = {Unsupervised Cross-lingual Representation Learning at Scale},
	author       = {Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1911.02116}
}
@inproceedings{wav2vec2,
	title        = {wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations},
	author       = {Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
	year         = 2020,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 33,
	pages        = {12449--12460},
	url          = {https://proceedings.neurips.cc/paper_files/paper/2020/file/92d1e1eb1cd6f9fba3227870bb6d7f07-Paper.pdf},
	editor       = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin}
}
@inproceedings{klein-nabi-2023-micse,
	title        = {mi{CSE}: Mutual Information Contrastive Learning for Low-shot Sentence Embeddings},
	author       = {Klein, Tassilo  and Nabi, Moin},
	year         = 2023,
	month        = jul,
	booktitle    = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Toronto, Canada},
	pages        = {6159--6177},
	url          = {https://aclanthology.org/2023.acl-long.339}
}
@inproceedings{zouhar-etal-2023-tokenization,
	title        = {Tokenization and the Noiseless Channel},
	author       = {Zouhar, Vil{\'e}m  and Meister, Clara  and Gastaldi, Juan  and Du, Li  and Sachan, Mrinmaya  and Cotterell, Ryan},
	year         = 2023,
	month        = jul,
	booktitle    = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Toronto, Canada},
	pages        = {5184--5207},
	doi          = {10.18653/v1/2023.acl-long.284},
	url          = {https://aclanthology.org/2023.acl-long.284}
}
@misc{liang2023xlmv,
	title        = {XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models},
	author       = {Davis Liang and Hila Gonen and Yuning Mao and Rui Hou and Naman Goyal and Marjan Ghazvininejad and Luke Zettlemoyer and Madian Khabsa},
	year         = 2023,
	eprint       = {2301.10472},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@inproceedings{contr1,
	title        = {Do More Negative Samples Necessarily Hurt In Contrastive Learning?},
	author       = {Awasthi, Pranjal and Dikkala, Nishanth and Kamath, Pritish},
	year         = 2022,
	month        = {17--23 Jul},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = 162,
	pages        = {1101--1116},
	url          = {https://proceedings.mlr.press/v162/awasthi22b.html},
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	pdf          = {https://proceedings.mlr.press/v162/awasthi22b/awasthi22b.pdf}
}
@misc{he2023debertav3,
	title        = {DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing},
	author       = {Pengcheng He and Jianfeng Gao and Weizhu Chen},
	year         = 2023,
	eprint       = {2111.09543},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@inproceedings{contr2,
	title        = {Investigating the Role of Negatives in Contrastive Representation Learning},
	author       = {Ash, Jordan and Goel, Surbhi and Krishnamurthy, Akshay and Misra, Dipendra},
	year         = 2022,
	month        = {28--30 Mar},
	booktitle    = {Proceedings of The 25th International Conference on Artificial Intelligence and Statistics},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = 151,
	pages        = {7187--7209},
	url          = {https://proceedings.mlr.press/v151/ash22a.html},
	editor       = {Camps-Valls, Gustau and Ruiz, Francisco J. R. and Valera, Isabel},
	pdf          = {https://proceedings.mlr.press/v151/ash22a/ash22a.pdf}
}
@misc{zhou2021frequencybased,
	title        = {Frequency-based Distortions in Contextualized Word Embeddings},
	author       = {Kaitlyn Zhou and Kawin Ethayarajh and Dan Jurafsky},
	year         = 2021,
	eprint       = {2104.08465},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@inproceedings{jain-etal-2023-contraclm,
	title        = {{C}ontra{CLM}: Contrastive Learning For Causal Language Model},
	author       = {Jain, Nihal  and Zhang, Dejiao  and Ahmad, Wasi Uddin  and Wang, Zijian  and Nan, Feng  and Li, Xiaopeng  and Tan, Ming  and Nallapati, Ramesh  and Ray, Baishakhi  and Bhatia, Parminder  and Ma, Xiaofei  and Xiang, Bing},
	year         = 2023,
	month        = jul,
	booktitle    = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Toronto, Canada},
	pages        = {6436--6459},
	url          = {https://aclanthology.org/2023.acl-long.355}
}
@misc{su2022contrastive,
	title        = {A Contrastive Framework for Neural Text Generation},
	author       = {Yixuan Su and Tian Lan and Yan Wang and Dani Yogatama and Lingpeng Kong and Nigel Collier},
	year         = 2022,
	eprint       = {2202.06417},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@misc{xFormers2022,
	title        = {xFormers: A modular and hackable Transformer modelling library},
	author       = {Benjamin Lefaudeux and Francisco Massa and Diana Liskovich and Wenhan Xiong and Vittorio Caggiano and Sean Naren and Min Xu and Jieru Hu and Marta Tintore and Susan Zhang and Patrick Labatut and Daniel Haziza},
	year         = 2022,
	howpublished = {\url{https://github.com/facebookresearch/xformers}}
}
@misc{oord2019representation,
	title        = {Representation Learning with Contrastive Predictive Coding},
	author       = {Aaron van den Oord and Yazhe Li and Oriol Vinyals},
	year         = 2019,
	eprint       = {1807.03748},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@inproceedings{schneider19_interspeech,
	title        = {{wav2vec: Unsupervised Pre-Training for Speech Recognition}},
	author       = {Steffen Schneider and Alexei Baevski and Ronan Collobert and Michael Auli},
	year         = 2019,
	booktitle    = {Proc. Interspeech 2019},
	pages        = {3465--3469},
	doi          = {10.21437/Interspeech.2019-1873}
}
@misc{sermanet2018timecontrastive,
	title        = {Time-Contrastive Networks: Self-Supervised Learning from Video},
	author       = {Pierre Sermanet and Corey Lynch and Yevgen Chebotar and Jasmine Hsu and Eric Jang and Stefan Schaal and Sergey Levine},
	year         = 2018,
	eprint       = {1704.06888},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}
@article{turc2019,
	title        = {Well-Read Students Learn Better: On the Importance of Pre-training Compact Models},
	author       = {Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1908.08962v2}
}
@inproceedings{Hamborg2017,
	title        = {news-please: A Generic News Crawler and Extractor},
	author       = {Hamborg, Felix and Meuschke, Norman and Breitinger, Corinna and Gipp, Bela},
	year         = 2017,
	month        = {March},
	booktitle    = {Proceedings of the 15th International Symposium of Information Science},
	location     = {Berlin},
	pages        = {218--223},
	doi          = {10.5281/zenodo.4120316}
}
@article{2023arXiv230110472L,
	title        = {{XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models}},
	author       = {{Liang}, Davis and {Gonen}, Hila and {Mao}, Yuning and {Hou}, Rui and {Goyal}, Naman and {Ghazvininejad}, Marjan and {Zettlemoyer}, Luke and {Khabsa}, Madian},
	year         = 2023,
	month        = jan,
	journal      = {arXiv e-prints},
	pages        = {arXiv:2301.10472},
	doi          = {10.48550/arXiv.2301.10472},
	keywords     = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	eid          = {arXiv:2301.10472},
	archiveprefix = {arXiv},
	eprint       = {2301.10472},
	primaryclass = {cs.CL},
	adsurl       = {https://ui.adsabs.harvard.edu/abs/2023arXiv230110472L},
	adsnote      = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{meister_natbias,
	title        = {A Natural Bias for Language Generation Models},
	author       = {Meister, Clara and Stokowiec, Wojciech and Pimentel, Tiago and Yu, Lei and Rimell, Laura and Kuncoro, Adhiguna},
	year         = 2022,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.2212.09686},
	url          = {https://arxiv.org/abs/2212.09686},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@article{gao19,
	title        = {Representation Degeneration Problem in Training Natural Language Generation Models},
	author       = {Gao, Jun and He, Di and Tan, Xu and Qin, Tao and Wang, Liwei and Liu, Tie-Yan},
	year         = 2019,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.1907.12009},
	url          = {https://arxiv.org/abs/1907.12009},
	copyright    = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
	keywords     = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@article{zhou_freq,
	title        = {Frequency-based Distortions in Contextualized Word Embeddings},
	author       = {Zhou, Kaitlyn and Ethayarajh, Kawin and Jurafsky, Dan},
	year         = 2021,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.2104.08465},
	url          = {https://arxiv.org/abs/2104.08465},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@book{APA:83,
	title        = {Publications Manual},
	author       = {{American Psychological Association}},
	year         = 1983,
	publisher    = {American Psychological Association},
	address      = {Washington, DC}
}
@article{Chandra:81,
	title        = {Alternation},
	author       = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year         = 1981,
	journal      = {Journal of the Association for Computing Machinery},
	volume       = 28,
	number       = 1,
	pages        = {114--133},
	doi          = {10.1145/322234.322243}
}
@inproceedings{andrew2007scalable,
	title        = {Scalable training of {$L_1$}-regularized log-linear models},
	author       = {Andrew, Galen and Gao, Jianfeng},
	year         = 2007,
	booktitle    = {Proceedings of the 24th International Conference on Machine Learning},
	pages        = {33--40},
	url          = {https://dl.acm.org/doi/abs/10.1145/1273496.1273501}
}
@book{Gusfield:97,
	title        = {Algorithms on Strings, Trees and Sequences},
	author       = {Dan Gusfield},
	year         = 1997,
	publisher    = {Cambridge University Press},
	address      = {Cambridge, UK},
	url          = {https://www.cambridge.org/core/books/algorithms-on-strings-trees-and-sequences/F0B095049C7E6EF5356F0A26686C20D3}
}
@article{rasooli-tetrault-2015,
	title        = {Yara Parser: {A} Fast and Accurate Dependency Parser},
	author       = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
	year         = 2015,
	journal      = {Computing Research Repository},
	volume       = {arXiv:1503.06733},
	url          = {http://arxiv.org/abs/1503.06733},
	note         = {version 2}
}
@article{algayres-etal-2022-dp,
	title        = {{DP}-Parse: Finding Word Boundaries from Raw Speech with an Instance Lexicon},
	author       = {Algayres, Robin  and Ricoul, Tristan  and Karadayi, Julien  and Lauren{\c{c}}on, Hugo  and Zaiem, Salah  and Mohamed, Abdelrahman  and Sagot, Beno{\^\i}t  and Dupoux, Emmanuel},
	year         = 2022,
	journal      = {Transactions of the Association for Computational Linguistics},
	publisher    = {MIT Press},
	address      = {Cambridge, MA},
	volume       = 10,
	pages        = {1051--1065},
	doi          = {10.1162/tacl_a_00505},
	url          = {https://aclanthology.org/2022.tacl-1.61}
}
@misc{mnih2012fast,
	title        = {A Fast and Simple Algorithm for Training Neural Probabilistic Language Models},
	author       = {Andriy Mnih and Yee Whye Teh},
	year         = 2012,
	eprint       = {1206.6426},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@inproceedings{nce,
	title        = {Noise-contrastive estimation: A new estimation principle for unnormalized statistical models},
	author       = {Gutmann, Michael and Hyvärinen, Aapo},
	year         = 2010,
	month        = {13--15 May},
	booktitle    = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
	publisher    = {PMLR},
	address      = {Chia Laguna Resort, Sardinia, Italy},
	series       = {Proceedings of Machine Learning Research},
	volume       = 9,
	pages        = {297--304},
	url          = {https://proceedings.mlr.press/v9/gutmann10a.html},
	editor       = {Teh, Yee Whye and Titterington, Mike},
	pdf          = {http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf}
}
@inproceedings{kumar2018vmf,
	title        = {Von Mises-Fisher Loss for Training Sequence to Sequence Models with Continuous Outputs},
	author       = {Sachin Kumar and Yulia Tsvetkov},
	year         = 2019,
	booktitle    = {Proc. of ICLR},
	url          = {https://arxiv.org/pdf/1812.04616.pdf}
}
@inproceedings{10.5555/3295222.3295378,
	title        = {Neural discrete representation learning},
	author       = {van den Oord, Aaron and Vinyals, Oriol and Kavukcuoglu, Koray},
	year         = 2017,
	booktitle    = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
	location     = {Long Beach, California, USA},
	publisher    = {Curran Associates Inc.},
	address      = {Red Hook, NY, USA},
	series       = {NIPS'17},
	pages        = {6309–6318},
	isbn         = 9781510860964,
	abstract     = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of "posterior collapse" -ߞ where the latents are ignored when they are paired with a powerful autoregressive decoder -ߞ typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
	numpages     = 10
}
@inproceedings{quick_train_bengio_03,
	title        = {Quick Training of Probabilistic Neural Nets by Importance Sampling},
	author       = {Bengio, Yoshua and Senecal, Jean-S{\'{e}}bastien},
	year         = 2003,
	month        = {03--06 Jan},
	booktitle    = {Proceedings of the Ninth International Workshop on Artificial Intelligence and Statistics},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = {R4},
	pages        = {17--24},
	url          = {https://proceedings.mlr.press/r4/bengio03a.html},
	note         = {Reissued by PMLR on 01 April 2021.},
	editor       = {Bishop, Christopher M. and Frey, Brendan J.},
	pdf          = {http://proceedings.mlr.press/r4/bengio03a/bengio03a.pdf}
}
% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@book{Aho:72,
	title        = {The Theory of Parsing, Translation and Compiling},
	author       = {Alfred V. Aho and Jeffrey D. Ullman},
	year         = 1972,
	publisher    = {Prentice-Hall},
	address      = {Englewood Cliffs, NJ},
	volume       = 1
}
@inproceedings{iskander-etal-2023-shielded,
	title        = {Shielded Representations: Protecting Sensitive Attributes Through Iterative Gradient-Based Projection},
	author       = {Iskander, Shadi  and Radinsky, Kira  and Belinkov, Yonatan},
	year         = 2023,
	month        = jul,
	booktitle    = {Findings of the Association for Computational Linguistics: ACL 2023},
	publisher    = {Association for Computational Linguistics},
	address      = {Toronto, Canada},
	pages        = {5961--5977},
	doi          = {10.18653/v1/2023.findings-acl.369},
	url          = {https://aclanthology.org/2023.findings-acl.369}
}
@misc{orr2020bootleg,
	title        = {Bootleg: Chasing the Tail with Self-Supervised Named Entity Disambiguation},
	author       = {Laurel Orr and Megan Leszczynski and Simran Arora and Sen Wu and Neel Guha and Xiao Ling and Christopher Re},
	year         = 2020,
	eprint       = {2010.10363},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@inproceedings{feng-etal-2023-pretraining,
	title        = {From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair {NLP} Models},
	author       = {Feng, Shangbin  and Park, Chan Young  and Liu, Yuhan  and Tsvetkov, Yulia},
	year         = 2023,
	month        = jul,
	booktitle    = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Toronto, Canada},
	pages        = {11737--11762},
	doi          = {10.18653/v1/2023.acl-long.656},
	url          = {https://aclanthology.org/2023.acl-long.656}
}
@book{gini1912variabilita,
	title        = {Variabilit{\`a} e mutabilit{\`a}: contributo allo studio delle distribuzioni e delle relazioni statistiche.[Fasc. I.]},
	author       = {Gini, Corrado},
	year         = 1912,
	publisher    = {Tipogr. di P. Cuppini}
}
@article{MerityXBS16,
	title        = {Pointer Sentinel Mixture Models},
	author       = {Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
	year         = 2016,
	journal      = {CoRR},
	volume       = {abs/1609.07843},
	url          = {http://arxiv.org/abs/1609.07843},
	eprinttype   = {arXiv},
	eprint       = {1609.07843},
	timestamp    = {Thu, 21 Mar 2019 11:19:44 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/MerityXBS16.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{meister_natbias,
	title        = {A Natural Bias for Language Generation Models},
	author       = {Meister, Clara and Stokowiec, Wojciech and Pimentel, Tiago and Yu, Lei and Rimell, Laura and Kuncoro, Adhiguna},
	year         = 2022,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.2212.09686},
	url          = {https://arxiv.org/abs/2212.09686},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@article{warstadt2018neural,
	title        = {Neural Network Acceptability Judgments},
	author       = {Warstadt, Alex and Singh, Amanpreet and Bowman, Samuel R},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1805.12471}
}
% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@article{beltagy2020longformer,
	title        = {Longformer: The long-document transformer},
	author       = {Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2004.05150}
}
@article{BISCARRI201892,
	title        = {A simple and fast method for computing the Poisson binomial distribution function},
	author       = {William Biscarri and Sihai Dave Zhao and Robert J. Brunner},
	year         = 2018,
	journal      = {Computational Statistics \& Data Analysis},
	volume       = 122,
	pages        = {92--100},
	doi          = {https://doi.org/10.1016/j.csda.2018.01.007},
	issn         = {0167-9473},
	url          = {https://www.sciencedirect.com/science/article/pii/S0167947318300082}
}
@article{chung2016hierarchical,
	title        = {Hierarchical multiscale recurrent neural networks},
	author       = {Chung, Junyoung and Ahn, Sungjin and Bengio, Yoshua},
	year         = 2016,
	journal      = {arXiv preprint arXiv:1609.01704}
}
@article{clark2022canine,
	title        = {Canine: Pre-training an efficient tokenization-free encoder for language representation},
	author       = {Clark, Jonathan H and Garrette, Dan and Turc, Iulia and Wieting, John},
	year         = 2022,
	journal      = {Transactions of the Association for Computational Linguistics},
	publisher    = {MIT Press},
	volume       = 10,
	pages        = {73--91}
}
@article{graves2013generating,
	title        = {Generating sequences with recurrent neural networks},
	author       = {Graves, Alex},
	year         = 2013,
	journal      = {arXiv preprint arXiv:1308.0850}
}
@article{hochreiter1997long,
	title        = {Long short-term memory},
	author       = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
	year         = 1997,
	journal      = {Neural computation},
	publisher    = {MIT Press},
	volume       = 9,
	number       = 8,
	pages        = {1735--1780}
}
@article{jaegle2021perceiver,
	title        = {Perceiver {IO:} {A} General Architecture for Structured Inputs {\&} Outputs},
	author       = {Andrew Jaegle and Sebastian Borgeaud and Jean{-}Baptiste Alayrac and Carl Doersch and Catalin Ionescu and David Ding and Skanda Koppula and Daniel Zoran and Andrew Brock and Evan Shelhamer and Olivier J. H{\'{e}}naff and Matthew M. Botvinick and Andrew Zisserman and Oriol Vinyals and Jo{\~{a}}o Carreira},
	year         = 2021,
	journal      = {CoRR},
	volume       = {abs/2107.14795},
	url          = {https://arxiv.org/abs/2107.14795},
	eprinttype   = {arXiv},
	eprint       = {2107.14795},
	timestamp    = {Tue, 03 Aug 2021 14:53:34 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2107-14795.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{Jzefowicz2016ExploringTL,
	title        = {Exploring the Limits of Language Modeling},
	author       = {Rafal J{\'o}zefowicz and Oriol Vinyals and Mike Schuster and Noam M. Shazeer and Yonghui Wu},
	year         = 2016,
	journal      = {ArXiv},
	volume       = {abs/1602.02410}
}
@inproceedings{kim2016character,
	title        = {Character-aware neural language models},
	author       = {Kim, Yoon and Jernite, Yacine and Sontag, David and Rush, Alexander M},
	year         = 2016,
	booktitle    = {Thirtieth AAAI conference on artificial intelligence}
}
@article{mielke2021between,
	title        = {Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP},
	author       = {Mielke, Sabrina J and Alyafeai, Zaid and Salesky, Elizabeth and Raffel, Colin and Dey, Manan and Gall{\'e}, Matthias and Raja, Arun and Si, Chenglei and Lee, Wilson Y and Sagot, Beno{\^\i}t and others},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2112.10508}
}
@article{mofijul2022vocabulary,
	title        = {A Vocabulary-Free Multilingual Neural Tokenizer for End-to-End Task Learning},
	author       = {Mofijul Islam, Md and Aguilar, Gustavo and Ponnusamy, Pragaash and Mathialagan, Clint Solomon and Ma, Chengyuan and Guo, Chenlei},
	year         = 2022,
	journal      = {arXiv e-prints},
	pages        = {arXiv--2204}
}
@misc{poibin_fft,
	title        = {An Algorithm for Computing the Distribution Function of the Generalized Poisson-Binomial Distribution},
	author       = {Zhang, Man and Hong, Yili and Balakrishnan, Narayanaswamy},
	year         = 2017,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.1702.01326},
	url          = {https://arxiv.org/abs/1702.01326},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Computation (stat.CO), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@article{raffel2020t5,
	title        = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
	author       = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
	year         = 2020,
	journal      = {Journal of Machine Learning Research},
	volume       = 21,
	number       = 140,
	pages        = {1--67},
	url          = {http://jmlr.org/papers/v21/20-074.html}
}
@inproceedings{sutskever2011generating,
	title        = {Generating text with recurrent neural networks},
	author       = {Sutskever, Ilya and Martens, James and Hinton, Geoffrey E},
	year         = 2011,
	booktitle    = {ICML}
}
@inproceedings{tay2021charformer,
	title        = {Charformer: Fast Character Transformers via Gradient-based Subword Tokenization},
	author       = {Tay, Yi and Tran, Vinh Q and Ruder, Sebastian and Gupta, Jai and Chung, Hyung Won and Bahri, Dara and Qin, Zhen and Baumgartner, Simon and Yu, Cong and Metzler, Donald},
	year         = 2021,
	booktitle    = {International Conference on Learning Representations}
}
@inproceedings{vaswani2017attention,
	title        = {Attention is All you Need},
	author       = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
	year         = 2017,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 30,
	pages        = {},
	url          = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
	editor       = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett}
}
@article{wu2016google,
	title        = {Google's neural machine translation system: Bridging the gap between human and machine translation},
	author       = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and others},
	year         = 2016,
	journal      = {arXiv preprint arXiv:1609.08144}
}
@inproceedings{wulczyn2017ex,
	title        = {Ex machina: Personal attacks seen at scale},
	author       = {Wulczyn, Ellery and Thain, Nithum and Dixon, Lucas},
	year         = 2017,
	booktitle    = {Proceedings of the 26th international conference on world wide web},
	pages        = {1391--1399}
}
@article{xue2022byt5,
	title        = {ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models},
	author       = {Xue, Linting and Barua, Aditya and Constant, Noah and Al-Rfou, Rami and Narang, Sharan and Kale, Mihir and Roberts, Adam and Raffel, Colin},
	year         = 2022,
	journal      = {Transactions of the Association for Computational Linguistics},
	publisher    = {MIT Press},
	volume       = 10,
	pages        = {291--306}
}
@article{DBLP:journals/corr/abs-1804-04235,
	title        = {Adafactor: Adaptive Learning Rates with Sublinear Memory Cost},
	author       = {Noam Shazeer and Mitchell Stern},
	year         = 2018,
	journal      = {CoRR},
	volume       = {abs/1804.04235},
	url          = {http://arxiv.org/abs/1804.04235},
	eprinttype   = {arXiv},
	eprint       = {1804.04235},
	timestamp    = {Mon, 13 Aug 2018 16:48:15 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1804-04235.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{clark-etal-2020-pre,
	title        = {Pre-Training Transformers as Energy-Based Cloze Models},
	author       = {Clark, Kevin  and Luong, Minh-Thang  and Le, Quoc  and Manning, Christopher D.},
	year         = 2020,
	month        = nov,
	booktitle    = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {285--294},
	doi          = {10.18653/v1/2020.emnlp-main.20},
	url          = {https://aclanthology.org/2020.emnlp-main.20},
	editor       = {Webber, Bonnie  and Cohn, Trevor  and He, Yulan  and Liu, Yang},
	abstract     = {We introduce Electric, an energy-based cloze model for representation learning over text. Like BERT, it is a conditional generative model of tokens given their contexts. However, Electric does not use masking or output a full distribution over tokens that could occur in a context. Instead, it assigns a scalar energy score to each input token indicating how likely it is given its context. We train Electric using an algorithm based on noise-contrastive estimation and elucidate how this learning objective is closely related to the recently proposed ELECTRA pre-training method. Electric performs well when transferred to downstream tasks and is particularly effective at producing likelihood scores for text: it re-ranks speech recognition n-best lists better than language models and much faster than masked language models. Furthermore, it offers a clearer and more principled view of what ELECTRA learns during pre-training.}
}
@inproceedings{jean-etal-2015-using,
	title        = {On Using Very Large Target Vocabulary for Neural Machine Translation},
	author       = {Jean, S{\'e}bastien  and Cho, Kyunghyun  and Memisevic, Roland  and Bengio, Yoshua},
	year         = 2015,
	month        = jul,
	booktitle    = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Beijing, China},
	pages        = {1--10},
	doi          = {10.3115/v1/P15-1001},
	url          = {https://aclanthology.org/P15-1001},
	editor       = {Zong, Chengqing  and Strube, Michael}
}
@inproceedings{ma-collins-2018-noise,
	title        = {Noise Contrastive Estimation and Negative Sampling for Conditional Models: Consistency and Statistical Efficiency},
	author       = {Ma, Zhuang  and Collins, Michael},
	year         = 2018,
	month        = oct # {-} # nov,
	booktitle    = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	publisher    = {Association for Computational Linguistics},
	address      = {Brussels, Belgium},
	pages        = {3698--3707},
	doi          = {10.18653/v1/D18-1405},
	url          = {https://aclanthology.org/D18-1405},
	editor       = {Riloff, Ellen  and Chiang, David  and Hockenmaier, Julia  and Tsujii, Jun{'}ichi},
	abstract     = {Noise Contrastive Estimation (NCE) is a powerful parameter estimation method for log-linear models, which avoids calculation of the partition function or its derivatives at each training step, a computationally demanding step in many cases. It is closely related to negative sampling methods, now widely used in NLP. This paper considers NCE-based estimation of conditional models. Conditional models are frequently encountered in practice; however there has not been a rigorous theoretical analysis of NCE in this setting, and we will argue there are subtle but important questions when generalizing NCE to the conditional case. In particular, we analyze two variants of NCE for conditional models: one based on a classification objective, the other based on a ranking objective. We show that the ranking-based variant of NCE gives consistent parameter estimates under weaker assumptions than the classification-based method; we analyze the statistical efficiency of the ranking-based and classification-based variants of NCE; finally we describe experiments on synthetic data and language modeling showing the effectiveness and tradeoffs of both methods.}
}
@inproceedings{nangia-etal-2020-crows,
	title        = {{C}row{S}-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models},
	author       = {Nangia, Nikita  and Vania, Clara  and Bhalerao, Rasika  and Bowman, Samuel R.},
	year         = 2020,
	month        = nov,
	booktitle    = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {1953--1967},
	doi          = {10.18653/v1/2020.emnlp-main.154},
	url          = {https://aclanthology.org/2020.emnlp-main.154},
	editor       = {Webber, Bonnie  and Cohn, Trevor  and He, Yulan  and Liu, Yang},
	abstract     = {Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs. As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress.}
}
@inproceedings{rust-etal-2021-good,
	title        = {How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models},
	author       = {Rust, Phillip  and Pfeiffer, Jonas  and Vuli{\'c}, Ivan  and Ruder, Sebastian  and Gurevych, Iryna},
	year         = 2021,
	month        = aug,
	booktitle    = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {3118--3135},
	doi          = {10.18653/v1/2021.acl-long.243},
	url          = {https://aclanthology.org/2021.acl-long.243},
	editor       = {Zong, Chengqing  and Xia, Fei  and Li, Wenjie  and Navigli, Roberto},
	abstract     = {In this work, we provide a systematic and comprehensive empirical comparison of pretrained multilingual language models versus their monolingual counterparts with regard to their monolingual task performance. We study a set of nine typologically diverse languages with readily available pretrained monolingual models on a set of five diverse monolingual downstream tasks. We first aim to establish, via fair and controlled comparisons, if a gap between the multilingual and the corresponding monolingual representation of that language exists, and subsequently investigate the reason for any performance difference. To disentangle conflating factors, we train new monolingual models on the same data, with monolingually and multilingually trained tokenizers. We find that while the pretraining data size is an important factor, a designated monolingual tokenizer plays an equally important role in the downstream performance. Our results show that languages that are adequately represented in the multilingual model{'}s vocabulary exhibit negligible performance decreases over their monolingual counterparts. We further find that replacing the original multilingual tokenizer with the specialized monolingual tokenizer improves the downstream performance of the multilingual model for almost every task and language.}
}
@inproceedings{garcia-etal-2021-towards,
	title        = {Towards Continual Learning for Multilingual Machine Translation via Vocabulary Substitution},
	author       = {Garcia, Xavier  and Constant, Noah  and Parikh, Ankur  and Firat, Orhan},
	year         = 2021,
	month        = jun,
	booktitle    = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {1184--1192},
	doi          = {10.18653/v1/2021.naacl-main.93},
	url          = {https://aclanthology.org/2021.naacl-main.93},
	editor       = {Toutanova, Kristina  and Rumshisky, Anna  and Zettlemoyer, Luke  and Hakkani-Tur, Dilek  and Beltagy, Iz  and Bethard, Steven  and Cotterell, Ryan  and Chakraborty, Tanmoy  and Zhou, Yichao},
	abstract     = {We propose a straightforward vocabulary adaptation scheme to extend the language capacity of multilingual machine translation models, paving the way towards efficient continual learning for multilingual machine translation. Our approach is suitable for large-scale datasets, applies to distant languages with unseen scripts, incurs only minor degradation on the translation performance for the original language pairs and provides competitive performance even in the case where we only possess monolingual data for the new languages.}
}
@inproceedings{romanov-shivade-2018-lessons,
	title        = {Lessons from Natural Language Inference in the Clinical Domain},
	author       = {Romanov, Alexey  and Shivade, Chaitanya},
	year         = 2018,
	month        = oct # {-} # nov,
	booktitle    = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	publisher    = {Association for Computational Linguistics},
	address      = {Brussels, Belgium},
	pages        = {1586--1596},
	doi          = {10.18653/v1/D18-1187},
	url          = {https://aclanthology.org/D18-1187},
	editor       = {Riloff, Ellen  and Chiang, David  and Hockenmaier, Julia  and Tsujii, Jun{'}ichi},
	abstract     = {State of the art models using deep neural networks have become very good in learning an accurate mapping from inputs to outputs. However, they still lack generalization capabilities in conditions that differ from the ones encountered during training. This is even more challenging in specialized, and knowledge intensive domains, where training data is limited. To address this gap, we introduce MedNLI - a dataset annotated by doctors, performing a natural language inference task (NLI), grounded in the medical history of patients. We present strategies to: 1) leverage transfer learning using datasets from the open domain, (e.g. SNLI) and 2) incorporate domain knowledge from external data and lexical sources (e.g. medical terminologies). Our results demonstrate performance gains using both strategies.}
}
@inproceedings{faisal-anastasopoulos-2021-investigating,
	title        = {Investigating Post-pretraining Representation Alignment for Cross-Lingual Question Answering},
	author       = {Faisal, Fahim  and Anastasopoulos, Antonios},
	year         = 2021,
	month        = nov,
	booktitle    = {Proceedings of the 3rd Workshop on Machine Reading for Question Answering},
	publisher    = {Association for Computational Linguistics},
	address      = {Punta Cana, Dominican Republic},
	pages        = {133--148},
	doi          = {10.18653/v1/2021.mrqa-1.14},
	url          = {https://aclanthology.org/2021.mrqa-1.14},
	editor       = {Fisch, Adam  and Talmor, Alon  and Chen, Danqi  and Choi, Eunsol  and Seo, Minjoon  and Lewis, Patrick  and Jia, Robin  and Min, Sewon},
	abstract     = {Human knowledge is collectively encoded in the roughly 6500 languages spoken around the world, but it is not distributed equally across languages. Hence, for information-seeking question answering (QA) systems to adequately serve speakers of all languages, they need to operate cross-lingually. In this work we investigate the capabilities of multilingually pretrained language models on cross-lingual QA. We find that explicitly aligning the representations across languages with a post-hoc finetuning step generally leads to improved performance. We additionally investigate the effect of data size as well as the language choice in this fine-tuning step, also releasing a dataset for evaluating cross-lingual QA systems.}
}
@inproceedings{hoffart-etal-2011-robust,
	title        = {Robust Disambiguation of Named Entities in Text},
	author       = {Hoffart, Johannes  and Yosef, Mohamed Amir  and Bordino, Ilaria  and F{\"u}rstenau, Hagen  and Pinkal, Manfred  and Spaniol, Marc  and Taneva, Bilyana  and Thater, Stefan  and Weikum, Gerhard},
	year         = 2011,
	month        = jul,
	booktitle    = {Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing},
	publisher    = {Association for Computational Linguistics},
	address      = {Edinburgh, Scotland, UK.},
	pages        = {782--792},
	url          = {https://aclanthology.org/D11-1072},
	editor       = {Barzilay, Regina  and Johnson, Mark}
}
@inproceedings{ma-etal-2020-charbert,
	title        = {{C}har{BERT}: Character-aware Pre-trained Language Model},
	author       = {Ma, Wentao  and Cui, Yiming  and Si, Chenglei  and Liu, Ting  and Wang, Shijin  and Hu, Guoping},
	year         = 2020,
	month        = dec,
	booktitle    = {Proceedings of the 28th International Conference on Computational Linguistics},
	publisher    = {International Committee on Computational Linguistics},
	address      = {Barcelona, Spain (Online)},
	pages        = {39--50},
	doi          = {10.18653/v1/2020.coling-main.4},
	url          = {https://aclanthology.org/2020.coling-main.4},
	editor       = {Scott, Donia  and Bel, Nuria  and Zong, Chengqing},
	abstract     = {Most pre-trained language models (PLMs) construct word representations at subword level with Byte-Pair Encoding (BPE) or its variations, by which OOV (out-of-vocab) words are almost avoidable. However, those methods split a word into subword units and make the representation incomplete and fragile. In this paper, we propose a character-aware pre-trained language model named CharBERT improving on the previous methods (such as BERT, RoBERTa) to tackle these problems. We first construct the contextual word embedding for each token from the sequential character representations, then fuse the representations of characters and the subword representations by a novel heterogeneous interaction module. We also propose a new pre-training task named NLM (Noisy LM) for unsupervised character representation learning. We evaluate our method on question answering, sequence labeling, and text classification tasks, both on the original datasets and adversarial misspelling test sets. The experimental results show that our method can significantly improve the performance and robustness of PLMs simultaneously.}
}
@inproceedings{gpt3,
	title        = {Language Models are Few-Shot Learners},
	author       = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	year         = 2020,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 33,
	pages        = {1877--1901},
	url          = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
	editor       = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin}
}
@article{palm,
	title        = {PaLM: scaling language modeling with pathways},
	author       = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sashank and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
	year         = 2024,
	month        = {mar},
	journal      = {J. Mach. Learn. Res.},
	publisher    = {JMLR.org},
	volume       = 24,
	number       = 1,
	issn         = {1532-4435},
	issue_date   = {January 2023},
	abstract     = {Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540- billion parameter, densely activated, Transformer language model, which we call Pathways Language Model (PaLM).We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.},
	articleno    = 240,
	numpages     = 113,
	keywords     = {large language models, few-shot learning, natural language processing, scalable deep learning}
}
@article{le2023bloom,
	title        = {Bloom: A 176b-parameter open-access multilingual language model},
	author       = {Le Scao, Teven and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
	year         = 2023
}
@article{flan_t5,
	title        = {Scaling Instruction-Finetuned Language Models},
	author       = {Hyung Won Chung and Le Hou and Shayne Longpre and Barret Zoph and Yi Tay and William Fedus and Yunxuan Li and Xuezhi Wang and Mostafa Dehghani and Siddhartha Brahma and Albert Webson and Shixiang Shane Gu and Zhuyun Dai and Mirac Suzgun and Xinyun Chen and Aakanksha Chowdhery and Alex Castro-Ros and Marie Pellat and Kevin Robinson and Dasha Valter and Sharan Narang and Gaurav Mishra and Adams Yu and Vincent Zhao and Yanping Huang and Andrew Dai and Hongkun Yu and Slav Petrov and Ed H. Chi and Jeff Dean and Jacob Devlin and Adam Roberts and Denny Zhou and Quoc V. Le and Jason Wei},
	year         = 2024,
	journal      = {Journal of Machine Learning Research},
	volume       = 25,
	number       = 70,
	pages        = {1--53},
	url          = {http://jmlr.org/papers/v25/23-0870.html}
}
@inproceedings{tokarchuk-niculae-2022-target,
	title        = {On Target Representation in Continuous-output Neural Machine Translation},
	author       = {Tokarchuk, Evgeniia  and Niculae, Vlad},
	year         = 2022,
	month        = may,
	booktitle    = {Proceedings of the 7th Workshop on Representation Learning for NLP},
	publisher    = {Association for Computational Linguistics},
	address      = {Dublin, Ireland},
	pages        = {227--235},
	doi          = {10.18653/v1/2022.repl4nlp-1.24},
	url          = {https://aclanthology.org/2022.repl4nlp-1.24},
	editor       = {Gella, Spandana  and He, He  and Majumder, Bodhisattwa Prasad  and Can, Burcu  and Giunchiglia, Eleonora  and Cahyawijaya, Samuel  and Min, Sewon  and Mozes, Maximilian  and Li, Xiang Lorraine  and Augenstein, Isabelle  and Rogers, Anna  and Cho, Kyunghyun  and Grefenstette, Edward  and Rimell, Laura  and Dyer, Chris},
	abstract     = {Continuous generative models proved their usefulness in high-dimensional data, such as image and audio generation. However, continuous models for text generation have received limited attention from the community. In this work, we study continuous text generation using Transformers for neural machine translation (NMT). We argue that the choice of embeddings is crucial for such models, so we aim to focus on one particular aspect{''}:{''} target representation via embeddings. We explore pretrained embeddings and also introduce knowledge transfer from the discrete Transformer model using embeddings in Euclidean and non-Euclidean spaces. Our results on the WMT Romanian-English and English-Turkish benchmarks show such transfer leads to the best-performing continuous model.}
}
@inproceedings{su-etal-2022-tacl,
	title        = {{T}a{CL}: Improving {BERT} Pre-training with Token-aware Contrastive Learning},
	author       = {Su, Yixuan  and Liu, Fangyu  and Meng, Zaiqiao  and Lan, Tian  and Shu, Lei  and Shareghi, Ehsan  and Collier, Nigel},
	year         = 2022,
	month        = jul,
	booktitle    = {Findings of the Association for Computational Linguistics: NAACL 2022},
	publisher    = {Association for Computational Linguistics},
	address      = {Seattle, United States},
	pages        = {2497--2507},
	doi          = {10.18653/v1/2022.findings-naacl.191},
	url          = {https://aclanthology.org/2022.findings-naacl.191},
	editor       = {Carpuat, Marine  and de Marneffe, Marie-Catherine  and Meza Ruiz, Ivan Vladimir},
	abstract     = {Masked language models (MLMs) such as BERT have revolutionized the field of Natural Language Understanding in the past few years. However, existing pre-trained MLMs often output an anisotropic distribution of token representations that occupies a narrow subset of the entire representation space. Such token representations are not ideal, especially for tasks that demand discriminative semantic meanings of distinct tokens. In this work, we propose TaCL (Token-aware Contrastive Learning), a novel continual pre-training approach that encourages BERT to learn an isotropic and discriminative distribution of token representations. TaCL is fully unsupervised and requires no additional data. We extensively test our approach on a wide range of English and Chinese benchmarks. The results show that TaCL brings consistent and notable improvements over the original BERT model. Furthermore, we conduct detailed analysis to reveal the merits and inner-workings of our approach.}
}
@inproceedings{clark-etal-2019-boolq,
	title        = {{B}ool{Q}: Exploring the Surprising Difficulty of Natural Yes/No Questions},
	author       = {Clark, Christopher  and Lee, Kenton  and Chang, Ming-Wei  and Kwiatkowski, Tom  and Collins, Michael  and Toutanova, Kristina},
	year         = 2019,
	month        = jun,
	booktitle    = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Minneapolis, Minnesota},
	pages        = {2924--2936},
	doi          = {10.18653/v1/N19-1300},
	url          = {https://aclanthology.org/N19-1300},
	editor       = {Burstein, Jill  and Doran, Christy  and Solorio, Thamar},
	abstract     = {In this paper we study yes/no questions that are naturally occurring {---} meaning that they are generated in unprompted and unconstrained settings. We build a reading comprehension dataset, BoolQ, of such questions, and show that they are unexpectedly challenging. They often query for complex, non-factoid information, and require difficult entailment-like inference to solve. We also explore the effectiveness of a range of transfer learning baselines. We find that transferring from entailment data is more effective than transferring from paraphrase or extractive QA data, and that it, surprisingly, continues to be very beneficial even when starting from massive pre-trained language models such as BERT. Our best method trains BERT on MultiNLI and then re-trains it on our train set. It achieves 80.4{\%} accuracy compared to 90{\%} accuracy of human annotators (and 62{\%} majority-baseline), leaving a significant gap for future work.}
}
@inproceedings{jin-etal-2019-pubmedqa,
	title        = {{P}ub{M}ed{QA}: A Dataset for Biomedical Research Question Answering},
	author       = {Jin, Qiao  and Dhingra, Bhuwan  and Liu, Zhengping  and Cohen, William  and Lu, Xinghua},
	year         = 2019,
	month        = nov,
	booktitle    = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
	publisher    = {Association for Computational Linguistics},
	address      = {Hong Kong, China},
	pages        = {2567--2577},
	doi          = {10.18653/v1/D19-1259},
	url          = {https://aclanthology.org/D19-1259},
	editor       = {Inui, Kentaro  and Jiang, Jing  and Ng, Vincent  and Wan, Xiaojun},
	abstract     = {We introduce PubMedQA, a novel biomedical question answering (QA) dataset collected from PubMed abstracts. The task of PubMedQA is to answer research questions with yes/no/maybe (e.g.: Do preoperative statins reduce atrial fibrillation after coronary artery bypass grafting?) using the corresponding abstracts. PubMedQA has 1k expert-annotated, 61.2k unlabeled and 211.3k artificially generated QA instances. Each PubMedQA instance is composed of (1) a question which is either an existing research article title or derived from one, (2) a context which is the corresponding abstract without its conclusion, (3) a long answer, which is the conclusion of the abstract and, presumably, answers the research question, and (4) a yes/no/maybe answer which summarizes the conclusion. PubMedQA is the first QA dataset where reasoning over biomedical research texts, especially their quantitative contents, is required to answer the questions. Our best performing model, multi-phase fine-tuning of BioBERT with long answer bag-of-word statistics as additional supervision, achieves 68.1{\%} accuracy, compared to single human performance of 78.0{\%} accuracy and majority-baseline of 55.2{\%} accuracy, leaving much room for improvement. PubMedQA is publicly available at \url{https://pubmedqa.github.io}.}
}
@inproceedings{dasigi-etal-2021-dataset,
	title        = {A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers},
	author       = {Dasigi, Pradeep  and Lo, Kyle  and Beltagy, Iz  and Cohan, Arman  and Smith, Noah A.  and Gardner, Matt},
	year         = 2021,
	month        = jun,
	booktitle    = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {4599--4610},
	doi          = {10.18653/v1/2021.naacl-main.365},
	url          = {https://aclanthology.org/2021.naacl-main.365},
	editor       = {Toutanova, Kristina  and Rumshisky, Anna  and Zettlemoyer, Luke  and Hakkani-Tur, Dilek  and Beltagy, Iz  and Bethard, Steven  and Cotterell, Ryan  and Chakraborty, Tanmoy  and Zhou, Yichao},
	abstract     = {Readers of academic research papers often read with the goal of answering specific questions. Question Answering systems that can answer those questions can make consumption of the content much more efficient. However, building such tools requires data that reflect the difficulty of the task arising from complex reasoning about claims made in multiple parts of a paper. In contrast, existing information-seeking question answering datasets usually contain questions about generic factoid-type information. We therefore present Qasper, a dataset of 5049 questions over 1585 Natural Language Processing papers. Each question is written by an NLP practitioner who read only the title and abstract of the corresponding paper, and the question seeks information present in the full text. The questions are then answered by a separate set of NLP practitioners who also provide supporting evidence to answers. We find that existing models that do well on other QA tasks do not perform well on answering these questions, underperforming humans by at least 27 F1 points when answering them from entire papers, motivating further research in document-grounded, information-seeking QA, which our dataset is designed to facilitate.}
}
@book{Fellbaum1998,
	title        = {{WordNet: An Electronic Lexical Database}},
	author       = {Fellbaum, Christiane},
	year         = 1998,
	month        = {05},
	publisher    = {The MIT Press},
	doi          = {10.7551/mitpress/7287.001.0001},
	isbn         = 9780262272551,
	url          = {https://doi.org/10.7551/mitpress/7287.001.0001},
	abstract     = {{WordNet is an on-line lexical reference system whose design isinspired by current psycholinguistic theories of human lexical memory;version 1.6 is the most up-to-date version of the system.WordNet, an electronic lexical database, is considered to be the most important resource available to researchers in computational linguistics, text analysis, and many related areas. Its design is inspired by current psycholinguistic and computational theories of human lexical memory. English nouns, verbs, adjectives, and adverbs are organized into synonym sets, each representing one underlying lexicalized concept. Different relations link the synonym sets.The purpose of this volume is twofold. First, it discusses the design of WordNet and the theoretical motivations behind it. Second, it provides a survey of representative applications, including word sense identification, information retrieval, selectional preferences of verbs, and lexical chains. Contributors Reem Al-Halimi, Robert C. Berwick, J. F. M. Burg, Martin Chodorow, Christiane Fellbaum, Joachim Grabowski, Sanda Harabagiu, Marti A. Hearst, Graeme Hirst, Douglas A. Jones, Rick Kazman, Karen T. Kohl, Shari Landes, Claudia Leacock, George A. Miller, Katherine J. Miller, Dan Moldovan, Naoyuki Nomura, Uta Priss, Philip Resnik, David St-Onge, Randee Tengi, Reind P. van de Riet, Ellen VoorheesBradford Books imprint}}
}
@article{joshi-etal-2020-spanbert,
	title        = {{S}pan{BERT}: Improving Pre-training by Representing and Predicting Spans},
	author       = {Joshi, Mandar  and Chen, Danqi  and Liu, Yinhan  and Weld, Daniel S.  and Zettlemoyer, Luke  and Levy, Omer},
	year         = 2020,
	journal      = {Transactions of the Association for Computational Linguistics},
	publisher    = {MIT Press},
	address      = {Cambridge, MA},
	volume       = 8,
	pages        = {64--77},
	doi          = {10.1162/tacl_a_00300},
	url          = {https://aclanthology.org/2020.tacl-1.5},
	editor       = {Johnson, Mark  and Roark, Brian  and Nenkova, Ani},
	abstract     = {We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our approach extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. SpanBERT consistently outperforms BERT and our better-tuned baselines, with substantial gains on span selection tasks such as question answering and coreference resolution. In particular, with the same training data and model size as BERTlarge, our single model obtains 94.6{\%} and 88.7{\%} F1 on SQuAD 1.1 and 2.0 respectively. We also achieve a new state of the art on the OntoNotes coreference resolution task (79.6{\%} F1), strong performance on the TACRED relation extraction benchmark, and even gains on GLUE.1}
}
@inproceedings{sennrich-etal-2016-neural,
	title        = {Neural Machine Translation of Rare Words with Subword Units},
	author       = {Sennrich, Rico  and Haddow, Barry  and Birch, Alexandra},
	year         = 2016,
	month        = aug,
	booktitle    = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Berlin, Germany},
	pages        = {1715--1725},
	doi          = {10.18653/v1/P16-1162},
	url          = {https://aclanthology.org/P16-1162},
	editor       = {Erk, Katrin  and Smith, Noah A.}
}
@inproceedings{kudo-2018-subword,
	title        = {Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates},
	author       = {Kudo, Taku},
	year         = 2018,
	month        = jul,
	booktitle    = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Melbourne, Australia},
	pages        = {66--75},
	doi          = {10.18653/v1/P18-1007},
	url          = {https://aclanthology.org/P18-1007},
	editor       = {Gurevych, Iryna  and Miyao, Yusuke},
	abstract     = {Subword units are an effective way to alleviate the open vocabulary problems in neural machine translation (NMT). While sentences are usually converted into unique subword sequences, subword segmentation is potentially ambiguous and multiple segmentations are possible even with the same vocabulary. The question addressed in this paper is whether it is possible to harness the segmentation ambiguity as a noise to improve the robustness of NMT. We present a simple regularization method, subword regularization, which trains the model with multiple subword segmentations probabilistically sampled during training. In addition, for better subword sampling, we propose a new subword segmentation algorithm based on a unigram language model. We experiment with multiple corpora and report consistent improvements especially on low resource and out-of-domain settings.}
}
@inproceedings{bostrom-durrett-2020-byte,
	title        = {Byte Pair Encoding is Suboptimal for Language Model Pretraining},
	author       = {Bostrom, Kaj  and Durrett, Greg},
	year         = 2020,
	month        = nov,
	booktitle    = {Findings of the Association for Computational Linguistics: EMNLP 2020},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {4617--4624},
	doi          = {10.18653/v1/2020.findings-emnlp.414},
	url          = {https://aclanthology.org/2020.findings-emnlp.414},
	editor       = {Cohn, Trevor  and He, Yulan  and Liu, Yang},
	abstract     = {The success of pretrained transformer language models (LMs) in natural language processing has led to a wide range of pretraining setups. In particular, these models employ a variety of subword tokenization methods, most notably byte-pair encoding (BPE) (Sennrich et al., 2016; Gage, 1994), the WordPiece method (Schuster and Nakajima, 2012), and unigram language modeling (Kudo, 2018), to segment text. However, to the best of our knowledge, the literature does not contain a direct evaluation of the impact of tokenization on language model pretraining. We analyze differences between BPE and unigram LM tokenization, finding that the latter method recovers subword units that align more closely with morphology and avoids problems stemming from BPE{'}s greedy construction procedure. We then compare the fine-tuned task performance of identical transformer masked language models pretrained with these tokenizations. Across downstream tasks and two languages (English and Japanese), we find that the unigram LM tokenization method matches or outperforms BPE. We hope that developers of future pretrained LMs will consider adopting the unigram LM method over the more prevalent BPE.}
}
@inproceedings{peters-etal-2018-deep,
	title        = {Deep Contextualized Word Representations},
	author       = {Peters, Matthew E.  and Neumann, Mark  and Iyyer, Mohit  and Gardner, Matt  and Clark, Christopher  and Lee, Kenton  and Zettlemoyer, Luke},
	year         = 2018,
	month        = jun,
	booktitle    = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {New Orleans, Louisiana},
	pages        = {2227--2237},
	doi          = {10.18653/v1/N18-1202},
	url          = {https://aclanthology.org/N18-1202},
	editor       = {Walker, Marilyn  and Ji, Heng  and Stent, Amanda},
	abstract     = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.}
}
@inproceedings{meister-etal-2023-natural,
	title        = {A Natural Bias for Language Generation Models},
	author       = {Meister, Clara  and Stokowiec, Wojciech  and Pimentel, Tiago  and Yu, Lei  and Rimell, Laura  and Kuncoro, Adhiguna},
	year         = 2023,
	month        = jul,
	booktitle    = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Toronto, Canada},
	pages        = {243--255},
	doi          = {10.18653/v1/2023.acl-short.22},
	url          = {https://aclanthology.org/2023.acl-short.22},
	editor       = {Rogers, Anna  and Boyd-Graber, Jordan  and Okazaki, Naoaki}
}
@inproceedings{lai-etal-2023-mitigating,
	title        = {Mitigating Data Imbalance and Representation Degeneration in Multilingual Machine Translation},
	author       = {Lai, Wen  and Chronopoulou, Alexandra  and Fraser, Alexander},
	year         = 2023,
	month        = dec,
	booktitle    = {Findings of the Association for Computational Linguistics: EMNLP 2023},
	publisher    = {Association for Computational Linguistics},
	address      = {Singapore},
	pages        = {14279--14294},
	doi          = {10.18653/v1/2023.findings-emnlp.953},
	url          = {https://aclanthology.org/2023.findings-emnlp.953},
	editor       = {Bouamor, Houda  and Pino, Juan  and Bali, Kalika}
}
@misc{godey2024anisotropy,
	title        = {Anisotropy Is Inherent to Self-Attention in Transformers},
	author       = {Nathan Godey and Éric de la Clergerie and Benoît Sagot},
	year         = 2024,
	eprint       = {2401.12143},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@article{freq-based-dist,
	title        = {Frequency-based Distortions in Contextualized Word Embeddings},
	author       = {Kaitlyn Zhou and Kawin Ethayarajh and Dan Jurafsky},
	year         = 2021,
	journal      = {CoRR},
	volume       = {abs/2104.08465},
	url          = {https://arxiv.org/abs/2104.08465},
	eprinttype   = {arXiv},
	eprint       = {2104.08465},
	timestamp    = {Mon, 26 Apr 2021 17:25:10 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2104-08465.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{gemmateam2024gemma,
	title        = {Gemma: Open Models Based on Gemini Research and Technology},
	author       = {Gemma Team and Thomas Mesnard and Cassidy Hardin and Robert Dadashi and Surya Bhupatiraju and Shreya Pathak and Laurent Sifre and Morgane Rivière and Mihir Sanjay Kale and Juliette Love and Pouya Tafti and Léonard Hussenot and Aakanksha Chowdhery and Adam Roberts and Aditya Barua and Alex Botev and Alex Castro-Ros and Ambrose Slone and Amélie Héliou and Andrea Tacchetti and Anna Bulanova and Antonia Paterson and Beth Tsai and Bobak Shahriari and Charline Le Lan and Christopher A. Choquette-Choo and Clément Crepy and Daniel Cer and Daphne Ippolito and David Reid and Elena Buchatskaya and Eric Ni and Eric Noland and Geng Yan and George Tucker and George-Christian Muraru and Grigory Rozhdestvenskiy and Henryk Michalewski and Ian Tenney and Ivan Grishchenko and Jacob Austin and James Keeling and Jane Labanowski and Jean-Baptiste Lespiau and Jeff Stanway and Jenny Brennan and Jeremy Chen and Johan Ferret and Justin Chiu and Justin Mao-Jones and Katherine Lee and Kathy Yu and Katie Millican and Lars Lowe Sjoesund and Lisa Lee and Lucas Dixon and Machel Reid and Maciej Mikuła and Mateo Wirth and Michael Sharman and Nikolai Chinaev and Nithum Thain and Olivier Bachem and Oscar Chang and Oscar Wahltinez and Paige Bailey and Paul Michel and Petko Yotov and Pier Giuseppe Sessa and Rahma Chaabouni and Ramona Comanescu and Reena Jana and Rohan Anil and Ross McIlroy and Ruibo Liu and Ryan Mullins and Samuel L Smith and Sebastian Borgeaud and Sertan Girgin and Sholto Douglas and Shree Pandya and Siamak Shakeri and Soham De and Ted Klimenko and Tom Hennigan and Vlad Feinberg and Wojciech Stokowiec and Yu-hui Chen and Zafarali Ahmed and Zhitao Gong and Tris Warkentin and Ludovic Peran and Minh Giang and Clément Farabet and Oriol Vinyals and Jeff Dean and Koray Kavukcuoglu and Demis Hassabis and Zoubin Ghahramani and Douglas Eck and Joelle Barral and Fernando Pereira and Eli Collins and Armand Joulin and Noah Fiedel and Evan Senter and Alek Andreev and Kathleen Kenealy},
	year         = 2024,
	eprint       = {2403.08295},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@software{together2023redpajama,
	title        = {RedPajama: an Open Dataset for Training Large Language Models},
	author       = {Together Computer},
	year         = 2023,
	month        = October,
	url          = {https://github.com/togethercomputer/RedPajama-Data}
}
@inproceedings{oscar,
	title        = {{Asynchronous Pipeline for Processing Huge Corpora on Medium to Low Resource Infrastructures}},
	author       = {Ortiz Su{\'a}rez, Pedro Javier and Sagot, Beno{\^i}t and Romary, Laurent},
	year         = 2019,
	booktitle    = {{7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)}},
	publisher    = {{Leibniz-Institut f{\"u}r Deutsche Sprache}},
	address      = {Cardiff, United Kingdom},
	doi          = {10.14618/IDS-PUB-9021},
	url          = {https://inria.hal.science/hal-02148693},
	editor       = {Piotr Ba{\'n}ski and Adrien Barbaresi and Hanno Biber and Evelyn Breiteneder and Simon Clematide and Marc Kupietz and Harald L{\"u}ngen and Caroline Iliadi},
	hal_pdf      = {https://inria.hal.science/hal-02148693/file/Asynchronous_Pipeline_for_Processing_Huge_Corpora_on_Medium_to_Low_Resource_Infrastructures.pdf}
}
@article{merrill-etal-2022-saturated,
	title        = {Saturated Transformers are Constant-Depth Threshold Circuits},
	author       = {Merrill, William  and Sabharwal, Ashish  and Smith, Noah A.},
	year         = 2022,
	journal      = {Transactions of the Association for Computational Linguistics},
	publisher    = {MIT Press},
	address      = {Cambridge, MA},
	volume       = 10,
	pages        = {843--856},
	doi          = {10.1162/tacl_a_00493},
	url          = {https://aclanthology.org/2022.tacl-1.49},
	editor       = {Roark, Brian  and Nenkova, Ani}
}
@inproceedings{tay2022scale,
	title        = {Scale Efficiently: Insights from Pretraining and Finetuning Transformers},
	author       = {Yi Tay and Mostafa Dehghani and Jinfeng Rao and William Fedus and Samira Abnar and Hyung Won Chung and Sharan Narang and Dani Yogatama and Ashish Vaswani and Donald Metzler},
	year         = 2022,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=f2OYVDyfIB}
}
@misc{petty2023impact,
	title        = {The Impact of Depth and Width on Transformer Language Model Generalization},
	author       = {Jackson Petty and Sjoerd van Steenkiste and Ishita Dasgupta and Fei Sha and Dan Garrette and Tal Linzen},
	year         = 2023,
	eprint       = {2310.19956},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@book{wals,
	title        = {WALS Online (v2020.3)},
	year         = 2013,
	publisher    = {Zenodo},
	doi          = {10.5281/zenodo.7385533},
	url          = {https://doi.org/10.5281/zenodo.7385533},
	editor       = {Matthew S. Dryer and Martin Haspelmath},
	type         = {Data set}
}
@inproceedings{saleva-lignos-2021-effectiveness,
	title        = {The Effectiveness of Morphology-aware Segmentation in Low-Resource Neural Machine Translation},
	author       = {Saleva, Jonne  and Lignos, Constantine},
	year         = 2021,
	month        = apr,
	booktitle    = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {164--174},
	doi          = {10.18653/v1/2021.eacl-srw.22},
	url          = {https://aclanthology.org/2021.eacl-srw.22},
	editor       = {Sorodoc, Ionut-Teodor  and Sushil, Madhumita  and Takmaz, Ece  and Agirre, Eneko},
	abstract     = {This paper evaluates the performance of several modern subword segmentation methods in a low-resource neural machine translation setting. We compare segmentations produced by applying BPE at the token or sentence level with morphologically-based segmentations from LMVR and MORSEL. We evaluate translation tasks between English and each of Nepali, Sinhala, and Kazakh, and predict that using morphologically-based segmentation methods would lead to better performance in this setting. However, comparing to BPE, we find that no consistent and reliable differences emerge between the segmentation methods. While morphologically-based methods outperform BPE in a few cases, what performs best tends to vary across tasks, and the performance of segmentation methods is often statistically indistinguishable.}
}
@article{Gage1994bpe,
	title        = {A new algorithm for data compression},
	author       = {Philip Gage},
	year         = 1994,
	journal      = {The C Users Journal archive},
	volume       = 12,
	pages        = {23--38},
	url          = {https://api.semanticscholar.org/CorpusID:59804030}
}
@article{mle,
	title        = {On the Mathematical Foundations of Theoretical Statistics},
	author       = {R. A. Fisher},
	year         = 1922,
	journal      = {Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character},
	publisher    = {Royal Society},
	volume       = 222,
	pages        = {309--368},
	issn         = {02643952},
	url          = {http://www.jstor.org/stable/91208},
	urldate      = {2024-08-09}
}
@book{jurafsky_course,
	title        = {Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition},
	author       = {Jurafsky, Daniel and Martin, James H.},
	year         = 2000,
	publisher    = {Prentice Hall PTR},
	address      = {USA},
	isbn         = {0130950696},
	edition      = {1st},
	abstract     = {From the Publisher:This book takes an empirical approach to language processing, based on applying statistical and other machine-learning algorithms to large corpora.   Methodology   boxes are included in each chapter.  Each chapter is built around one or more worked examples  to demonstrate the main idea of the chapter. Covers the fundamental algorithms of various fields, whether originally proposed for spoken or written language to demonstrate how the same algorithm can be used for speech recognition and word-sense disambiguation. Emphasis on web and other practical applications. Emphasis on scientific evaluation. Useful as a reference for professionals in any of the areas of speech and language processing.}
}
@inbook{rnn_origins,
	title        = {Learning Internal Representations by Error Propagation},
	author       = {Rumelhart, David E. and McClelland, James L.},
	year         = 1987,
	booktitle    = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition: Foundations},
	volume       = {},
	number       = {},
	pages        = {318--362},
	doi          = {},
	keywords     = {}
}
@inproceedings{xiao2024efficient,
	title        = {Efficient Streaming Language Models with Attention Sinks},
	author       = {Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis},
	year         = 2024,
	booktitle    = {The Twelfth International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=NG7sS51zVF}
}
@misc{oren2024transformersmultistaternns,
	title        = {Transformers are Multi-State RNNs},
	author       = {Matanel Oren and Michael Hassid and Nir Yarden and Yossi Adi and Roy Schwartz},
	year         = 2024,
	url          = {https://arxiv.org/abs/2401.06104},
	eprint       = {2401.06104},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@inproceedings{keyformer,
	title        = {Keyformer: KV Cache reduction through key tokens selection for Efficient Generative Inference},
	author       = {Adnan, Muhammad and Arunkumar, Akhil and Jain, Gaurav and Nair, Prashant and Soloveychik, Ilya and Kamath, Purushotham},
	year         = 2024,
	booktitle    = {Proceedings of Machine Learning and Systems},
	volume       = 6,
	pages        = {114--127},
	url          = {https://proceedings.mlsys.org/paper_files/paper/2024/file/48fecef47b19fe501d27d338b6d52582-Paper-Conference.pdf},
	editor       = {P. Gibbons and G. Pekhimenko and C. De Sa}
}
@inproceedings{nawrot2024dynamic,
	title        = {Dynamic Memory Compression: Retrofitting {LLM}s for Accelerated Inference},
	author       = {Piotr Nawrot and Adrian {\L}a{\'n}cucki and Marcin Chochowski and David Tarjan and Edoardo Ponti},
	year         = 2024,
	booktitle    = {Forty-first International Conference on Machine Learning},
	url          = {https://openreview.net/forum?id=tDRYrAkOB7}
}
@misc{shi2024costdownreviewmethods,
	title        = {Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption},
	author       = {Luohe Shi and Hongyi Zhang and Yao Yao and Zuchao Li and Hai Zhao},
	year         = 2024,
	url          = {https://arxiv.org/abs/2407.18003},
	eprint       = {2407.18003},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@inproceedings{h2o,
	title        = {H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models},
	author       = {Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R\'{e}, Christopher and Barrett, Clark and Wang, Zhangyang "Atlas" and Chen, Beidi},
	year         = 2023,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 36,
	pages        = {34661--34710},
	url          = {https://proceedings.neurips.cc/paper_files/paper/2023/file/6ceefa7b15572587b78ecfcebb2827f8-Paper-Conference.pdf},
	editor       = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine}
}
@inproceedings{ainslie-etal-2023-gqa,
	title        = {{GQA}: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
	author       = {Ainslie, Joshua  and Lee-Thorp, James  and de Jong, Michiel  and Zemlyanskiy, Yury  and Lebron, Federico  and Sanghai, Sumit},
	year         = 2023,
	month        = dec,
	booktitle    = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
	publisher    = {Association for Computational Linguistics},
	address      = {Singapore},
	pages        = {4895--4901},
	doi          = {10.18653/v1/2023.emnlp-main.298},
	url          = {https://aclanthology.org/2023.emnlp-main.298},
	editor       = {Bouamor, Houda  and Pino, Juan  and Bali, Kalika},
	abstract     = {Multi-query attention (MQA), which only uses a single key-value head, drastically speeds up decoder inference. However, MQA can lead to quality degradation, and moreover it may not be desirable to train a separate model just for faster inference. We (1) propose a recipe for uptraining existing multi-head language model checkpoints into models with MQA using 5{\%} of original pre-training compute, and (2) introduce grouped-query attention (GQA), a generalization of multi-query attention which uses an intermediate (more than one, less than number of query heads) number of key-value heads. We show that uptrained GQA achieves quality close to multi-head attention with comparable speed to MQA.}
}
@misc{shazeer2019fasttransformerdecodingwritehead,
	title        = {Fast Transformer Decoding: One Write-Head is All You Need},
	author       = {Noam Shazeer},
	year         = 2019,
	url          = {https://arxiv.org/abs/1911.02150},
	eprint       = {1911.02150},
	archiveprefix = {arXiv},
	primaryclass = {cs.NE}
}
@article{HochSchm97,
	title        = {Long Short-Term Memory},
	author       = {Sepp Hochreiter and Jürgen Schmidhuber},
	year         = 1997,
	journal      = {Neural Computation},
	volume       = 9,
	number       = 8,
	pages        = {1735--1780},
	optabstract  = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	optdoi       = {10.1162/neco.1997.9.8.1735},
	opteprint    = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
	opturl       = {http://dx.doi.org/10.1162/neco.1997.9.8.1735}
}
@inproceedings{miyamoto-cho-2016-gated,
	title        = {Gated Word-Character Recurrent Language Model},
	author       = {Miyamoto, Yasumasa  and Cho, Kyunghyun},
	year         = 2016,
	month        = nov,
	booktitle    = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
	publisher    = {Association for Computational Linguistics},
	address      = {Austin, Texas},
	pages        = {1992--1997},
	doi          = {10.18653/v1/D16-1209},
	url          = {https://aclanthology.org/D16-1209},
	editor       = {Su, Jian  and Duh, Kevin  and Carreras, Xavier}
}
@inproceedings{weight_decay,
	title        = {A simple weight decay can improve generalization},
	author       = {Krogh, Anders and Hertz, John A.},
	year         = 1991,
	booktitle    = {Proceedings of the 4th International Conference on Neural Information Processing Systems},
	location     = {Denver, Colorado},
	publisher    = {Morgan Kaufmann Publishers Inc.},
	address      = {San Francisco, CA, USA},
	series       = {NIPS'91},
	pages        = {950–957},
	isbn         = 1558602224,
	abstract     = {It has been observed in numerical simulations that a weight decay can improve generalization in a feed-forward neural network. This paper explains why. It is proven that a weight decay has two effects in a linear network. First, it suppresses any irrelevant components of the weight vector by choosing the smallest vector that solves the learning problem. Second, if the size is chosen right, a weight decay can suppress some of the effects of static noise on the targets, which improves generalization quite a lot. It is then shown how to extend these results to networks with hidden layers and non-linear units. Finally the theory is confirmed by some numerical simulations using the data from NetTalk.},
	numpages     = 8
}
@article{morales-brotons2024exponential,
	title        = {Exponential Moving Average of Weights in Deep Learning: Dynamics and Benefits},
	author       = {Daniel Morales-Brotons and Thijs Vogels and Hadrien Hendrikx},
	year         = 2024,
	journal      = {Transactions on Machine Learning Research},
	issn         = {2835-8856},
	url          = {https://openreview.net/forum?id=2M9CUnYnBA},
	note         = {}
}
@inproceedings{9709917,
	title        = {On Feature Decorrelation in Self-Supervised Learning},
	author       = {T. Hua and W. Wang and Z. Xue and S. Ren and Y. Wang and H. Zhao},
	year         = 2021,
	month        = {oct},
	booktitle    = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
	publisher    = {IEEE Computer Society},
	address      = {Los Alamitos, CA, USA},
	volume       = {},
	pages        = {9578--9588},
	doi          = {10.1109/ICCV48922.2021.00946},
	issn         = {},
	url          = {https://doi.ieeecomputersociety.org/10.1109/ICCV48922.2021.00946},
	abstract     = {In self-supervised representation learning, a common idea behind most of the state-of-the-art approaches is to enforce the robustness of the representations to predefined augmentations. A potential issue of this idea is the existence of completely collapsed solutions (i.e., constant features), which are typically avoided implicitly by carefully chosen implementation details. In this work, we study a relatively concise framework containing the most common components from recent approaches. We verify the existence of complete collapse and discover another reachable collapse pattern that is usually overlooked, namely dimensional collapse. We connect dimensional collapse with strong correlations between axes and consider such connection as a strong motivation for feature decorrelation (i.e., standardizing the covariance matrix). The gains from feature decorrelation are verified empirically to highlight the importance and the potential of this insight.},
	keywords     = {representation learning;computer vision;correlation;robustness;decorrelation;covariance matrices}
}
@inproceedings{pmlr-v139-tian21a,
	title        = {Understanding self-supervised learning dynamics without contrastive pairs},
	author       = {Tian, Yuandong and Chen, Xinlei and Ganguli, Surya},
	year         = 2021,
	month        = {18--24 Jul},
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = 139,
	pages        = {10268--10278},
	url          = {https://proceedings.mlr.press/v139/tian21a.html},
	editor       = {Meila, Marina and Zhang, Tong},
	pdf          = {http://proceedings.mlr.press/v139/tian21a/tian21a.pdf},
	abstract     = {While contrastive approaches of self-supervised learning (SSL) learn representations by minimizing the distance between two augmented views of the same data point (positive pairs) and maximizing views from different data points (negative pairs), recent \emph{non-contrastive} SSL (e.g., BYOL and SimSiam) show remarkable performance {\it without} negative pairs, with an extra learnable predictor and a stop-gradient operation. A fundamental question rises: why they do not collapse into trivial representation? In this paper, we answer this question via a simple theoretical study and propose a novel approach, \ourmethod{}, that \emph{directly} sets the linear predictor based on the statistics of its inputs, rather than trained with gradient update. On ImageNet, it performs comparably with more complex two-layer non-linear predictors that employ BatchNorm and outperforms linear predictor by $2.5%$ in 300-epoch training (and $5%$ in 60-epoch). \ourmethod{} is motivated by our theoretical study of the nonlinear learning dynamics of non-contrastive SSL in simple linear networks. Our study yields conceptual insights into how non-contrastive SSL methods learn, how they avoid representational collapse, and how multiple factors, like predictor networks, stop-gradients, exponential moving averages, and weight decay all come into play. Our simple theory recapitulates the results of real-world ablation studies in both STL-10 and ImageNet. Code is released\footnote{\url{https://github.com/facebookresearch/luckmatters/tree/master/ssl}}.}
}
@misc{devoto2024simpleeffectivel2normbased,
	title        = {A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache Compression},
	author       = {Alessio Devoto and Yu Zhao and Simone Scardapane and Pasquale Minervini},
	year         = 2024,
	url          = {https://arxiv.org/abs/2406.11430},
	eprint       = {2406.11430},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@misc{logit_lens,
	title        = {interpreting gpt: the logit lens},
	author       = {Nostalgebraist},
	year         = 2020,
	url          = {https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens}
}
@inproceedings{vig-belinkov-2019-analyzing,
	title        = {Analyzing the Structure of Attention in a Transformer Language Model},
	author       = {Vig, Jesse  and Belinkov, Yonatan},
	year         = 2019,
	month        = aug,
	booktitle    = {Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
	publisher    = {Association for Computational Linguistics},
	address      = {Florence, Italy},
	pages        = {63--76},
	doi          = {10.18653/v1/W19-4808},
	url          = {https://aclanthology.org/W19-4808},
	editor       = {Linzen, Tal  and Chrupa{\l}a, Grzegorz  and Belinkov, Yonatan  and Hupkes, Dieuwke},
	abstract     = {The Transformer is a fully attention-based alternative to recurrent networks that has achieved state-of-the-art results across a range of NLP tasks. In this paper, we analyze the structure of attention in a Transformer language model, the GPT-2 small pretrained model. We visualize attention for individual instances and analyze the interaction between attention and syntax over a large corpus. We find that attention targets different parts of speech at different layer depths within the model, and that attention aligns with dependency relations most strongly in the middle layers. We also find that the deepest layers of the model capture the most distant relationships. Finally, we extract exemplar sentences that reveal highly specific patterns targeted by particular attention heads.}
}
@inproceedings{dar-etal-2023-analyzing,
	title        = {Analyzing Transformers in Embedding Space},
	author       = {Dar, Guy  and Geva, Mor  and Gupta, Ankit  and Berant, Jonathan},
	year         = 2023,
	month        = jul,
	booktitle    = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Toronto, Canada},
	pages        = {16124--16170},
	doi          = {10.18653/v1/2023.acl-long.893},
	url          = {https://aclanthology.org/2023.acl-long.893},
	editor       = {Rogers, Anna  and Boyd-Graber, Jordan  and Okazaki, Naoaki},
	abstract     = {Understanding Transformer-based models has attracted significant attention, as they lie at the heart of recent technological advances across machine learning. While most interpretability methods rely on running models over inputs, recent work has shown that a zero-pass approach, where parameters are interpreted directly without a forward/backward pass is feasible for some Transformer parameters, and for two-layer attention networks. In this work, we present a theoretical analysis where all parameters of a trained Transformer are interpreted by projecting them into the embedding space, that is, the space of vocabulary items they operate on. We derive a simple theoretical framework to support our arguments and provide ample evidence for its validity. First, an empirical analysis showing that parameters of both pretrained and fine-tuned models can be interpreted in embedding space. Second, we present two applications of our framework: (a) aligning the parameters of different models that share a vocabulary, and (b) constructing a classifier without training by {``}translating{''} the parameters of a fine-tuned classifier to parameters of a different model that was only pretrained. Overall, our findings open the door to interpretation methods that, at least in part, abstract away from model specifics and operate in the embedding space only.}
}
@inproceedings{prakash-lee-2023-layered,
	title        = {Layered Bias: Interpreting Bias in Pretrained Large Language Models},
	author       = {Prakash, Nirmalendu  and Lee, Roy Ka-Wei},
	year         = 2023,
	month        = dec,
	booktitle    = {Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP},
	publisher    = {Association for Computational Linguistics},
	address      = {Singapore},
	pages        = {284--295},
	doi          = {10.18653/v1/2023.blackboxnlp-1.22},
	url          = {https://aclanthology.org/2023.blackboxnlp-1.22},
	editor       = {Belinkov, Yonatan  and Hao, Sophie  and Jumelet, Jaap  and Kim, Najoung  and McCarthy, Arya  and Mohebbi, Hosein},
	abstract     = {Large language models (LLMs) like GPT and PALM have excelled in numerous natural language processing (NLP) tasks such as text generation, question answering, and translation. However, they are also found to have inherent social biases. To address this, recent studies have proposed debiasing techniques like iterative nullspace projection (INLP) and Counterfactual Data Augmentation (CDA). Additionally, there{'}s growing interest in understanding the intricacies of these models. Some researchers focus on individual neural units, while others examine specific layers. In our study, we benchmark newly released models, assess the impact of debiasing methods, and investigate how biases are linked to different transformer layers using a method called Logit Lens. Specifically, we evaluate three modern LLMs: OPT, LLaMA, and LLaMA2, and their debiased versions. Our experiments are based on two popular bias evaluation datasets, StereoSet and CrowS-Pairs, and we perform a layer-by-layer analysis using the Logit Lens.}
}
@inproceedings{voita-etal-2019-analyzing,
	title        = {Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned},
	author       = {Voita, Elena  and Talbot, David  and Moiseev, Fedor  and Sennrich, Rico  and Titov, Ivan},
	year         = 2019,
	month        = jul,
	booktitle    = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	publisher    = {Association for Computational Linguistics},
	address      = {Florence, Italy},
	pages        = {5797--5808},
	doi          = {10.18653/v1/P19-1580},
	url          = {https://aclanthology.org/P19-1580},
	editor       = {Korhonen, Anna  and Traum, David  and M{\`a}rquez, Llu{\'\i}s},
	abstract     = {Multi-head self-attention is a key component of the Transformer, a state-of-the-art architecture for neural machine translation. In this work we evaluate the contribution made by individual attention heads to the overall performance of the model and analyze the roles played by them in the encoder. We find that the most important and confident heads play consistent and often linguistically-interpretable roles. When pruning heads using a method based on stochastic gates and a differentiable relaxation of the L0 penalty, we observe that specialized heads are last to be pruned. Our novel pruning method removes the vast majority of heads without seriously affecting performance. For example, on the English-Russian WMT dataset, pruning 38 out of 48 encoder heads results in a drop of only 0.15 BLEU.}
}
@conference{bahdanau_nmt,
	title        = {Neural machine translation by jointly learning to align and translate},
	author       = {Dzmitry Bahdanau and Cho, {Kyung Hyun} and Yoshua Bengio},
	year         = 2015,
	month        = jan,
	day          = 1,
	note         = {3rd International Conference on Learning Representations, ICLR 2015 ; Conference date: 07-05-2015 Through 09-05-2015},
	abstract     = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder–decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	language     = {English (US)}
}
@misc{cho2014learningphraserepresentationsusing,
	title        = {Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation},
	author       = {Kyunghyun Cho and Bart van Merrienboer and Caglar Gulcehre and Dzmitry Bahdanau and Fethi Bougares and Holger Schwenk and Yoshua Bengio},
	year         = 2014,
	url          = {https://arxiv.org/abs/1406.1078},
	eprint       = {1406.1078},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@article{silu,
	title        = {Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning},
	author       = {Elfwing, Stefan and Uchibe, Eiji and Doya, Kenji},
	year         = 2018,
	month        = {01},
	journal      = {Neural Networks},
	volume       = 107,
	pages        = {},
	doi          = {10.1016/j.neunet.2017.12.012}
}
@misc{hendrycks2023gaussianerrorlinearunits,
	title        = {Gaussian Error Linear Units (GELUs)},
	author       = {Dan Hendrycks and Kevin Gimpel},
	year         = 2023,
	url          = {https://arxiv.org/abs/1606.08415},
	eprint       = {1606.08415},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@article{relu,
	title        = {Visual Feature Extraction by a Multilayered Network of Analog Threshold Elements},
	author       = {Fukushima, Kunihiko},
	year         = 1969,
	journal      = {IEEE Transactions on Systems Science and Cybernetics},
	volume       = 5,
	number       = 4,
	pages        = {322--333},
	doi          = {10.1109/TSSC.1969.300225},
	keywords     = {Feature extraction;Network synthesis;Biological system modeling;Computational modeling;Computer simulation;Biology computing;Computer networks;Character recognition;Handwriting recognition;Biological systems}
}
@misc{ba2016layernormalization,
	title        = {Layer Normalization},
	author       = {Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
	year         = 2016,
	url          = {https://arxiv.org/abs/1607.06450},
	eprint       = {1607.06450},
	archiveprefix = {arXiv},
	primaryclass = {stat.ML}
}
@inproceedings{residual_conn,
	title        = {Deep Residual Learning for Image Recognition},
	author       = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year         = 2016,
	booktitle    = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	volume       = {},
	number       = {},
	pages        = {770--778},
	doi          = {10.1109/CVPR.2016.90},
	keywords     = {Training;Degradation;Complexity theory;Image recognition;Neural networks;Visualization;Image segmentation}
}
@inproceedings{rnn_eval,
	title        = {Empirical evaluation of gated recurrent neural networks on sequence modeling},
	author       = {Junyoung Chung and Caglar Gulcehre and Kyunghyun Cho and Yoshua Bengio},
	year         = 2014,
	booktitle    = {NIPS 2014 Workshop on Deep Learning, December 2014},
	language     = {English (US)}
}
@inproceedings{mikolov10_interspeech,
	title        = {{Recurrent neural network based language model}},
	author       = {Tomáš Mikolov and Martin Karafiát and Lukáš Burget and Jan Černocký and Sanjeev Khudanpur},
	year         = 2010,
	booktitle    = {Proc. Interspeech 2010},
	pages        = {1045--1048},
	doi          = {10.21437/Interspeech.2010-343},
	issn         = {2958-1796}
}
@article{kneser_ney,
	title        = {On structuring probabilistic dependences in stochastic language modelling},
	author       = {Hermann Ney and Ute Essen and Reinhard Kneser},
	year         = 1994,
	journal      = {Computer Speech \& Language},
	volume       = 8,
	number       = 1,
	pages        = {1--38},
	doi          = {https://doi.org/10.1006/csla.1994.1001},
	issn         = {0885-2308},
	url          = {https://www.sciencedirect.com/science/article/pii/S0885230884710011},
	abstract     = {In this paper, we study the problem of stochastic language modelling from the viewpoint of introducing suitable structures into the conditional probability distributions. The task of these distributions is to predict the probability of a new word by looking at M or even all predecessor words. The conventional approach is to limit M to 1 or 2 and to interpolate the resulting bigram and trigram models with a unigram model in a linear fashion. However, there are many other structures that can be used to model the probabilistic dependences between the predecessor word and the word to be predicted. The structures considered in this paper are: nonlinear interpolation as an alternative to linear interpolation; equivalence classes for word histories and single words; cache memory and word associations. For the optimal estimation of nonlinear and linear interpolation parameters, the leaving-one-out method is systematically used. For the determination of word equivalence classes in a bigram model, an automatic clustering procedure has been adapted. To capture long-distance dependences, we consider various models for word-by-word dependences; the cache model may be viewed as a special type of self-association. Experimental results are presented for two text databases, a Germany database and an English database.}
}
@article{bengio2000neural,
	title        = {A neural probabilistic language model},
	author       = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal},
	year         = 2000,
	journal      = {Advances in neural information processing systems},
	volume       = 13
}
@inproceedings{provilkov-etal-2020-bpe,
	title        = {{BPE}-Dropout: Simple and Effective Subword Regularization},
	author       = {Provilkov, Ivan  and Emelianenko, Dmitrii  and Voita, Elena},
	year         = 2020,
	month        = jul,
	booktitle    = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {1882--1892},
	doi          = {10.18653/v1/2020.acl-main.170},
	url          = {https://aclanthology.org/2020.acl-main.170},
	editor       = {Jurafsky, Dan  and Chai, Joyce  and Schluter, Natalie  and Tetreault, Joel},
	abstract     = {Subword segmentation is widely used to address the open vocabulary problem in machine translation. The dominant approach to subword segmentation is Byte Pair Encoding (BPE), which keeps the most frequent words intact while splitting the rare ones into multiple tokens. While multiple segmentations are possible even with the same vocabulary, BPE splits words into unique sequences; this may prevent a model from better learning the compositionality of words and being robust to segmentation errors. So far, the only way to overcome this BPE imperfection, its deterministic nature, was to create another subword segmentation algorithm (Kudo, 2018). In contrast, we show that BPE itself incorporates the ability to produce multiple segmentations of the same word. We introduce BPE-dropout - simple and effective subword regularization method based on and compatible with conventional BPE. It stochastically corrupts the segmentation procedure of BPE, which leads to producing multiple segmentations within the same fixed BPE framework. Using BPE-dropout during training and the standard BPE during inference improves translation quality up to 2.3 BLEU compared to BPE and up to 0.9 BLEU compared to the previous subword regularization.}
}
@inproceedings{gronroos-etal-2018-cognate,
	title        = {Cognate-aware morphological segmentation for multilingual neural translation},
	author       = {Gr{\"o}nroos, Stig-Arne  and Virpioja, Sami  and Kurimo, Mikko},
	year         = 2018,
	month        = oct,
	booktitle    = {Proceedings of the Third Conference on Machine Translation: Shared Task Papers},
	publisher    = {Association for Computational Linguistics},
	address      = {Belgium, Brussels},
	pages        = {386--393},
	doi          = {10.18653/v1/W18-6410},
	url          = {https://aclanthology.org/W18-6410},
	editor       = {Bojar, Ond{\v{r}}ej  and Chatterjee, Rajen  and Federmann, Christian  and Fishel, Mark  and Graham, Yvette  and Haddow, Barry  and Huck, Matthias  and Yepes, Antonio Jimeno  and Koehn, Philipp  and Monz, Christof  and Negri, Matteo  and N{\'e}v{\'e}ol, Aur{\'e}lie  and Neves, Mariana  and Post, Matt  and Specia, Lucia  and Turchi, Marco  and Verspoor, Karin},
	abstract     = {This article describes the Aalto University entry to the WMT18 News Translation Shared Task. We participate in the multilingual subtrack with a system trained under the constrained condition to translate from English to both Finnish and Estonian. The system is based on the Transformer model. We focus on improving the consistency of morphological segmentation for words that are similar orthographically, semantically, and distributionally; such words include etymological cognates, loan words, and proper names. For this, we introduce Cognate Morfessor, a multilingual variant of the Morfessor method. We show that our approach improves the translation quality particularly for Estonian, which has less resources for training the translation model.}
}
@inproceedings{press-wolf-2017-using,
	title        = {Using the Output Embedding to Improve Language Models},
	author       = {Press, Ofir  and Wolf, Lior},
	year         = 2017,
	month        = apr,
	booktitle    = {Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},
	publisher    = {Association for Computational Linguistics},
	address      = {Valencia, Spain},
	pages        = {157--163},
	url          = {https://aclanthology.org/E17-2025},
	editor       = {Lapata, Mirella  and Blunsom, Phil  and Koller, Alexander},
	abstract     = {We study the topmost weight matrix of neural network language models. We show that this matrix constitutes a valid word embedding. When training language models, we recommend tying the input embedding and this output embedding. We analyze the resulting update rules and show that the tied embedding evolves in a more similar way to the output embedding than to the input embedding in the untied model. We also offer a new method of regularizing the output embedding. Our methods lead to a significant reduction in perplexity, as we are able to show on a variety of neural network language models. Finally, we show that weight tying can reduce the size of neural translation models to less than half of their original size without harming their performance.}
}
@book{aronoff2022morphology,
	title        = {What is morphology?},
	author       = {Aronoff, Mark and Fudeman, Kirsten},
	year         = 2022,
	publisher    = {John Wiley \& Sons}
}
@inproceedings{clement_maf,
	title        = {{MAF: a Morphosyntactic Annotation Framework}},
	author       = {Cl{\'e}ment, Lionel and Villemonte de La Clergerie, {\'E}ric},
	year         = 2005,
	booktitle    = {{2nd Language \& Technology Conference (LTC'05)}},
	address      = {Poznan, Poland},
	series       = {2nd Language \& Technology Conference (LTC'05)},
	pages        = {90--94},
	url          = {https://hal.science/hal-01104466},
	editor       = {Vetulani, Zygmunt},
	keywords     = {multiword expression ; computational lexicon},
	hal_id       = {hal-01104466},
	hal_version  = {v1}
}
@misc{mielke2021wordscharactersbriefhistory,
	title        = {Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP},
	author       = {Sabrina J. Mielke and Zaid Alyafeai and Elizabeth Salesky and Colin Raffel and Manan Dey and Matthias Gallé and Arun Raja and Chenglei Si and Wilson Y. Lee and Benoît Sagot and Samson Tan},
	year         = 2021,
	url          = {https://arxiv.org/abs/2112.10508},
	eprint       = {2112.10508},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@inbook{longacre,
	title        = {Hierarchy in Language},
	author       = {Robert E. Longacre},
	year         = 1970,
	booktitle    = {Method and Theory in Linguistics},
	publisher    = {De Gruyter Mouton},
	address      = {Berlin, Boston},
	pages        = {173--196},
	doi          = {doi:10.1515/9783110872521.173},
	isbn         = 9783110872521,
	url          = {https://doi.org/10.1515/9783110872521.173},
	editor       = {Paul L. Garvin},
	lastchecked  = {2024-08-29}
}
@article{jaro,
	title        = {Advances in Record-Linkage Methodology as Applied to Matching the 1985 Census of Tampa, Florida},
	author       = {Matthew A. Jaro},
	year         = 1989,
	journal      = {Journal of the American Statistical Association},
	publisher    = {ASA Website},
	volume       = 84,
	number       = 406,
	pages        = {414--420},
	doi          = {10.1080/01621459.1989.10478785},
	url          = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1989.10478785},
	eprint       = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1989.10478785}
}
@article{baharudin2010review,
	title        = {A Review of Machine Learning Algorithms for Text-Documents Classification},
	author       = {Baharudin, Baharum and Lee, Lam Hong and Khan, Khairullah},
	year         = 2010,
	journal      = {Journal of Advances in Information Technology},
	publisher    = {Engineering and Technology Publishing},
	volume       = 1,
	number       = 1,
	pages        = 4
}
@book{zipf_psycho-biology_1935,
	title        = {The psycho-biology of language : an introduction to dynamic philology},
	shorttitle   = {The psycho-biology of language},
	author       = {Zipf, George Kingsley},
	year         = 1935,
	publisher    = {Houghton Mifflin},
	address      = {Oxford, England},
	series       = {The psycho-biology of language: an introduction to dynamic philology},
	abstract     = {An account of discoveries pertaining to linguistic change, presenting many problems to the psychologist whose interest lies in speech-behavior or meaning.  Harvard Book List (edited) 1955 \#268 (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	added-at     = {2021-11-29T12:47:08.000+0100},
	biburl       = {https://www.bibsonomy.org/bibtex/2bd6861139bfec20e99c41def3897b9e9/lepsky},
	interhash    = {ccec2f286b0b2ff9db6fe1b004971b9e},
	intrahash    = {bd6861139bfec20e99c41def3897b9e9},
	keywords     = {statistik termgewichtung},
	timestamp    = {2021-11-29T12:47:08.000+0100}
}
@book{chowdhury2010introduction,
	title        = {Introduction to modern information retrieval},
	author       = {Chowdhury, Gobinda G},
	year         = 2010,
	publisher    = {Facet publishing}
}
@article{hamming,
	title        = {Error detecting and error correcting codes},
	author       = {Hamming, R. W.},
	year         = 1950,
	journal      = {The Bell System Technical Journal},
	volume       = 29,
	number       = 2,
	pages        = {147--160},
	doi          = {10.1002/j.1538-7305.1950.tb00463.x},
	keywords     = {}
}
@inproceedings{levenshtein1966binary,
	title        = {Binary Codes Capable of Correcting Deletions, Insertions and Reversals},
	author       = {Levenshtein, VI},
	year         = 1966,
	booktitle    = {Soviet Physics Doklady},
	volume       = 10,
	pages        = 707
}

@article{anthropo,
	abstract = {This essay focuses on anthropomorphism as both a form of hype and fallacy. As a form of hype, anthropomorphism is shown to exaggerate AI capabilities and performance by attributing human-like traits to systems that do not possess them. As a fallacy, anthropomorphism is shown to distort moral judgments about AI, such as those concerning its moral character and status, as well as judgments of responsibility and trust. By focusing on these two dimensions of anthropomorphism in AI, the essay highlights negative ethical consequences of the phenomenon in this field.},
	author = {Placani, Adriana},
	date = {2024/08/01},
	date-added = {2024-09-27 17:51:30 +0200},
	date-modified = {2024-09-27 17:51:30 +0200},
	doi = {10.1007/s43681-024-00419-4},
	id = {Placani2024},
	isbn = {2730-5961},
	journal = {AI and Ethics},
	number = {3},
	pages = {691--698},
	title = {Anthropomorphism in AI: hype and fallacy},
	url = {https://doi.org/10.1007/s43681-024-00419-4},
	volume = {4},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1007/s43681-024-00419-4}}

@inproceedings{
xu2021how,
title={How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks},
author={Keyulu Xu and Mozhi Zhang and Jingling Li and Simon Shaolei Du and Ken-Ichi Kawarabayashi and Stefanie Jegelka},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=UH-cmocLJC}
}

@article{turing1950computing,
  added-at = {2012-06-19T15:53:23.000+0200},
  author = {Turing, A. M.},
  biburl = {https://www.bibsonomy.org/bibtex/2c6b8db241dec2cec3477ce771abebb8f/jaeschke},
  copyright = {Copyright © 1950 Oxford University Press},
  interhash = {3f7a151a4f79fe75b4bb148b41279a9b},
  intrahash = {c6b8db241dec2cec3477ce771abebb8f},
  issn = {00264423},
  journal = {Mind},
  jstor_articletype = {research-article},
  jstor_formatteddate = {Oct., 1950},
  keywords = {},
  language = {English},
  number = 236,
  pages = {433--460},
  publisher = {Oxford University Press on behalf of the Mind Association},
  series = {New Series},
  timestamp = {2012-06-19T15:53:23.000+0200},
  title = {Computing Machinery and Intelligence},
  url = {http://www.jstor.org/stable/2251299},
  volume = 59,
  year = 1950
}


@inproceedings{
huang2024compression,
title={Compression Represents Intelligence Linearly},
author={Yuzhen Huang and Jinghan Zhang and Zifei Shan and Junxian He},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=SHMj84U5SH}
}

@inproceedings{
deletang2024language,
title={Language Modeling Is Compression},
author={Gregoire Deletang and Anian Ruoss and Paul-Ambroise Duquenne and Elliot Catt and Tim Genewein and Christopher Mattern and Jordi Grau-Moya and Li Kevin Wenliang and Matthew Aitchison and Laurent Orseau and Marcus Hutter and Joel Veness},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=jznbgiynus}
}

@article{lecun2022path,
  title={A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27},
  author={LeCun, Yann},
  journal={Open Review},
  volume={62},
  number={1},
  pages={1--62},
  year={2022}
}

@misc{penedo2024finewebdatasetsdecantingweb,
      title={The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale}, 
      author={Guilherme Penedo and Hynek Kydlíček and Loubna Ben allal and Anton Lozhkov and Margaret Mitchell and Colin Raffel and Leandro Von Werra and Thomas Wolf},
      year={2024},
      eprint={2406.17557},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.17557}
}

@inproceedings{
hu2022lora,
title={Lo{RA}: Low-Rank Adaptation of Large Language Models},
author={Edward J Hu and yelong shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=nZeVKeeFYf9}
}

@inproceedings{10.1145/2939672.2939778,
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939778},
doi = {10.1145/2939672.2939778},
abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1135–1144},
numpages = {10},
keywords = {black box classifier, explaining machine learning, interpretability, interpretable machine learning},
location = {San Francisco, California, USA},
series = {KDD '16}
}

@article{HORNIK1990551,
title = {Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks},
journal = {Neural Networks},
volume = {3},
number = {5},
pages = {551-560},
year = {1990},
issn = {0893-6080},
doi = {https://doi.org/10.1016/0893-6080(90)90005-6},
url = {https://www.sciencedirect.com/science/article/pii/0893608090900056},
author = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
keywords = {Approximation, Derivatives, Sobolev space, Feedforward networks},
abstract = {We give conditions ensuring that multilayer feedforward networks with as few as a single hidden layer and an appropriately smooth hidden layer activation function are capable of arbitrarily accurate approximation to an arbitrary function and its derivatives. In fact, these networks can approximate functions that are not differentiable in the classical sense, but possess only a generalized derivative, as is the case for certain piecewise differentiable functions. The conditions imposed on the hidden layer activation function are relatively mild; the conditions imposed on the domain of the function to be approximated have practical implications. Our approximation results provide a previously missing theoretical justification for the use of multilayer feedforward networks in applications requiring simultaneous approximation of a function and its derivatives.}
}

@article{bengio_repr,
	title        = {Representation Learning: A Review and New Perspectives},
	author       = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
	year         = 2013,
	month        = {aug},
	journal      = {IEEE Trans. Pattern Anal. Mach. Intell.},
	publisher    = {IEEE Computer Society},
	address      = {USA},
	volume       = 35,
	number       = 8,
	pages        = {1798–1828},
	doi          = {10.1109/TPAMI.2013.50},
	issn         = {0162-8828},
	url          = {https://doi.org/10.1109/TPAMI.2013.50},
	issue_date   = {August 2013},
	abstract     = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.},
	numpages     = 31,
	keywords     = {unsupervised learning, representation learning, neural nets, feature learning, autoencoder, Speech recognition, Neural networks, Manifolds, Machine learning, Learning systems, Feature extraction, Deep learning, Boltzmann machine, Abstracts}
}
@inproceedings{biderman2023pythia,
	title        = {Pythia: A suite for analyzing large language models across training and scaling},
	author       = {Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin Gregory and Bradley, Herbie and O’Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and others},
	year         = 2023,
	booktitle    = {International Conference on Machine Learning},
	pages        = {2397--2430},
	organization = {PMLR}
}
@article{low_rank,
	title        = {Literature survey on low rank approximation of matrices},
	author       = {N. Kishore Kumar and J. Schneider},
	year         = 2017,
	journal      = {Linear and Multilinear Algebra},
	publisher    = {Taylor & Francis},
	volume       = 65,
	number       = 11,
	pages        = {2212--2244},
	doi          = {10.1080/03081087.2016.1267104},
	url          = {https://doi.org/10.1080/03081087.2016.1267104}
}
@misc{chinchilla_scaling,
	title        = {Training Compute-Optimal Large Language Models},
	author       = {Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
	year         = 2022,
	eprint       = {2203.15556},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@misc{tinyllama,
	title        = {TinyLlama: An Open-Source Small Language Model},
	author       = {Peiyuan Zhang and Guangtao Zeng and Tianduo Wang and Wei Lu},
	year         = 2024,
	eprint       = {2401.02385},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@inproceedings{chang-mccallum-2022-softmax,
	title        = {Softmax Bottleneck Makes Language Models Unable to Represent Multi-mode Word Distributions},
	author       = {Chang, Haw-Shiuan  and McCallum, Andrew},
	year         = 2022,
	month        = may,
	booktitle    = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Dublin, Ireland},
	pages        = {8048--8073},
	doi          = {10.18653/v1/2022.acl-long.554},
	url          = {https://aclanthology.org/2022.acl-long.554},
	editor       = {Muresan, Smaranda  and Nakov, Preslav  and Villavicencio, Aline}
}
@misc{wikitext,
	title        = {Pointer Sentinel Mixture Models},
	author       = {Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
	year         = 2016,
	eprint       = {1609.07843},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@inproceedings{rudman-etal-2022-isoscore,
	title        = {{I}so{S}core: Measuring the Uniformity of Embedding Space Utilization},
	author       = {Rudman, William  and Gillman, Nate  and Rayne, Taylor  and Eickhoff, Carsten},
	year         = 2022,
	month        = may,
	booktitle    = {Findings of the Association for Computational Linguistics: ACL 2022},
	publisher    = {Association for Computational Linguistics},
	address      = {Dublin, Ireland},
	pages        = {3325--3339},
	doi          = {10.18653/v1/2022.findings-acl.262},
	url          = {https://aclanthology.org/2022.findings-acl.262},
	editor       = {Muresan, Smaranda  and Nakov, Preslav  and Villavicencio, Aline}
}
@misc{jiang2023mistral,
	title        = {Mistral 7B},
	author       = {Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
	year         = 2023,
	eprint       = {2310.06825},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@misc{almazrouei2023falcon,
	title        = {The Falcon Series of Open Language Models},
	author       = {Ebtesam Almazrouei and Hamza Alobeidli and Abdulaziz Alshamsi and Alessandro Cappelli and Ruxandra Cojocaru and Mérouane Debbah and Étienne Goffinet and Daniel Hesslow and Julien Launay and Quentin Malartic and Daniele Mazzotta and Badreddine Noune and Baptiste Pannier and Guilherme Penedo},
	year         = 2023,
	eprint       = {2311.16867},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@misc{brown2020language,
	title        = {Language Models are Few-Shot Learners},
	author       = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
	year         = 2020,
	eprint       = {2005.14165},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@misc{eval-harness,
	title        = {A framework for few-shot language model evaluation},
	author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
	year         = 2023,
	month        = 12,
	publisher    = {Zenodo},
	doi          = {10.5281/zenodo.10256836},
	url          = {https://zenodo.org/records/10256836},
	version      = {v0.4.0}
}
@inproceedings{imdb,
	title        = {Learning Word Vectors for Sentiment Analysis},
	author       = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},
	year         = 2011,
	month        = {June},
	booktitle    = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
	publisher    = {Association for Computational Linguistics},
	address      = {Portland, Oregon, USA},
	pages        = {142--150},
	url          = {http://www.aclweb.org/anthology/P11-1015}
}
@inproceedings{srebro2003weighted,
	title        = {Weighted low-rank approximations},
	author       = {Srebro, Nathan and Jaakkola, Tommi},
	year         = 2003,
	booktitle    = {Proceedings of the 20th international conference on machine learning (ICML-03)},
	pages        = {720--727}
}
@inproceedings{intrinsic_d,
	title        = {Intrinsic Dimension Estimation for Robust Detection of {AI}-Generated Texts},
	author       = {Eduard Tulchinskii and Kristian Kuznetsov and Kushnareva Laida and Daniil Cherniavskii and Sergey Nikolenko and Evgeny Burnaev and Serguei Barannikov and Irina Piontkovskaya},
	year         = 2023,
	booktitle    = {Thirty-seventh Conference on Neural Information Processing Systems},
	url          = {https://openreview.net/forum?id=8uOZ0kNji6}
}
@inproceedings{rudman2024stable,
	title        = {Stable Anisotropic Regularization},
	author       = {William Rudman and Carsten Eickhoff},
	year         = 2024,
	booktitle    = {The Twelfth International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=dbQH9AOVd5}
}
@misc{nrusimha2024mitigatingimpactoutlierchannels,
	title        = {Mitigating the Impact of Outlier Channels for Language Model Quantization with Activation Regularization},
	author       = {Aniruddha Nrusimha and Mayank Mishra and Naigang Wang and Dan Alistarh and Rameswar Panda and Yoon Kim},
	year         = 2024,
	url          = {https://arxiv.org/abs/2404.03605},
	eprint       = {2404.03605},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@inproceedings{ahmadian2023intriguing,
	title        = {Intriguing Properties of Quantization at Scale},
	author       = {Arash Ahmadian and Saurabh Dash and Hongyu Chen and Bharat Venkitesh and Zhen Stephen Gou and Phil Blunsom and Ahmet {\"U}st{\"u}n and Sara Hooker},
	year         = 2023,
	booktitle    = {Thirty-seventh Conference on Neural Information Processing Systems},
	url          = {https://openreview.net/forum?id=IYe8j7Gy8f}
}
@inproceedings{haemmerl-etal-2023-exploring,
	title        = {Exploring Anisotropy and Outliers in Multilingual Language Models for Cross-Lingual Semantic Sentence Similarity},
	author       = {H{\"a}mmerl, Katharina  and Fastowski, Alina  and Libovick{\'y}, Jind{\v{r}}ich  and Fraser, Alexander},
	year         = 2023,
	month        = jul,
	booktitle    = {Findings of the Association for Computational Linguistics: ACL 2023},
	publisher    = {Association for Computational Linguistics},
	address      = {Toronto, Canada},
	pages        = {7023--7037},
	doi          = {10.18653/v1/2023.findings-acl.439},
	url          = {https://aclanthology.org/2023.findings-acl.439},
	editor       = {Rogers, Anna  and Boyd-Graber, Jordan  and Okazaki, Naoaki},
	abstract     = {Previous work has shown that the representations output by contextual language models are more anisotropic than static type embeddings, and typically display outlier dimensions. This seems to be true for both monolingual and multilingual models, although much less work has been done on the multilingual context. Why these outliers occur and how they affect the representations is still an active area of research. We investigate outlier dimensions and their relationship to anisotropy in multiple pre-trained multilingual language models. We focus on cross-lingual semantic similarity tasks, as these are natural tasks for evaluating multilingual representations. Specifically, we examine sentence representations. Sentence transformers which are fine-tuned on parallel resources (that are not always available) perform better on this task, and we show that their representations are more isotropic. However, we aim to improve multilingual representations in general. We investigate how much of the performance difference can be made up by only transforming the embedding space without fine-tuning, and visualise the resulting spaces. We test different operations: Removing individual outlier dimensions, cluster-based isotropy enhancement, and ZCA whitening. We publish our code for reproducibility.}
}
@inproceedings{phdim,
	title        = {A Fractal Dimension for Measures via Persistent Homology},
	author       = {Adams, Henry and Aminian, Manuchehr and Farnell, Elin and Kirby, Michael and Mirth, Joshua and Neville, Rachel and Peterson, Chris and Shonkwiler, Clayton},
	year         = 2020,
	booktitle    = {Topological Data Analysis},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {1--31},
	isbn         = {978-3-030-43408-3},
	editor       = {Baas, Nils A. and Carlsson, Gunnar E. and Quick, Gereon and Szymik, Markus and Thaule, Marius},
	abstract     = {We use persistent homology in order to define a family of fractal dimensions, denoted dimPHi($\mu$){\$}{\$}{\backslash}dim {\_}{\{}{\backslash}mathrm {\{}PH{\}}{\}}^i({\backslash}mu ){\$}{\$}for each homological dimension i{\thinspace}≥{\thinspace}0, assigned to a probability measure $\mu$ on a metric space. The case of zero-dimensional homology (i{\thinspace}={\thinspace}0) relates to work by Steele (Ann Probab 16(4): 1767--1787, 1988) studying the total length of a minimal spanning tree on a random sampling of points. Indeed, if $\mu$ is supported on a compact subset of Euclidean space ℝm{\$}{\$}{\backslash}mathbb {\{}R{\}}^m{\$}{\$}for m{\thinspace}≥{\thinspace}2, then Steele's work implies that dimPH0($\mu$)=m{\$}{\$}{\backslash}dim {\_}{\{}{\backslash}mathrm {\{}PH{\}}{\}}^0({\backslash}mu )=m{\$}{\$}if the absolutely continuous part of $\mu$ has positive mass, and otherwise dimPH0($\mu$)<m{\$}{\$}{\backslash}dim {\_}{\{}{\backslash}mathrm {\{}PH{\}}{\}}^0({\backslash}mu )<m{\$}{\$}. Experiments suggest that similar results may be true for higher-dimensional homology 0{\thinspace}<{\thinspace}i{\thinspace}<{\thinspace}m, though this is an open question. Our fractal dimension is defined by considering a limit, as the number of points n goes to infinity, of the total sum of the i-dimensional persistent homology interval lengths for n random points selected from $\mu$ in an i.i.d. fashion. To some measures $\mu$, we are able to assign a finer invariant, a curve measuring the limiting distribution of persistent homology interval lengths as the number of points goes to infinity. We prove this limiting curve exists in the case of zero-dimensional homology when $\mu$ is the uniform distribution over the unit interval, and conjecture that it exists when $\mu$ is the rescaled probability measure for a compact set in Euclidean space with positive Lebesgue measure.}
}
@inproceedings{finlayson2024closing,
	title        = {Closing the Curious Case of Neural Text Degeneration},
	author       = {Matthew Finlayson and John Hewitt and Alexander Koller and Swabha Swayamdipta and Ashish Sabharwal},
	year         = 2024,
	booktitle    = {The Twelfth International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=dONpC9GL1o}
}
@article{scaling_manifold,
	title        = {Scaling Laws from the Data Manifold Dimension},
	author       = {Utkarsh Sharma and Jared Kaplan},
	year         = 2022,
	journal      = {Journal of Machine Learning Research},
	volume       = 23,
	number       = 9,
	pages        = {1--34},
	url          = {http://jmlr.org/papers/v23/20-1111.html}
}
@article{ACKLEY1985147,
	title        = {A learning algorithm for boltzmann machines},
	author       = {David H. Ackley and Geoffrey E. Hinton and Terrence J. Sejnowski},
	year         = 1985,
	journal      = {Cognitive Science},
	volume       = 9,
	number       = 1,
	pages        = {147--169},
	doi          = {https://doi.org/10.1016/S0364-0213(85)80012-4},
	issn         = {0364-0213},
	url          = {https://www.sciencedirect.com/science/article/pii/S0364021385800124},
	abstract     = {The computational power of massively parallel networks of simple processing elements resides in the communication bandwidth provided by the hardware connections between elements. These connections can allow a significant fraction of the knowledge of the system to be applied to an instance of a problem in a very short time. One kind of computation for which massively parallel networks appear to be well suited is large constraint satisfaction searches, but to use the connections efficiently two conditions must be met: First, a search technique that is suitable for parallel networks must be found. Second, there must be some way of choosing internal representations which allow the preexisting hardware connections to be used efficiently for encoding the constraints in the domain being searched. We describe a general parallel search method, based on statistical mechanics, and we show how it leads to a general learning rule for modifying the connection strengths so as to incorporate knowledge about a task domain in an efficient way. We describe some simple examples in which the learning algorithm creates internal representations that are demonstrably the most efficient way of using the preexisting connectivity structure.}
}
@inproceedings{Welleck2020Neural,
	title        = {Neural Text Generation With Unlikelihood Training},
	author       = {Sean Welleck and Ilia Kulikov and Stephen Roller and Emily Dinan and Kyunghyun Cho and Jason Weston},
	year         = 2020,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=SJeYe0NtvH}
}
@misc{zhou2021frequencybaseddistortionscontextualizedword,
	title        = {Frequency-based Distortions in Contextualized Word Embeddings},
	author       = {Kaitlyn Zhou and Kawin Ethayarajh and Dan Jurafsky},
	year         = 2021,
	url          = {https://arxiv.org/abs/2104.08465},
	eprint       = {2104.08465},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@inproceedings{freitag-al-onaizan-2017-beam,
	title        = {Beam Search Strategies for Neural Machine Translation},
	author       = {Freitag, Markus  and Al-Onaizan, Yaser},
	year         = 2017,
	month        = aug,
	booktitle    = {Proceedings of the First Workshop on Neural Machine Translation},
	publisher    = {Association for Computational Linguistics},
	address      = {Vancouver},
	pages        = {56--60},
	doi          = {10.18653/v1/W17-3207},
	url          = {https://aclanthology.org/W17-3207},
	editor       = {Luong, Thang  and Birch, Alexandra  and Neubig, Graham  and Finch, Andrew},
	abstract     = {The basic concept in Neural Machine Translation (NMT) is to train a large Neural Network that maximizes the translation performance on a given parallel corpus. NMT is then using a simple left-to-right beam-search decoder to generate new translations that approximately maximize the trained conditional probability. The current beam search strategy generates the target sentence word by word from left-to-right while keeping a fixed amount of active candidates at each time step. First, this simple search is less adaptive as it also expands candidates whose scores are much worse than the current best. Secondly, it does not expand hypotheses if they are not within the best scoring candidates, even if their scores are close to the best one. The latter one can be avoided by increasing the beam size until no performance improvement can be observed. While you can reach better performance, this has the drawback of a slower decoding speed. In this paper, we concentrate on speeding up the decoder by applying a more flexible beam search strategy whose candidate size may vary at each time step depending on the candidate scores. We speed up the original decoder by up to 43{\%} for the two language pairs German to English and Chinese to English without losing any translation quality.}
}
@inproceedings{Holtzman2020The,
	title        = {The Curious Case of Neural Text Degeneration},
	author       = {Ari Holtzman and Jan Buys and Li Du and Maxwell Forbes and Yejin Choi},
	year         = 2020,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=rygGQyrFvH}
}
@inproceedings{sigsoftmax,
	title        = {Sigsoftmax: Reanalysis of the Softmax Bottleneck},
	author       = {Kanai, Sekitoshi and Fujiwara, Yasuhiro and Yamanaka, Yuki and Adachi, Shuichi},
	year         = 2018,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 31,
	pages        = {},
	url          = {https://proceedings.neurips.cc/paper_files/paper/2018/file/9dcb88e0137649590b755372b040afad-Paper.pdf},
	editor       = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett}
}
@misc{lin2021breaking,
	title        = {Breaking the Softmax Bottleneck for Sequential Recommender Systems with Dropout and Decoupling},
	author       = {Ying-Chen Lin},
	year         = 2021,
	eprint       = {2110.05409},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@article{ngram_svd,
	title        = {A linear space representation of language probability through SVD of N-gram matrix},
	author       = {Terashima, Shiro and Takeda, Kazuya and Itakura, Fumitada},
	year         = 2003,
	journal      = {Electronics and Communications in Japan (Part III: Fundamental Electronic Science)},
	volume       = 86,
	number       = 8,
	pages        = {61--70},
	doi          = {https://doi.org/10.1002/ecjc.10106},
	url          = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ecjc.10106},
	keywords     = {singular value decomposition, number of dimensions, entropy, clustering},
	eprint       = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/ecjc.10106}
}
@inproceedings{NEURIPS2023_74bb24dc,
	title        = {Language Model Tokenizers Introduce Unfairness Between Languages},
	author       = {Petrov, Aleksandar and La Malfa, Emanuele and Torr, Philip and Bibi, Adel},
	year         = 2023,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 36,
	pages        = {36963--36990},
	url          = {https://proceedings.neurips.cc/paper_files/paper/2023/file/74bb24dca8334adce292883b4b651eda-Paper-Conference.pdf},
	editor       = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine}
}
@article{gumbel-orig,
	title        = {Les valeurs extr\^emes des distributions statistiques},
	author       = {Gumbel, E.J.},
	year         = 1935,
	journal      = {Annales de l'institut Henri Poincar\'e},
	publisher    = {INSTITUT HENRI POINCAR\'E ET LES PRESSES UNIVERSITAIRES DE FRANCE},
	volume       = 5,
	number       = 2,
	pages        = {115--158},
	url          = {http://www.numdam.org/item/AIHP_1935__5_2_115_0/},
	zbl          = {0011.36102},
	language     = {fr}
}
@article{rillig_2023,
	title        = {Risks and Benefits of Large Language Models for the Environment},
	author       = {Rillig, Matthias C. and Ågerstrand, Marlene and Bi, Mohan and Gould, Kenneth A. and Sauerland, Uli},
	year         = 2023,
	journal      = {Environmental Science \& Technology},
	volume       = 57,
	number       = 9,
	pages        = {3464--3466},
	doi          = {10.1021/acs.est.3c01106},
	url          = {https://doi.org/10.1021/acs.est.3c01106},
	note         = {PMID: 36821477},
	eprint       = {https://doi.org/10.1021/acs.est.3c01106}
}
@article{su14095172,
	title        = {Unraveling the Hidden Environmental Impacts of AI Solutions for Environment Life Cycle Assessment of AI Solutions},
	author       = {Ligozat, Anne-Laure and Lefevre, Julien and Bugeau, Aurélie and Combaz, Jacques},
	year         = 2022,
	journal      = {Sustainability},
	volume       = 14,
	number       = 9,
	doi          = {10.3390/su14095172},
	issn         = {2071-1050},
	url          = {https://www.mdpi.com/2071-1050/14/9/5172},
	article-number = 5172,
	abstract     = {In the past ten years, artificial intelligence has encountered such dramatic progress that it is now seen as a tool of choice to solve environmental issues and, in the first place, greenhouse gas emissions (GHG). At the same time, the deep learning community began to realize that training models with more and more parameters require a lot of energy and, as a consequence, GHG emissions. To our knowledge, questioning the complete net environmental impacts of AI solutions for the environment (AI for Green) and not only GHG, has never been addressed directly. In this article, we propose to study the possible negative impacts of AI for Green. First, we review the different types of AI impacts; then, we present the different methodologies used to assess those impacts and show how to apply life cycle assessment to AI services. Finally, we discuss how to assess the environmental usefulness of a general AI service and point out the limitations of existing work in AI for Green.}
}
@inproceedings{10.1145/3461702.3462624,
	title        = {Persistent Anti-Muslim Bias in Large Language Models},
	author       = {Abid, Abubakar and Farooqi, Maheen and Zou, James},
	year         = 2021,
	booktitle    = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
	location     = {Virtual Event, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {AIES '21},
	pages        = {298–306},
	doi          = {10.1145/3461702.3462624},
	isbn         = 9781450384735,
	url          = {https://doi.org/10.1145/3461702.3462624},
	abstract     = {It has been observed that large-scale language models capture undesirable societal biases, e.g. relating to race and gender; yet religious bias has been relatively unexplored. We demonstrate that GPT-3, a state-of-the-art contextual language model, captures persistent Muslim-violence bias. We probe GPT-3 in various ways, including prompt completion, analogical reasoning, and story generation, to understand this anti-Muslim bias, demonstrating that it appears consistently and creatively in different uses of the model and that it is severe even compared to biases about other religious groups. For instance, Muslim is analogized to terrorist in 23\% of test cases, while Jewish is mapped to its most common stereotype, money, in 5\% of test cases. We quantify the positive distraction needed to overcome this bias with adversarial text prompts, and find that use of the most positive 6 adjectives reduces violent completions for Muslims from 66\% to 20\%, but which is still higher than for other religious groups.},
	numpages     = 9,
	keywords     = {stereotypes, machine learning, language models, ethics, bias}
}
@inproceedings{nadeem-etal-2021-stereoset,
	title        = {{S}tereo{S}et: Measuring stereotypical bias in pretrained language models},
	author       = {Nadeem, Moin  and Bethke, Anna  and Reddy, Siva},
	year         = 2021,
	month        = aug,
	booktitle    = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {5356--5371},
	doi          = {10.18653/v1/2021.acl-long.416},
	url          = {https://aclanthology.org/2021.acl-long.416},
	editor       = {Zong, Chengqing  and Xia, Fei  and Li, Wenjie  and Navigli, Roberto},
	abstract     = {A stereotype is an over-generalized belief about a particular group of people, e.g., Asians are good at math or African Americans are athletic. Such beliefs (biases) are known to hurt target groups. Since pretrained language models are trained on large real-world data, they are known to capture stereotypical biases. It is important to quantify to what extent these biases are present in them. Although this is a rapidly growing area of research, existing literature lacks in two important aspects: 1) they mainly evaluate bias of pretrained language models on a small set of artificial sentences, even though these models are trained on natural data 2) current evaluations focus on measuring bias without considering the language modeling ability of a model, which could lead to misleading trust on a model even if it is a poor language model. We address both these problems. We present StereoSet, a large-scale natural English dataset to measure stereotypical biases in four domains: gender, profession, race, and religion. We contrast both stereotypical bias and language modeling ability of popular models like BERT, GPT-2, RoBERTa, and XLnet. We show that these models exhibit strong stereotypical biases. Our data and code are available at \url{https://stereoset.mit.edu}.}
}
@inproceedings{kotek2023gender,
	title        = {Gender bias and stereotypes in large language models},
	author       = {Kotek, Hadas and Dockum, Rikker and Sun, David},
	year         = 2023,
	booktitle    = {Proceedings of the ACM collective intelligence conference},
	pages        = {12--24}
}
@inproceedings{parrots_bender,
	title        = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
	author       = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
	year         = 2021,
	booktitle    = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
	location     = {Virtual Event, Canada},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {FAccT '21},
	pages        = {610–623},
	doi          = {10.1145/3442188.3445922},
	isbn         = 9781450383097,
	url          = {https://doi.org/10.1145/3442188.3445922},
	abstract     = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
	numpages     = 14
}
@article{nystromformer,
	title        = {Nyströmformer: A Nyström-based Algorithm for Approximating Self-Attention},
	author       = {Xiong, Yunyang and Zeng, Zhanpeng and Chakraborty, Rudrasis and Tan, Mingxing and Fung, Glenn and Li, Yin and Singh, Vikas},
	year         = 2021,
	month        = {May},
	journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = 35,
	number       = 16,
	pages        = {14138--14148},
	doi          = {10.1609/aaai.v35i16.17664},
	url          = {https://ojs.aaai.org/index.php/AAAI/article/view/17664},
	abstractnote = {Transformers have emerged as a powerful tool for a broad range of natural language processing tasks. A key component that drives the impressive performance of Transformers is the self-attention mechanism that encodes the influence or dependence of other tokens on each specific token. While beneficial, the quadratic complexity of self-attention on the input sequence length has limited its application to longer sequences - a topic being actively studied in the community. To address this limitation, we propose Nyströmformer - a model that exhibits favorable scalability as a function of sequence length. Our idea is based on adapting the Nyström method to approximate standard self-attention with O(n) complexity. The scalability of Nyströmformer enables application to longer sequences with thousands of tokens. We perform evaluations on multiple downstream tasks on the GLUE benchmark and IMDB reviews with standard sequence length, and find that our Nyströmformer performs comparably, or in a few cases, even slightly better, than standard self-attention. On longer sequence tasks in the Long Range Arena (LRA) benchmark, Nyströmformer performs favorably relative to other efficient self-attention methods. Our code is available at https://github.com/mlpen/Nystromformer.}
}
@article{wang2020linformer,
	title        = {Linformer: Self-Attention with Linear Complexity},
	author       = {Wang, Sinong and Li, Belinda and Khabsa, Madian and Fang, Han and Ma, Hao},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2006.04768}
}
@inproceedings{yu2023megabyte,
	title        = {{MEGABYTE}: Predicting Million-byte Sequences with Multiscale Transformers},
	author       = {LILI YU and Daniel Simig and Colin Flaherty and Armen Aghajanyan and Luke Zettlemoyer and Mike Lewis},
	year         = 2023,
	booktitle    = {Thirty-seventh Conference on Neural Information Processing Systems},
	url          = {https://openreview.net/forum?id=JTmO2V9Xpz}
}
@inproceedings{nawrot-etal-2023-efficient,
	title        = {Efficient Transformers with Dynamic Token Pooling},
	author       = {Nawrot, Piotr  and Chorowski, Jan  and Lancucki, Adrian  and Ponti, Edoardo Maria},
	year         = 2023,
	month        = jul,
	booktitle    = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Toronto, Canada},
	pages        = {6403--6417},
	doi          = {10.18653/v1/2023.acl-long.353},
	url          = {https://aclanthology.org/2023.acl-long.353},
	editor       = {Rogers, Anna  and Boyd-Graber, Jordan  and Okazaki, Naoaki},
	abstract     = {Transformers achieve unrivalled performance in modelling language, but remain inefficient in terms of memory and time complexity. A possible remedy is to reduce the sequence length in the intermediate layers by pooling fixed-length segments of tokens. Nevertheless, natural units of meaning, such as words or phrases, display varying sizes. To address this mismatch, we equip language models with a dynamic-pooling mechanism, which predicts segment boundaries in an autoregressive fashion. We compare several methods to infer boundaries, including end-to-end learning through stochastic re-parameterisation, supervised learning (based on segmentations from subword tokenizers or spikes in conditional entropy), as well as linguistically motivated boundaries. We perform character-level evaluation on texts from multiple datasets and morphologically diverse languages. The results demonstrate that dynamic pooling, which jointly segments and models language, is both faster and more accurate than vanilla Transformers and fixed-length pooling within the same computational budget.}
}
@inproceedings{grivas-etal-2022-low,
	title        = {Low-Rank Softmax Can Have Unargmaxable Classes in Theory but Rarely in Practice},
	author       = {Grivas, Andreas  and Bogoychev, Nikolay  and Lopez, Adam},
	year         = 2022,
	month        = may,
	booktitle    = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Dublin, Ireland},
	pages        = {6738--6758},
	doi          = {10.18653/v1/2022.acl-long.465},
	url          = {https://aclanthology.org/2022.acl-long.465},
	editor       = {Muresan, Smaranda  and Nakov, Preslav  and Villavicencio, Aline},
	abstract     = {Classifiers in natural language processing (NLP) often have a large number of output classes. For example, neural language models (LMs) and machine translation (MT) models both predict tokens from a vocabulary of thousands. The Softmax output layer of these models typically receives as input a dense feature representation, which has much lower dimensionality than the output. In theory, the result is some words may be impossible to be predicted via argmax, irrespective of input features, and empirically, there is evidence this happens in small language models (Demeter et al., 2020). In this paper we ask whether it can happen in practical large language models and translation models. To do so, we develop algorithms to detect such unargmaxable tokens in public models. We find that 13 out of 150 models do indeed have such tokens; however, they are very infrequent and unlikely to impact model quality. We release our algorithms and code to the public.}
}
@inproceedings{softmax_bottleneck,
	title        = {Breaking the Softmax Bottleneck: A High-Rank {RNN} Language Model},
	author       = {Zhilin Yang and Zihang Dai and Ruslan Salakhutdinov and William W. Cohen},
	year         = 2018,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=HkwZSG-CZ}
}
@misc{faysse2024croissantllm,
	title        = {CroissantLLM: A Truly Bilingual French-English Language Model},
	author       = {Manuel Faysse and Patrick Fernandes and Nuno M. Guerreiro and António Loison and Duarte M. Alves and Caio Corro and Nicolas Boizard and João Alves and Ricardo Rei and Pedro H. Martins and Antoni Bigata Casademunt and François Yvon and André F. T. Martins and Gautier Viaud and Céline Hudelot and Pierre Colombo},
	year         = 2024,
	eprint       = {2402.00786},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@misc{beyond_chinchilla,
	title        = {Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws},
	author       = {Nikhil Sardana and Jonathan Frankle},
	year         = 2023,
	eprint       = {2401.00448},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@article{kaplan_scaling,
	title        = {Scaling Laws for Neural Language Models},
	author       = {Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeff Wu and Dario Amodei},
	year         = 2020,
	journal      = {ArXiv},
	volume       = {abs/2001.08361},
	url          = {https://api.semanticscholar.org/CorpusID:210861095}
}
@inproceedings{lambada,
	title        = {The {LAMBADA} dataset: Word prediction requiring a broad discourse context},
	author       = {Paperno, Denis  and  Kruszewski, Germ\'{a}n  and  Lazaridou, Angeliki  and  Pham, Ngoc Quan  and  Bernardi, Raffaella  and  Pezzelle, Sandro  and  Baroni, Marco  and  Boleda, Gemma  and  Fernandez, Raquel},
	year         = 2016,
	month        = {August},
	booktitle    = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Berlin, Germany},
	pages        = {1525--1534},
	url          = {http://www.aclweb.org/anthology/P16-1144}
}
@inproceedings{bondarenko2023quantizable,
	title        = {Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing},
	author       = {Yelysei Bondarenko and Markus Nagel and Tijmen Blankevoort},
	year         = 2023,
	booktitle    = {Thirty-seventh Conference on Neural Information Processing Systems},
	url          = {https://openreview.net/forum?id=sbusw6LD41}
}
@misc{lee2024owq,
	title        = {OWQ: Outlier-Aware Weight Quantization for Efficient Fine-Tuning and Inference of Large Language Models},
	author       = {Changhun Lee and Jungyu Jin and Taesu Kim and Hyungjun Kim and Eunhyeok Park},
	year         = 2024,
	eprint       = {2306.02272},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@comment{jabref-meta: databaseType:bibtex;}
