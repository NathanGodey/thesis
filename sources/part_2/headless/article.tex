
Natural Language Processing (NLP) has seen tremendous progress in recent years thanks to the development of large-scale neural language models. These models have been shown to be effective in a wide range of NLP tasks such as text classification, question answering, and machine translation, either in fine-tuning, few-shot and zero-shot settings. These approaches usually involve a self-supervised pre-training step, based on tasks requiring predictions of contextual probability distributions over a large vocabulary of tokens.

However, the need for a language modeling projection head can be a limitation as it requires additional memory, slows down training and impedes scaling up to large token vocabularies. In this chapter, we propose a novel pretraining approach called Headless Language Modeling, which removes the need to predict probability distributions and rather focuses on leveraging contrastive learning to reconstruct sequences of input embeddings. Instead of adding a projection head towards a high-dimensional vocabulary space in order to make a prediction about a given token, we teach those models to contrastively output static embeddings corresponding to this token. The static embeddings we use for this are the model's own input embeddings. Due to its resemblance with the well-established weight-tying trick \citep{press-wolf-2017-using,he2023debertav3}, we call this pre-training technique \textit{Contrastive Weight Tying} (CWT).

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\linewidth]{sources/part_2/headless/imgs/hlm_basic.pdf}
    \caption{Masked Headless Language Modeling (HLM) using Contrastive Weight Tying. The CWT objective aims to contrastively predict masked input representations using in-batch negative examples.}
    \label{fig:cwt_schema}
\end{figure}

We find that our approach outperforms usual language modeling counterparts in several aspects and by substantial margins. First, it drastically speeds up training by freeing up GPU memory and avoiding the costly language modeling projection, thus allowing up to 2$\times$ acceleration of the training throughput, and up to 20$\times$ less compute requirements to achieve similar performance. Moreover, given the same amount of training tokens, headless language models (HLMs) significantly outperform their classical counterparts on downstream tasks, as shown by a 2.7 gain in LAMBADA accuracy for our headless generative model. Finally, given similar compute budgets, HLMs bring substantial gains for NLU tasks, with our BERT reproduction scoring 1.6 points above its classical counterpart on the GLUE benchmark. We also show that headless models can benefit from larger token vocabularies at a much more reasonable cost than classical models.


In terms of implementation\footnote{Our pretraining and fine-tuning code is published in \url{https://github.com/NathanGodey/headless-lm}}, our approach can be used as a drop-in replacement in usual pretraining codebases, as it only requires a change in the loss computation that can be applied to any kind of language model.

Overall, we make several contributions in this chapter:
\begin{itemize}
    \item We introduce a pretraining objective that replaces cross-entropy, thus removing the need to project on the vocabulary high-dimensional space and instead learning to contrastively predict latent representations of tokens;
    \item Using this technique, we pretrain encoder and decoder models for English, and a multilingual encoder model;
    \item We show the various benefits of headless training in terms of data-efficiency, compute-efficiency, and performance;
    \item We explore the effects of micro-batch size and vocabulary size on downstream performance, and provide an ablation study of our contrastive objective. 
\end{itemize}



\section{Related Work: Token-level Contrastive Learning}

\paragraph{Contrastive approaches in NLP}
The idea of relieving language models of the need to predict probabilities over the whole token vocabulary has been explored in the importance sampling literature \citep{quick_train_bengio_03,mnih2012fast,jean-etal-2015-using, ma-collins-2018-noise}. These methods approximate the denominator of the softmax by using only a subset of the possible tokens. Those approaches usually rely on variants of the Noise-Contrastive Estimation objective \citep{nce} that use unique negative samples, contrary to our approach that samples representations uniformly from the batch. \citet{kumar2018vmf} and \citet{tokarchuk-niculae-2022-target} use contrastive objectives based on cosine-similarity to match pre-trained static embeddings for Machine Translation. 
% 
We instead use the model's input embeddings as trainable target representations.

\paragraph{Contrastive self-supervised learning}
The Contrastive Predictive Coding loss \citep{oord2019representation} initiated the use of pretraining approaches based on a contrastive learning objective, an idea that has obtained success in many modalities over the years \citep{sermanet2018timecontrastive,schneider19_interspeech, wav2vec2,algayres-etal-2022-dp}. 
%
In NLP, contrastive learning has proven efficient in the training of sentence-level models \citep{gao-etal-2021-simcse, yan-etal-2021-consert, klein-nabi-2023-micse}. Token-level approaches rely on contrastive auxiliary objectives that are added to the usual cross-entropy loss. SimCTG \citep{su2022contrastive} introduces a token-level contrastive objective using in-batch output representations as negative samples, and adds this objective to a sentence-level contrastive loss and a regular causal LM loss. TaCL \citep{su-etal-2022-tacl} relies on a similar technique for encoder models, where a teacher model is used to produce negative samples. ContraCLM \citep{jain-etal-2023-contraclm} uses an auxiliary contrastive loss for code generation.

Our work differ by using a token-level contrastive objective not as an approximation of cross-entropy as in importance sampling, or as a regularizing auxilary loss as in \citet{su2022contrastive}, but rather as a standalone token-level representation learning objective. The resulting representation models can then be fine-tuned for downstream classification and regression tasks, or for language modeling.



\section{Method}
\subsection{Classical framework}
\label{sec:cwt}
\begin{figure*}[h]
    \centering
    \begin{subfigure}[b]{0.7\linewidth}
         \includegraphics[width=\linewidth]{sources/part_2/headless/imgs/weight_tying.pdf}
         \caption{Vanilla}
         \label{fig:comparison_schema_wt}
    \end{subfigure}
    \begin{subfigure}[b]{0.7\linewidth}
         \includegraphics[width=\linewidth]{sources/part_2/headless/imgs/contrastive_weight_tying.pdf}
         \caption{Contrastive (ours)}
         \label{fig:comparison_schema_cwt}
    \end{subfigure}
    \caption{Schematic comparison of the classical weight tying approach and the Contrastive Weight Tying loss. The green embedding corresponds to the masked token.}
    \label{fig:comparison_schema}
\end{figure*}

We consider a batch $\mathcal{B} = (w^i_t)_{i \in [1, N], t \in [1,L]}$ of $N$ token sequences of length $L$. We also produce a slightly altered version of these sequences $\tilde{\mathcal{B}} = (\tilde{w^i_t})_{i \in [1, N], t \in [1,\tilde{L}]}$, optionally using masking or random replacement for instance, as some pretraining objectives require. We introduce an embedding matrix $e_\theta \in \mathbb{R}^{V \times d_m}$ where $V$ is the token vocabulary size and $D$ is the hidden dimension, and a sequence-to-sequence model $T_\theta: \mathbb{R}^{N\times L\times d_m} \rightarrow \mathbb{R}^{N\times L\times d_m}$ both based on a set of parameters $\theta \in \mathbb{R}^P$.

A classical language modeling approach consists in selecting a subset of tokens $\mathcal{B}_\mathcal{S} = (w^i_t)_{i,t \in \mathcal{S}}$, and then estimating a probability distribution over the token vocabulary for these tokens from the $(w^i_t)$ sequences, using $e_\theta$ and $T_\theta$. Learning occurs as $\mathcal{B}_\mathcal{S}$ is partially altered in $(\tilde{w}^i_t)$ (e.g. in Masked Language Modeling) or internally in $T_\theta$ (e.g. decoder models), and contextual information is essential for $e_\theta$ and $T_\theta$ to accurately estimate the tokens in $\mathcal{B}_\mathcal{S}$.

A trick that has been used in many such approaches relies on using $e_\theta$'s transpose ($e_\theta^T$) as a projection from the output space of $T_\theta$ to $\mathbb{R}^V$. This approach, called weight tying \citep{press-wolf-2017-using}, can be written for a given sequence at index $i \in [1, N]$ as:
$$
\hat{p}^i_t = \sigma \left( e_\theta^T \left(T_\theta \left( e_\theta \left(\widetilde{\mathbf{w}}^i\right) \right)_j\right) \right)
$$
where $\hat{p}^i_t$ is the estimated distribution for the $t$-th token of the $i$-th sequence. Weight tying has been shown to improve performance while reducing the number of parameters \citep{electra}. Cross-entropy loss is then used as an objective function:
$$
\mathcal{L}(\theta, \mathcal{B}, \tilde{\mathcal{B}}) = - \frac{1}{|\mathcal{S}|}\sum_{i,t \in \mathcal{S}} \mathbf{1}_{w^i_t} \cdot \log(\hat{p}^i_t)
$$

\subsection{Headless modeling}

While weight tying does not use additional parameters, the projection $e_\theta^T$ actually has a non-negligible computational cost, which increases as the token vocabulary grows. Like \citet{gao2018representation}, we advocate that the weight tying approach tends to maximize the scalar product between the input embedding of the original token $e_\theta(w^i_t)$ and the output representation at the same position $o^{\theta, i}_t = T_\theta(e_\theta(\tilde{\mathbf{w}}^{i}))_t$, under the contrastive regularization of the softmax function.

Based on this understanding, we design an objective that directly optimizes this scalar product while not requiring the computation of the $e_\theta^T$ projection. As we do not use this projection, we cannot rely on softmax regularization anymore, and instead introduce a contrastive loss using the in-batch samples from $\mathcal{S}$ as negatives. All in all, our contrastive loss can be written as:
$$
\mathcal{L}_c(\theta, \mathcal{B}, \tilde{\mathcal{B}}) = - \frac{1}{|\mathcal{S}|} \sum_{i \in \mathcal{S}} \log \sum_{t \in \mathcal{S}^i} \frac{e^{o^{\theta, i}_t \cdot e_\theta(w^i_t)}}{\sum_{k,l \in \mathcal{S}} e^{o^{\theta, i}_t \cdot e_\theta(w^k_tl)}}
$$

We call this objective \textit{Contrastive Weight Tying} (CWT), as weight sharing is not used \textit{per se} but is set as a contrastive objective. Across the chapter, we \textit{do not combine} this loss function with the classical cross-entropy objective as in \citet{su2022contrastive}, and rather use it as the only pretraining objective. To the best of our knowledge, this work stands as the first attempt to pretrain language models in a self-supervised fashion using an explicit contrastive loss as the sole objective.

\subsection{The case of decoders: Causal Fine-tuning}
\label{sec:causal_fine-tuning}

We can easily adapt the Causal Language Modeling (CLM) objective using the Contrastive Weight Tying approach. Negative samples correspond to every input embedding at a different position in the batch. However, the resulting model is not directly able to generate text, as it has no projection head towards $\mathbb{R}^V$. A way to retrieve language generation capacities is to use the input embedding matrix transpose $e_{\theta}^T$ as a projection head \citep{kumar2018vmf,tokarchuk-niculae-2022-target}. Nevertheless, we observe that this approach yields poor performance (see \Cref{tab:lm_results}). Instead, we fine-tune the headless model and a language modeling head initialized with $e_{\theta}^T$ using the predictive CLM objective on a small portion ($<$2\%) of the pre-training dataset. This method allows recovering an effective language model.

\subsection{Theoretical considerations}
\label{sec:theory}
In terms of time and memory complexity, Headless Language Models (HLMs) are more efficient than classical language models under usual conditions. If we focus on the computation of the loss \textit{on a single device} from $|\mathcal{S}| = K$ output representations, a neural probabilistic LM requires $O(Kd_mV)$ operations while our headless approach performs $O(K^2d_m)$ operations\footnote{One could extend our CWT loss by picking a separate set $\mathcal{S}_N$ of negative samples. This allows to tune the number of negative samples, which is important in Contrastive Learning. However, for the sake of simplicity, and to avoid extensive hyperparameter tuning, we set $\mathcal{S}_N = \mathcal{S}$.}. Hence, when $K < V$, which is very common for micro-batch sizes that fit on one device, our CWT loss is more computationally efficient than cross-entropy. 
%
With regard to memory requirements, our CWT loss is also more efficient than its classical counterpart. On the one hand, the cross-entropy loss with weight tying stores the outputs of the $e_\theta^T$ projection of dimension $K \times V$ in the forward pass. On the other hand, our CWT loss stores the scalar product matrix of dimension $K \times N$, which is again smaller when $K < V$.

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.48\columnwidth}
         \includegraphics[width=\linewidth]{sources/part_2/headless/imgs/bert_train_speed_p13.png}
         \caption{Training latency}
         \label{fig:speedup}
    \end{subfigure}
    \begin{subfigure}{0.48\columnwidth}
         \includegraphics[width=\linewidth]{sources/part_2/headless/imgs/bert_memory_use_p13.png}
         \caption{Memory use}
         \label{fig:memuse}
    \end{subfigure}
    \caption{Comparison of time and memory complexities of a BERT-base model on a single RTX 8000 GPU.}
    \label{fig:comparison}
\end{figure}
In \Cref{fig:comparison}, we provide a preliminary empirical analysis of the speed and memory improvements when training a BERT-base model using original hyperparameters, i.e. sequences of 512 tokens and 15\% masking. We use HuggingFace's implementation for the Transformers blocks, and run experiments on a single RTX 8000 GPU.
We observe that training latency is significantly reduced by roughly 25\% for all batch sizes, and that the engine can handle a larger batch size due to the improvement in memory consumption.

\section{Experiments}
\label{sec:experiments}
We use the Contrastive Weight Tying objective for medium-scale pre-training experiments in different contexts. We focus on monolingual encoder and decoder architectures, but we also train one multilingual encoder as we believe the uniformity brought by our contrastive objective may improve cross-lingual alignment.
We compare our HLMs with classical language models that we pretrain on the same data with roughly similar compute budgets.

\subsection{Headless Monolingual Encoder}
\label{sec:mono_encoder}
We pretrain BERT-base architectures (110M parameters) for English on the OpenWebText2 dataset extracted from The Pile \citep{gao2020pile}. We use the tokenizer from the Pythia suite \citep{biderman2023pythia}, which was trained on The Pile and uses a 50k tokens vocabulary.
We mostly use hyperparameters from BERT \citep{devlin-etal-2019-bert}, although we remove the NSP objective as in RoBERTa \citep{roberta}. For the sake of simplicity, we use a sequence length of 128 for the whole training. We give a detailed overview of the hyperparameters in \Cref{app:train_mono_enc}.

We pretrain all models using 8 A100 GPUs, with a budget of roughly 1,000 hours each. To optimize training, we use memory-efficient self-attention as implemented in xFormers \citep{xFormers2022} for all experiments. For the vanilla MLM, we set a micro-batch size of 32 for each A100 GPU, then accumulate to the original 256 batch size at optimization level, and train on 1 million batches. For our headless approach, we observed that we could remain within compute budget when using a micro-batch size of 64. Hence, we use an effective batch size of 512 for the headless MLM (HMLM). Although the HMLM uses more pretraining sequences, it does not gain additional information compared to the vanilla MLM as both models perform several epochs on the OpenWebText2 dataset.


We evaluate on the GLUE benchmark, where we exclude the RTE dataset due to high standard deviations in the obtained scores. We fine-tune our models for 10 epochs on every dataset, and compute validation metrics once every fine-tuning epoch. We use the AdamW optimizer with a learning rate of $10^{-5}$, a weight decay of $0.01$ and a balanced cross-entropy loss objective. 

As a matter of fact, we noticed that using balanced cross-entropy loss for fine-tuning could further improve the performance of \underline{all} our monolingual encoders, and increase the gap between headless models and their vanilla counterparts. We also noticed empirically that it helped stabilize results for smaller datasets such as MRPC and COLA.

Let's consider a classification problem where the class distribution is described by frequencies $(f_c)_{c \in [1, C]}$. We can group the cross entropy loss $\mathcal{L}_{ce}$ as such:
$$
\mathcal{L}_{ce}(X, Y) = \sum_{c=1}^{C} \mathcal{L}_c(X, Y)
$$
where 
$$
\mathcal{L}_c(X, Y) = \sum_{i=1}^{N} \mathbf{1}_{y_i = c} \cdot \mathcal{L}_{ce}(x_i, y_i)
$$

Using this notation, the \textit{balanced cross-entropy loss} can be defined as:
$$
\mathcal{L}_{bce}(X, Y) = \sum_{c=1}^{C} \frac{\mathcal{L}_c(X, Y)}{f_c}
$$

In practice, we approximate the $(f_c)$ using the batch labels. The purpose of the balanced cross-entropy loss is to mitigate general and in-batch class imbalance.

We reproduce fine-tuning experiments with the more usual categorical cross-entropy loss only, and using moderately optimized hyperparameters for this loss (see \Cref{tab:hp_unbalanced}).

\begin{table*}[ht]
\centering \small
\begin{tabular}{cc|c}
\toprule
MLM type        & Balanced loss (FT) & \textbf{GLUE Avg.} \\ \midrule
Vanilla & $\times$ & 81.5 \tiny{($\pm$0.14)}         \\ 
Vanilla & \checkmark & 82.43 \tiny{($\pm$0.12)}         \\ 
Headless & $\times$ & 83.19 \tiny{($\pm$0.09)}            \\
Headless & \checkmark & \textbf{84.08} \tiny{($\pm$0.14)}           \\ \bottomrule
\end{tabular}
\caption{Results of Masked Language Models (MLMs) on the dev sets of the GLUE benchmark for different fine-tuning strategies. Results are averaged over 3 runs.}
\label{tab:glue_res_ce}
\end{table*}


\begin{table*}
\centering
\scalebox{0.75}{%
\begin{tabular}{ccc|cccccccc}
\toprule
MLM type        & Tokens (B) & GPU hours & MRPC                        & COLA                        & STS-B                      & SST2                        & QNLI                        & QQP                         & MNLI                        & \textbf{Avg.} \\ \midrule
Vanilla  &4.1& 989& \underline{85.87}          & 54.66          & 83.7         & 92.45        & 88.38      & 89.57         & 82.4       & 82.43 \tiny{($\pm$0.12)}         \\ 
Headless &4.1& 444& 85.31                       & \underline{58.35}                       & \underline{84.54}                      & \textbf{93.23}                       & \underline{89.49}                       & \underline{89.62}                       & \underline{82.54}                       & \underline{83.29} \tiny{($\pm$0.15)}            \\ 
Headless &8.2&888& \textbf{86.89} & \textbf{60.72} & \textbf{85.98 } & \underline{92.56} & \textbf{89.75} & \textbf{89.81} & \textbf{82.87 } & \textbf{84.08} \tiny{($\pm$0.14)} \\ \bottomrule
\end{tabular}}
\caption{Results of Masked Language Models (MLMs) on the dev sets of the GLUE benchmark. Best results are \textbf{bold} and second best are \underline{underlined}. We report Matthews' correlation for COLA, Spearman correlation for STS-B, and accuracy elsewhere. MNLI validation datasets are concatenated. All scores are averaged over 3 different seeds.}
\label{tab:glue_res}
\end{table*}

\begin{table}
\centering
\small
\begin{tabular}{@{}c|ccccc@{}}
\toprule
MLM type & BoolQ          & CB             & COPA           & WiC            & Avg.           \\ \midrule
Vanilla  & 68.8          & \textbf{77.8} & 60.2          & 64.9          & 67.9 \tiny{($\pm$0.4)}          \\ 
Headless & \textbf{69.8} & 74.7          & \textbf{62.7} & \textbf{67.2} & \textbf{68.6} \tiny{($\pm$0.6)} \\ \bottomrule
\end{tabular}
\caption{Results of Masked Language Models (MLMs) on the dev sets of datasets from the SuperGLUE benchmark. We report accuracy for all tasks. Scores are averaged over 10 fine-tuning runs.}
\end{table}

In \Cref{tab:glue_res}, we compare our headless MLM with the classical MLM on the GLUE benchmark. To ensure fair comparison, we display evaluations at similar amounts of tokens seen during pre-training, and at similar training durations on the same hardware. In both cases, the headless MLM outperforms the vanilla MLM by significant margins, showing that our CWT loss is both more data-efficient and compute-efficient in this setup.
%
We extend this analysis at various intervals along pretraining, and plot results in \Cref{fig:train_curve_mlm}. It shows that the headless MLM outperforms the downstream performance of its vanilla counterpart after using 25\% of its training compute. We notice that the performance gap is near constant across pretraining steps.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.48\columnwidth}
         \includegraphics[width=\linewidth]{sources/part_2/headless/imgs/bert_compare_hours_p13.png}
         \caption{Pretraining hours}
         \label{fig:bert_compare_hours}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\columnwidth}
         \includegraphics[width=\linewidth]{sources/part_2/headless/imgs/bert_compare_tokens_p13.png}
         \caption{Pretraining tokens}
         \label{fig:bert_compare_tokens}
    \end{subfigure}
    \caption{Comparison of GLUE average scores along pretraining.}
    \label{fig:train_curve_mlm}
\end{figure}



\subsection{Headless Monolingual Decoder}
\label{sec:mono_decoder}
We pretrain Pythia-70M architectures for English, sticking to the Pythia procedure \citep{biderman2023pythia} as much as possible. We use OpenWebText2 as a pretraining dataset. We train on 143,000 batches of 1,024 sequences of length 2,048 split over 16 V100 GPUs. We use exactly the same hyperparameters as in the Pythia suite. The micro-batch size is set to 32 in both cases.

%
As mentioned in \Cref{sec:causal_fine-tuning}, we fine-tune our headless models for CLM with an LM head initialized with $e_{\theta}^T$ for 10000 steps using an effective batch size of 256 (4$\times$ smaller that during pretraining), a learning rate of $10^{-4}$, and a constant learning rate schedule with 2000 linear warm-up steps. All other hyperparameters are kept similar to pretraining. We evaluate our models on the LAMBADA dataset and report accuracy and perplexity for zero-shot generation in \Cref{fig:train_curve_lm}.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.48\linewidth}
         \includegraphics[width=\linewidth]{sources/part_2/headless/imgs/lambada_acc_p13.png}
         \caption{Accuracy}
         \label{fig:lambada_acc}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\linewidth}
         \includegraphics[width=\linewidth]{sources/part_2/headless/imgs/lambada_ppl_p13.png}
         \caption{Perplexity}
         \label{fig:lambada_ppl}
    \end{subfigure}
    \caption{Comparison of LAMBADA metrics along pretraining. We display results for vanilla causal language modeling and headless models before and after causal LM fine-tuning. The pretraining token count for the fine-tuned HLM takes fine-tuning tokens into account.}
    \label{fig:train_curve_lm}
\end{figure}

\begin{table}
\centering
\scalebox{0.85}{%
\begin{tabular}{c|ccc}
\toprule
\multirow{2}{*}{LM type} & Validation & \multicolumn{2}{c}{LAMBADA} \\
        \cmidrule{2-2} \cmidrule{3-4}
        {} & Ppl. & Ppl. & Acc. \\ \midrule
Vanilla  &     \textbf{3.143}  &  170.23 & 19.52 \\ 
Headless  &     -  &  524.44 & 18.26 \\ 
Headless + FT & 3.283 & \textbf{153.5} & \textbf{22.2}\\ \bottomrule
\end{tabular}}
\caption{Results of the causal language models on the validation set after training, and on the LAMBADA dataset.}
\label{tab:lm_results}
\end{table}

We find that the HLM fine-tuned for predictive language modeling outperforms the vanilla model by a significant margin along training. We report language generation results in \Cref{tab:lm_results}. We observe that despite having a higher validation perplexity even after fine-tuning, the HLM is improving the zero-shot perplexity on the LAMBADA dataset.

We also study the zero-shot performance of the causal models on datasets taken from the LM Evaluation Harness. At this model scale, many tasks are not relevant and thus discarded, as the results do not always significantly outperform a random baseline. We also discarded tasks where the sample size was below 1000 or where comparison was not meaningful due to low performance gaps compared to the variance level. %Hence, a subset of tasks where comparison is relevant is shown in 
Hence, only a subset of the tasks is shown in
\Cref{tab:lm_zs_perf}.


\begin{table*}
\centering
\scalebox{0.85}{%
\begin{tabular}{cc|cccccccc}
\toprule
LM type       &GPU hours      & BoolQ          & CrowS-Pairs $\downarrow$   & RACE           & SciQ          & PubMedQA &  QASPER    \\ \midrule
Vanilla       &1712 \tiny{(-)} & 47.8 \tiny{($\pm$0.9)}          & 57.3 \tiny{($\pm$1.2)}         & 23.7 \tiny{($\pm$1.3)}           & \textbf{66.4} \tiny{($\pm$1.5)}  & 43.8 \tiny{($\pm$1.6)} &  41.9 \tiny{($\pm$4.8)}        \\ 
HLM \tiny{+ FT} & 1052 \tiny{(61\%)}         & \textbf{53.0}$^\dagger$ \tiny{($\pm$0.9)}           & \textbf{56.0} \tiny{($\pm$1.2)}         & \textbf{26.0} \tiny{($\pm$1.4)}  & 64.5 \tiny{($\pm$1.5)}          & \textbf{47.5}$^\dagger$ \tiny{($\pm$1.6)}  & \textbf{66.0}$^\dagger$ \tiny{($\pm$3.1)}      \\ \bottomrule
\end{tabular}}
\caption{Zero-shot evaluation of monolingual causal language models on datasets from the LM Evaluation Harness. We report the stereotype percentage for CrowS-Pairs and accuracy elsewhere. $^\dagger$: best scores that are significantly better than the second best score according to a one-tailed t-test with power 0.95.}
\label{tab:lm_zs_perf}
\end{table*}

In \Cref{tab:lm_zs_perf}, we find that the fine-tuned HLM outperforms the vanilla causal model by significant margins on BoolQ \citep{clark-etal-2019-boolq}, PubMedQA \citep{jin-etal-2019-pubmedqa} and QASPER \citep{dasigi-etal-2021-dataset}. Although we observe less statistically significant gaps for the other datasets, we still note that our HLM performs at least comparably to the vanilla baseline.
%
We also note that the HLM seems slightly less prone to stereotypes as measured by the CrowS-Pairs benchmark \citep{nangia-etal-2020-crows}.

Overall, using the Contrastive Weight Tying loss in the context of causal LM allows obtaining models on par with vanilla counterparts at a lower compute cost. We notice that the resulting models can get surprisingly good results in challenging datasets, hence showing language understanding capabilities, while being outclassed in language generation benchmarks (before predictive fine-tuning). We believe that this work shows that language generation needs to be considered as a \textit{downstream task} for HLMs, as they are designed to generate representations instead of words.

\section{Multilingual Encoder}
\label{sec:multi_mlm}
In this section, we pretrain small multilingual MLMs and evaluate their performance on the XNLI dataset \citep{conneau2018xnli}. 
%
Due to compute limitations, we consider architectures similar to the distilled multilingual BERT\footnote{Available at \url{https://huggingface.co/distilbert-base-multilingual-cased}} trained by \citet{sanh2019distilbert}. This model has 137M parameters, and uses a vocabulary of 119k tokens. As in \Cref{sec:mono_encoder}, we train a vanilla MLM and a headless counterpart. However, we share training hyperparameters such as batch size and total number of steps between both models, without compute considerations. For both experiments, we pretrain our models on 400k batches of 64 sequences of 128 tokens taken from the multilingual Wikipedia dataset using a single RTX8000 GPU. We select 90 million entries from 10 languages (Arabic, German, English, Spanish, French, Hindi, Italian, Japanese, Korean, and Chinese). Training hyperparameters can be found in \Cref{app:train_multi_enc}.

Models are then fine-tuned on the XNLI dataset, for both cross-lingual zero-shot transfer from English and target language fine-tuning. Fine-tuning hyperparameters can be found in \Cref{app:ft_multi_enc}.

\begin{table*}[h]
\centering
\scalebox{0.85}{%
\begin{tabular}{ccccccccc}
\toprule
MLM type        & ar             & de             & en             & es             & fr          & hi             & zh             & Avg.           \\ \midrule
\multicolumn{9}{l}{\textit{Fine-tuned on English only}} \\ \midrule
Vanilla  & 46.83          & 56.71          & 71.66          & 59.93          & 58.34       & 43.16          & 50.99          & 55.37 \tiny{($\pm$0.11)}          \\ 
Headless & \textbf{48.06} & \textbf{57.32} & \textbf{74.03} & \textbf{62.72} & \textbf{62} & \textbf{45.25} & \textbf{52.15} & \textbf{57.36} \tiny{($\pm$0.2)} \\ \midrule
\multicolumn{9}{l}{\textit{Fine-tuned on target language}} \\ \midrule
Vanilla  & 51.32          & 64.09          & 70.4           & 66.98          & 65.88          & 55.95 & 64.63          & 62.87 \tiny{($\pm$0.2)}         \\ 
Headless & \textbf{54.25} & \textbf{66.95} & \textbf{73.96} & \textbf{69.14} & \textbf{67.22} & \textbf{60.04} & \textbf{67.22} & \textbf{65.54} \tiny{($\pm$0.22)} \\ \bottomrule
\end{tabular}}
\caption{Evaluation of multilingual models on the XNLI benchmark. We report dev accuracy, averaged over 3 runs.}
\end{table*}

We display final results in \Cref{fig:train_curve_multilm}. We find that the headless approach leads to significantly better performance for every language in both cross-lingual transfer and language-specific fine-tuning. In average, the headless MLM outperforms its vanilla counterpart by 2 accuracy points in the cross-lingual scenario, and by 2.7 points in the language-specific fine-tuning experiments.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.48\columnwidth}
         \includegraphics[width=\linewidth]{sources/part_2/headless/imgs/xnli_translate_train_p13.png}
         \caption{Translate-Train: target language fine-tuning}
         \label{fig:translate_train}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\columnwidth}
         \includegraphics[width=\linewidth]{sources/part_2/headless/imgs/xnli_cross_p13.png}
         \caption{Translate-Test: English fine-tuning}
         \label{fig:translate_test}
    \end{subfigure}
    \caption{Comparison of XNLI average scores along pretraining for different setups. Models are fine-tuned/evaluated in Arabic, German, English, Spanish, French, Hindi and Chinese. %We display the standard error across seeds.
    }
    \label{fig:train_curve_multilm}
\end{figure}

In \Cref{fig:train_curve_multilm}, we evaluate the models at intermediate %checkpoints along pretraining,
pretraining checkpoints
and plot the XNLI average score as a function of used GPU hours. We observe that our HLM finishes training within 45\% of the time required by the vanilla %model. Moreover, our model outperforms %the performance level of 
mode, and that its performance level outperforms
the fully trained vanilla model after only using 5\% as much compute in \Cref{fig:translate_train}, and 22\% in \Cref{fig:translate_test}.




\section{Discussion}
\label{sec:discussion_hlm}
\paragraph{Token vocabulary} Training language models without output vocabulary projection makes using large vocabularies more affordable in terms of compute. As a matter of fact, the time complexity of HLMs during training is theoretically constant as we increase the vocabulary size. With input embedding lookup tables that do not require fully loading the $e_\theta$ weights, the memory complexity can also be kept constant with respect to the size of the vocabulary. This property could be useful for multilingual models relying on considerable vocabulary sizes, such as XLM-V \citep{2023arXiv230110472L}.

To verify this hypothesis, we pretrain models for different vocabulary sizes using the BERT-Small architecture from \citet{turc2019} and the CC-News dataset \citep{Hamborg2017}. Hyperparameter details can be found in \Cref{app:s_train_mono_enc}. For each vocabulary size, we train a BPE tokenizer similar to the BERT tokenizer, and pretrain a vanilla MLM and a headless MLM. We then compare average GLUE results, excluding RTE, MRPC and COLA, due to high variance at that model scale.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.48\columnwidth}
         \includegraphics[width=\linewidth]{sources/part_2/headless/imgs/bert_vocabsize_p13.png}
         \caption{GLUE average score}
         \label{fig:glue_tokencount}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\columnwidth}
         \includegraphics[width=\linewidth]{sources/part_2/headless/imgs/speed_vocabsize_p13.png}
         \caption{Training speed}
         \label{fig:throughput_tokencount}
    \end{subfigure}
    \caption{Comparison of downstream performance and training speed for small models trained using different token vocabulary sizes.}
    \label{fig:tokencount}
\end{figure}

\Cref{fig:tokencount} shows that HLMs can actually benefit from larger token vocabularies up to a certain extent, and that they outperform their vanilla counterparts for every vocabulary size. \Cref{fig:throughput_tokencount} demonstrates that increasing the vocabulary size comes at almost no decrease in training speed for the HLMs, contrary to vanilla MLMs. However, we observe a sudden throughput increase between 85k and 100k tokens vocabularies for both vanilla and headless models, which we attribute to a different handling of GPU memory and operations as the models get bigger.

\paragraph{Batch size}

As discussed in \Cref{sec:theory}, the micro-batch size used to compute the CWT loss is important as it impacts the training complexity by increasing the number of negative samples. Recent work on Contrastive Learning shows that there usually exists an optimal number of negative samples in terms of model performance \citep{contr1, contr2}. As a consequence, increasing the batch size when using CWT may not always be beneficial.

To study the impact of batch size on downstream performance, we pretrain small decoder models using different batch sizes. Our models are inspired from the smallest architecture of GPT2 \citep{gpt2} where many hyperparameters are divided by 4. More details about the pretraining procedure of these models can be found in \Cref{app:s_train_mono_dec}. HLMs are fine-tuned similarly to \Cref{sec:mono_decoder}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.70\linewidth]{sources/part_2/headless/imgs/batch_size_hours_p13.png}
    \caption{LAMBADA accuracy along pretraining for different batch sizes.}
    \label{fig:batch_size}
\end{figure}

In \Cref{fig:batch_size}, we observe that increasing batch size  leads to better performance for our HLMs. While smaller batch sizes train even faster, the headless model with the greatest batch size (128) is the only one that is able to significantly outperform its vanilla counterpart at the end of training.

\paragraph{Ablation study}

In \Cref{tab:ablation}, we conduct an ablation study by training small models using the hyperparameters described in \Cref{app:s_train_mono_enc} for different objectives. We observe that adding Cross-Entropy to CWT leads to slightly worse performance, at the cost of reduced throughput. We also notice that using a contrastive objective without using input embeddings as targets decreases performance, despite adding parameters during training. This shows the relevance of our weight tying approach.

\begin{table}[h]
\centering \small
\begin{tabular}{@{}cccc@{}}
\toprule
Objective           & Parameters & Throughput $\uparrow$     & GLUE avg.      \\ \midrule
Cross-Entropy       & x1        & x1             & 82.45          \\
Cross-Entropy + CWT & x1        & x0.87          & 82.93          \\
NCE (wo/ WT)        & x1.57        & \textbf{x2.47} & 82.91          \\
CWT                 & x1        & \textbf{x2.13} & \textbf{83.37} \\ \bottomrule
\end{tabular}
\caption{Ablation study using variants of the CWT objective. In CWT + Cross-Entropy, we add the objectives without specific weights. In NCE (wo/ WT), we adapt our CWT objective with an additional static embedding matrix instead of the model's input embeddings, which resembles \citet{ma-collins-2018-noise}.}
\label{tab:ablation}
\end{table}

\section{Modeling considerations} 
\subsection{Token-level similarity \& Frequency-related issues}
From a linguistic point of view, we hypothesize that an important difference between our approach and classical predictive modeling is the fact that \textit{headless modeling mostly pushes for discrimination between co-occurring tokens}, instead of imposing a contextual hierarchy over the whole vocabulary. For instance, in the case of synonyms A and B, each occurrence of A (or B) is pushing the input representations of A and B apart for predictive modeling, due to weight tying. For headless modeling, an occurrence of A will only push the representations apart if B appears in the same batch. Hence, the CWT objective could let models identify A and B as synonyms more easily. This argument is already mentioned in \citet{jean-etal-2015-using}.

To provide empirical evidence of this behavior, we study the representation similarity for pairs of synonyms for classical and headless models. 
We use WordNet \citep{Fellbaum1998} to extract synonym pairs and we then compute the cosine-similarity between the input embeddings corresponding to the two synonyms.
Resulting cosine-similarity distributions are displayed in \Cref{fig:cosine_input_embs}.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.48\columnwidth}
         \includegraphics[width=\linewidth]{sources/part_2/headless/imgs/input_embs_cosines_bert.png}
         \caption{Monolingual encoders}
         \label{fig:input_embs_cosines_bert}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\columnwidth}
         \includegraphics[width=\linewidth]{sources/part_2/headless/imgs/input_embs_cosines_pythia.png}
         \caption{Monolingual decoders}
         \label{fig:input_embs_cosines_pythia}
    \end{subfigure}
    \caption{Cosine-similarity distributions for pairs of WordNet synonyms.}
    \label{fig:cosine_input_embs}
\end{figure}

In \Cref{fig:cosine_input_embs}, we observe that HLMs tend to generally represent synonyms in a more similar way than vanilla LMs, as cosine-similarity distributions slightly drift towards higher values. In average, cosine-similarity between synonyms is 1.4 points higher for the encoder and roughly 7 points higher for both the original HLM decoder and its fine-tuned version.

However, we do not observe a radical difference between HLMs and classical LMs in this analysis of the input representations. A more thorough analysis of the latent spaces of both types of models could be relevant. For instance, comparing contextual representations of similar words across examples could help clarify this matter. We leave such analyses for future work.

Another advantage of pushing discrimination between co-occurring tokens only may be an improved feedback quality, as we expect distinguishing between co-occurring tokens to be more linguistically relevant than distinguishing between all tokens.

Finally, we believe that our method avoids the issue of cross-entropy regarding rare and unused tokens. \citet{gao19} prove that cross-entropy pushes the representations of rare and unused tokens in a shared direction, thus distorting the resulting embedding space. The CWT objective only updates these embeddings when they appear in the negative samples, which should result in more meaningful representations.

\subsection{Anisotropy Mitigation}

An implicit consequence of using a contrastive objective with in-batch negative samples is a uniformization of the representational space \citep{Wang2020Improving}. As a result, it can be expected that the CWT objective leads to more isotropic representations than vanilla cross-entropy, at least on the last layers of trained models. Moreover, using in-batch negative samples reduces the negative update issues mentioned in \citet{gao2018representation} and \citet{bis-etal-2021-much}, i.e. the distortions that are caused by some rare tokens being trained to be dissimilar to frequent ones. Indeed, for CLMs, as tokens used negative samples also appear in the batch, their representations are not only updated to contrast them with positive samples as they are also trained to be aligned with their corresponding ground-truth token representation.

To verify the effect of the CWT loss on anisotropy, we report layer-wise average cosine-similarity (see \Cref{ssec:rw_aniso}) in \Cref{fig:cosine_embs}.

\begin{figure}[H]
     \centering
     \begin{subfigure}[b]{0.48\columnwidth}
          \includegraphics[width=\linewidth]{sources/part_2/headless/imgs/anisotropy_mono.png}
          \caption{Monolingual}
          \label{fig:cosines_bert}
     \end{subfigure}
     \begin{subfigure}[b]{0.46\columnwidth}
          \includegraphics[width=\linewidth]{sources/part_2/headless/imgs/anisotropy_multi.png}
          \caption{Multilingual}
          \label{fig:cosines_mbert}
     \end{subfigure}
     \caption{Layer-wise anisotropy for monolingual and multilingual MLMs trained with the headless and vanilla objectives.}
     \label{fig:cosine_embs}
 \end{figure}

\Cref{fig:cosine_embs} shows that monolingual and multilingual headless models are nearly isotropic on their last layer, as opposed to models based on cross-entropy. Interestingly, both our monolingual pretrained encoder models have rather low anisotropy levels compared to the observations mentioned in \Cref{chap:anisotropy}. Moreover, in both cases, the anisotropy level in intermediate layers is actually higher for headless models, which proves that the effect of the loss on last-layer anisotropy is not always propagated to these layers. Hence, the representation degeneration in intermediate layers cannot be safely analyzed only from the loss perspective, as detailed in \Cref{chap:anisotropy}.

In our experiments, the Contrastive Weight Tying objective successfully mitigates last-layer anisotropy, which may further explain the performance and efficiency gap between headless and vanilla models.

\section*{Conclusion}
In this chapter, we present a new pretraining approach called headless language modeling, that removes the need to predict probability distributions over token vocabulary spaces and instead focuses on learning to reconstruct representations in a contrastive fashion. Our method only relies on changing the objective function, allowing for straightforward adaptations of classical language modeling pretraining objectives.

Using our contrastive objective, we pretrain headless monolingual and multilingual encoders, and a headless monolingual decoder. We demonstrate that headless pretraining is significantly more compute-efficient, data-efficient, and performant than classical predictive methods. We provide explanations for this improved performance based on representation analysis, as these models better embed semantic similarity and mitigate representation degeneration for the output layer.

A major advantage of our approach is that it enables the use of very large token vocabularies at virtually no increased cost.
%
We believe that this work paves the way for the exploration of contrastive techniques as a replacement of cross-entropy based pretraining objectives for NLP.


% Entries for the entire Anthology, followed by custom entries
% \bibliography{anthology,custom}
% \bibliographystyle{iclr2024_conference}

% \appendix


\section{Limitations}
One key limitation of this work is the scale of the used architectures. In recent months, the dawn of Large Language Models using billions of parameters reshaped the language modeling paradigm. The research process that led to this work is empirical and required extensive experimentation that could not be done at large scale in our academic compute budget. We believe that the results presented here are still sufficiently promising to be communicated and useful to the community. We leave the scaling of these techniques to future work.

It could be opposed to this work that as architectures grow in size, the proportion of compute that is associated with the output vocabulary projection shrinks. While we acknowledge that this effect may reduce the advantage of HLMs in terms of training throughput, our experiments show that HLMs are more performant for a given number of pretraining steps.

We chose not to compare with other efficient encoder architectures such as ELECTRA or DeBERTa in this chapter. We also chose not to apply our method to encoder-decoder architectures, or to subtle masking methods such as SpanBERT \citep{joshi-etal-2020-spanbert}. As a matter of fact, we argue that our work could be combined to these methods, and we thus believe that comparison is not relevant as these works are orthogonal to ours. We leave the intersection of these approaches for future work.

Finally, we decided to pick English for all monolingual experiments. Different behaviors could be observed for other languages, although our multilingual experiments gave no sign of such discrepancies.

\vspace{2em}

This chapter illustrates the effectiveness of pretraining NLP models in a pure representation learning paradigm, aiming at avoiding the distortion and bias phenomena observed in classical language models (see \Cref{part:analysis}). In this paradigm, language modeling can be seen as a downstream task, which lets us frame token-level cross-entropy as a fine-tuning objective.

Hence, the resulting language models are still briefly trained using cross-entropy on token distributions, which is prone to the limitations and bottleneck notably mentioned in \Cref{chap:softmax_bottleneck}. In the next chapter, we propose to abstract from the token-level paradigm that implicitly causes frequency-based and dimensional issues in the usual framework.