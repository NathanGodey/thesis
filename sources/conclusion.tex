\chapter{Conclusion}

\section{Summary}
Going back to our depiction of the current language modeling paradigm as a fundamentally behaviorist framework in \Cref{intro}, we explored in this thesis one of the possible research direction that goes beyond pure behaviorism, namely the analysis of hidden state spaces from a qualitative viewpoint. 

We first reviewed the state-of-the-art for language modeling techniques (\Cref{sec:rw_lm}), representation learning in NLP (\Cref{sec:rw_repl_nl}), and representation analysis in NLP models (\Cref{chap:rw_repr_ana}).

We then provided several qualitative representation evaluation scopes, that assess the expressivity and uniformity of representations in order to provide insights on the language models themselves.

In \Cref{chap:geobias}, we evaluate biases of language models through the lens of geographical knowledge by extracting geospatial information from the representations of named entities corresponding to locations. We then remark that the biases of language models as measured by this metric increase as the model size grows. We show that these biases tend to amplify biases that are inherent to the training datasets, thus proving this phenomenon to be an instance of frequency-based issues in the representations of language models.

We proceed by studying the more general question of frequency-based distortions in language models in \Cref{chap:softmax_bottleneck}, by extending previous works on the relation between token distributions and anisotropy. We explore a connection between the emergence of last-layer anisotropy, which is a form of representation degeneration, and an empirically observed performance saturation during training for some small language models. Moreover, we show that this representation degeneration is deeply connected with the softmax bottleneck, which can be summarized as a mismatch between the hidden dimension of the model and the intrinsic dimensionality of the target contextual log-probability space.

Nevertheless, this phenomenon is not sufficient to explain a similar anisotropy phenomenon measured in intermediate layers. In \Cref{chap:anisotropy}, we analyze this degeneration in intermediate layers and show that it is not specific to subword-level language models. We discover that anisotropy is characteristic of the Transformer architecture, and facilitates the sparsity of self-attention maps.

Finally, we explore diverse techniques to mitigate representation degeneration in language models, by proposing changes to the training objective, the data granularity, and the self-attention operation.

\Cref{chap:headless_lm} explores an in-batch contrastive objective used to train masked or causal language models. This approach implicitly and empirically mitigates last-layer anisotropy, while also improving data and compute-efficiency and allowing for much larger token vocabularies. However, this objective is not well-suited for language generation and requires a fine-tuning step using the classical cross-entropy loss. Hence, it does not entirely avoid the frequency-based distortions inherent to this objective.

Thus, we further extend the language modeling paradigm in \Cref{chap:manta}, by efficiently modeling text at byte-level, which should notably address the softmax bottleneck described in \Cref{chap:softmax_bottleneck}. Although our approach leads to more robust models that perform better on noisy textual data, we observe that our method did not significantly affect the degenerated nature of the intermediate representations of language models.

We hypothesize that this degeneration phenomenon, linked to attention sparsity in \Cref{chap:anisotropy}, can be mitigated by facilitating the densification of attention maps in Transformer-based models. In \Cref{chap:kv_comp}, we lay a path towards gradient-based neural compression of keys and values representations, and perform initial experiments that prove the potential of our method.

Overall, we explore the use of model analysis tools as ways to identify limitations, and as a source of insights towards improving language models.

\section{Discussion}
\subsection{Limitations}
Having discussed technical limitations individually for each chapter, we rather depict here more general limitations and potential objections to our overall approach.

First, we do not produce an extensive study of qualitative inner evaluation techniques for language models in this thesis. The research path that consists in making systematic observations about the inner workings of language models, and in then leveraging these observations to improve their performance is relatively underexplored, although it could be argued that it led to impactful works in recent years \citep{hu2022lora,xiao2024efficient}. This thesis rather focuses on a specific type of qualitative measurement, representation analysis, through the lens of inherent bias and latent uniformity. As such, our work only represents one example of research trajectory that explores alternatives to purely behaviorist strategies.

We are eager to see other qualitative evaluation approaches flourish in the future, for instance by leveraging interpretability methods, by identifying the inherent limitations of inductive biases, or by exploring the mechanical interactions between corresponding concepts in multimodal setups, in the hope that these works will lead to other extensions of the current AI paradigm.

Second, we want to emphasize that we do not consider representation degeneration to be inherently harmful to language models. As a matter of fact, as described in \Cref{ssec:rw_aniso}, several works prove that more anisotropic model can perform better for some downstream applications. Apart from cases of extreme dimensional collapse, there is no reason why distorted representational spaces should \textit{inevitably} lead to less performant models. We instead argue that the observed distortions are both counter-intuitive and should not be ``natural'', in the sense that random gradient-based updates are unlikely to lead to such geometries. As we have seen along the chapters of this thesis, explaining these distortions can shed light on constraints applied by the training procedure or the model architecture. Hence, reinforcing these distortions may have a positive impact on the model's behavior as it may help to strenghten inherent properties of the models (e.g. the sparsity of attention maps).

Finally, it could be argued that the methods developed in \Cref{chap:headless_lm}, \Cref{chap:manta} and \Cref{chap:softmax_bottleneck} still rely on classical evaluation methods, i.e. benchmarks evaluating models performance on examples corresponding to downstream tasks, as a final performance metric that decides on the relevance of each approach. We would like to clarify that we do not argue in this thesis \textit{against} any form of behaviorism in the approaches during training and inference, but rather advocate that the current paradigm where approaches and models are selected and ranked based \textit{solely} on such quantitative performance evaluations - which may include latency and memory-greediness measurements - could be completed with technical qualitative observations in order to design generally ``better'' appraoches and models. In that process, and as these benchmarks mimic the final use of models, it is crucial to measure the impact of novel methods driven by these qualitative observations on the quantitatively measured downstream performance of the models, in order to assess whether these methods generally improve language models and whether these qualitative observations are actually relevant to the overall quality of the model.


\subsection{Behaviorism and the community}
As we started presenting initial ideas about this work, a remark that was redundant among peers could be summarized with the following questions:

\begin{center}
    \textit{Why should we care about anisotropy? Language models still work fine, right?}
\end{center}

While it is true that as far as we tested them, the most impressive recent large language models use anisotropic intermediate representations, these questions show that the behaviorist point of view is not only widespread in research works but is also deeply embedded in the way some members of the research community view the problems at hand in modern Natural Language Processing.

We argue that this focus on performance has been driven by industrial research through some of the most influential recent works, and corresponds with an engineering view of NLP. Interestingly, this view does not seem to reflect the opinion of the majority of NLP researchers, as pictured in a meta-survey conducted among the ACL community \citep{michael-etal-2023-nlp}.

In this meta-survey, \citet{michael-etal-2023-nlp} report that only 17\% of NLP researchers believe that scale should solve any problem, and 51\% that LMs ``understand'' language. As much as 88\% opine that there is too much focus on benchmarks, 72\% that there is too much focus on scale, and 82\% that we should incorporate more interdisciplinary insights from sociolinguistics or cognitive science, for instance. Additionally, only 42\% believe that interpretability is not a promising research direction, among which industrial researchers are over-represented.

Overall, we observe that a vast fraction of the research community, and more particularly academic scholars, could embrace other paradigms towards improving language modeling, through better interpretations of the models and by leveraging concepts from other fields.

The field of mechanistic interpretability, which aligns with this research direction, has recently drawn attention from the field. We hope that these techniques will provide insights that will allow to identify caveats in the structure of language models.

\section{Perspectives}

In this final section, we provide future perspectives for the work presented in this thesis, including ideas that are natural continuations of this work, and perspectives that this work offers with respect to the future of the field of NLP.

\subsection{Future Work}

\paragraph{Tokenizer-free Headless language modeling} A logical way forward from this work consists in combining the approaches described in \Cref{chap:headless_lm} and \Cref{chap:manta} in order to alleviate models from the need to predict in byte-level output spaces at training time. As a matter of fact, training a decoder that handles pooled byte representations as in MANTa-LM using the contrastive loss described could mitigate the byte-level decoding throughput bottleneck described in \Cref{chap:manta}.

Two limitations arise from this idea. First, for causal language models, it is not trivial how to impose causality in such a framework, as the MANTa module does not impede byte information to be shared across blocks, and the prediction at index $t+1$ could leverage leaky information from the block at index $t$. Second, another issue that would affect both causal and non-causal language models is the difficulty to guarantee that the output space of the MANTa module (or any other character-pooling module) could be easily mapped to the output space of the whole model. For instance, if representation degeneration were to occur in the post-pooling section of the model itself for sparsity reasons, it would probably be harder to map input and output spaces especially if we use normalization layers in the character pooling module.

We have conducted experiments in that direction, by training masked language models based on the CharacterBERT pooling module \citep{el-boukkouri-etal-2020-characterbert} using the CWT loss, and observed training instability and poor convergence. We hypothesize that the second aforementioned reason is the main cause to the encountered difficulties, and leave more exploration of this direction for the future.

\paragraph{Character-to-semantic bottleneck} Another hypothesis that could help explain the lower performance of tokenizer-free headless language models, and more generally of byte-based models, could be the inherent difficulty to map character-level representations to higher-granularity semantic features, and the reduced interpolation properties offered by such a map. In token-level language models, input token embeddings are the leaves of the back-propagation graph and can thus be freely optimized, i.e. they can adopt \textit{any} values that optimize the final loss as they are features and not activations. In the case of byte or character-level models, the pooled representations used as the input of the larger predictive model are the results of the application of a module based on specific inductive biases on byte or character-level features. Hence, it can be questioned whether these pooled representations can efficiently and easily rival with the expressivity of classical token-level representations.

Exploring this question requires measuring the semantic expressivity of pooled representations in existing character-level models, and quantifying the inherent difficulty of the character-to-semantic mapping, either by training models to perform such a mapping, or by using a similar analysis to the softmax bottleneck identification in \Cref{chap:softmax_bottleneck}.

\paragraph{Doubly Dynamic KV cache compression} \citet{nawrot2024dynamic} and our work in \Cref{chap:kv_comp} pave the way for gradient-based KV cache compression. However, both approaches are limited by their inability to properly \textit{revisit} past decisions as generation advances. We can quickly remark that non-revisiting compression schemes are bound to linear memory complexity: if the probability of creating a new cache slot does not converge to $0$, then the cache memory is lower bound by a linear function.

However, we currently foresee two solutions towards reaching such revisiting compression schemes: re-compressing compressed sequence during generation, or allow for a non-contiguous compression scheme, that would allow to avoid repeating information when adding new slots. The first method poses challenges during training, as revisiting the nature of $K$ and $V$ representations during generation is hard to translate to a parallel computation in the self-attention process without incurring substantial efficiency costs. The second method poses modeling challenges, as it would require another compression mapping mechanism, and would probably be harder to control with a compression loss based solely on the decision variables as in \Cref{chap:kv_comp}.

\paragraph{Theoretical exploration of the softmax bottleneck} Although \Cref{chap:softmax_bottleneck} provides thorough empirical arguments in favor of our claim, we acknowledge that \Cref{main_thm} is not sufficient to fully characterize the interaction between spectral properties of the context-to-logits mapping and the optimal performance of the model. While exploring this theoretical problem, we found that the mathematical tools at our disposal - and maybe within our grasp - needed to be developed to help in this specific scenario.

For instance, the more complete problem could be framed as a weighted low-rank approximation (WLRA) where the individual contextual cross-entropy measurements could be weighted by the corresponding context probability. Nevertheless, the WLRA problem is not as extensively studied in the literature as we expected \citep{10.1145/2897518.2897639}, which did not provide relevant insights about our problem. Another foreseen limitation is the lack of clear relation between the rank function and the logarithm, especially when we need to estimate or bound the rank of a logit matrix resulting from a linear operation for which the rank can easily be measured.

We recognize that these limitations may also be overcome via a better expertise in the theoretical field that underly this question. We leave a deeper exploration of the theoretical characterization of our empirical observations for future work.

\paragraph{Alternatives to Cross-entropy} We argue that extending the observations made in \Cref{chap:geobias} and \Cref{chap:softmax_bottleneck} to other types of training objectives would yield interesting insights. First, we could study the impact of the CWT loss on both the representational biases of the model from an empirical viewpoint, and the softmax bottleneck effect from a theoretical viewpoint. We also hypothesize that knowledge distillation techniques such as DistilBERT \citep{sanh2019distilbert} may have interesting implications for these qualitative evaluation of the representations.

Along this thesis, we have explored alternative objectives for language models inspired from knowledged distillation, hoping to reduce the impact of frequency-based distortions on the inner workings of language models. For instance, we have framed language modeling as a multi-label classification task, where the model is asked to produce an acceptability probability for each token at each position, using labels generated using the contextual distributions of a classical language model. This approach led to promising initial results, and we wish to explore this method further in future works.

\paragraph{Understanding $QKV$ geometry} We have shown in \Cref{chap:anisotropy} that analyzing the geometry of attention-specific representations could shed light on phenomena that occur in language models. \citet{devoto2024simpleeffectivel2normbased} take advantage of a geometrical analysis of $QKV$ representations to propose an efficient KV cache compression technique. We suggest that this question is under-represented in the literature and could lead to substantial improvements, both regarding current models and when proposing novel architectures.


\subsection{Broader perspectives}

\paragraph{Model Evaluation} In this thesis, we argue in favor of an extension of what the community broadly deems as model evaluation. Practically, this includes refreshing probing techniques and similar counterparts to evaluate the internal representations of models. This may also be implemented by developing measurements of the informational efficiency of the inner mechanisms of models, for instance by measuring the amount of operations that lead to ignored information through sparse patterns, or the amount of computation that inherently extracts similar features from the data and is thus used inefficiently.

These examples could constitute benchmarks of their own, and be built into systematical evaluation libraries. We believe that creating uniform qualitative evaluation tools would dramatically ease this kind of analysis, as they could be seen as a model \textit{debugging} toolboxes that would provide extensive insights for architecture research among others.

\paragraph{Safety \& Alignment} As stated in \Cref{intro}, we believe that there is a direct, yet non-exhaustive, link between considering language-generating models as black-box systems and the increasing concerns in the general public and the scientific community with respect to AI safety. In our opinion, qualitative evaluation loops, where models are adapted to improve an observed suboptimal mechanism, should lead to more interpretable models as it will push the models towards more meaningful internal behaviors.

Thereby, we argue that non-behaviorist approaches actually bring us closer to safer AI systems, especially when they contribute to the design of such models. For instance, more interpretable models could be more easily edited and aligned, as their inner conceptual knowledge and its alignment to the users' expectations could be controled more easily.

We finally hypothesize that behaviorist alignment methods do not guarantee that the model is inherently aligned with the target behavior. For instance, a model could learn to produce expected outputs superficially, while still being perfectly able to produce unsafe content. This is a major challenge in the alignment field, which may be solved by internal alignment through qualitative evaluation, and potentially model editing to enforce alignment at weight level directly.

\paragraph{Interpretability}

In a nutshell, our work could be understood as a pledge for a paradigm shift in the interpretability field, going from current methods that help provide explanations and analyze the capabilities of the models to methods that provide insights about the limitations and possible improvements of models.

We believe that our contribution to interpretability itself could be further extended to more powerful tools that take linguistic information into account, and allow for a finer analysis of the inner workings of models. For instance, it would be interesting to see how the most advanced interpretability techniques relying on gradient-based estimations or neural circuits (see \Cref{chap:rw_repr_ana}) could be applied to characterize the limitations of language models, and how improvements could be made, for instance by enhancing gradient-based feedback and causation patterns, or by facilitating the emergence of efficient and non-redundant neural circuits in language models.

\vspace{2em}

Finally, we advocate that exploring these perspectives could fluidify the interactions between these three fields, which currently do not benefit from one another in an extensive way. We hope that building a framework that tends to unify these domains in the next years will bring substantial advancements to the field of NLP.
